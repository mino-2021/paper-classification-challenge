{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e22c03b",
   "metadata": {},
   "source": [
    "## 医学論文の自動仕分けチャレンジ 推論2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c63c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/gdrive')\n",
    "\n",
    "#!cp /gdrive/MyDrive/Datasets/signate-471/train.csv .\n",
    "#!cp /gdrive/MyDrive/Datasets/signate-471/test.csv .\n",
    "#!cp /gdrive/MyDrive/Datasets/signate-471/sample_submit.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c56ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b63231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers as T\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3a8515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    batch_size = 5 #16\n",
    "    num_workers = 3 #4\n",
    "    max_length =  256 #72\n",
    "    n_splits = 5\n",
    "    version = 121\n",
    "    model = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\" \n",
    "    tokenizer = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "    \n",
    "    epochs = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85045998",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../input/\"\n",
    "OUTPUT_DIR = \"../output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58bbf96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c6927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c7e7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
    "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "594765c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 471\n",
    "seed_torch(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6107bd10",
   "metadata": {},
   "source": [
    "## データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b73b316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
    "test = pd.read_csv(DATA_DIR + \"test.csv\")\n",
    "sub = pd.read_csv(DATA_DIR + \"sample_submit.csv\", header=None)\n",
    "sub.columns = [\"id\", \"judgement\"]\n",
    "TARGET = \"judgement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176e2360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023282372444280715\n"
     ]
    }
   ],
   "source": [
    "# この値を境に、モデルの出力を 0 と 1 にします。\n",
    "border = len(train[train[TARGET] == 1]) / len(train[TARGET])\n",
    "print(border)\n",
    "init_border = border"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7eab26",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e95fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # preprocess\n",
    "train[\"text\"] = train[\"title\"] + \" \" + train[\"abstract\"].fillna(\"\")\n",
    "test[\"text\"] = test[\"title\"] + \" \" + test[\"abstract\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "856008b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(train):\n",
    "\n",
    "    # 交差検証 用の番号を振ります。\n",
    "    Fold = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=seed) #5\n",
    "    for n, (train_index, val_index) in enumerate(Fold.split(train, train[TARGET])):\n",
    "        train.loc[val_index, \"fold\"] = int(n)\n",
    "    train[\"fold\"] = train[\"fold\"].astype(np.uint8)\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d675f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(test):\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ca7c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_train_data(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a92114",
   "metadata": {},
   "source": [
    "## データセット定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76116aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, df, model_name, include_labels=True):\n",
    "        #tokenizer = T.BertTokenizer.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.df = df\n",
    "        self.include_labels = include_labels\n",
    "\n",
    "        self.title = df[\"text\"].tolist()\n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.title,\n",
    "            padding = 'max_length',            \n",
    "            max_length = CFG.max_length,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        if self.include_labels:\n",
    "            self.labels = df[TARGET].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][idx])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][idx])\n",
    "\n",
    "        if self.include_labels:\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "            return input_ids, attention_mask, label\n",
    "\n",
    "        return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ebc85",
   "metadata": {},
   "source": [
    "## モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "774b39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        #self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model = T.BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model(input_ids=input_ids, attention_mask=attention_mask) #,labels=labels)\n",
    "        logits = out.logits\n",
    "        #out = self.sigmoid(out.logits).squeeze()\n",
    "        out = self.sigmoid(logits).squeeze()\n",
    "        #out = logits.squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f5491",
   "metadata": {},
   "source": [
    "## ツール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da98b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1cea44",
   "metadata": {},
   "source": [
    "## 学習補助関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf0c100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    start = end = time.time()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        batch_size = labels.size(0)\n",
    "        \n",
    "        y_preds = model(input_ids, attention_mask)\n",
    "        \n",
    "        loss = criterion(y_preds, labels)\n",
    "        \n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0 or step == (len(train_loader) - 1):\n",
    "            print(\n",
    "                f\"Epoch: [{epoch + 1}][{step}/{len(train_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(train_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "            )\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ba8db",
   "metadata": {},
   "source": [
    "## 評価 補助関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6a3064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    start = end = time.time()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(valid_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # record score\n",
    "        preds.append(y_preds.to(\"cpu\").numpy())\n",
    "\n",
    "        if step % 100 == 0 or step == (len(valid_loader) - 1):\n",
    "            print(\n",
    "                f\"EVAL: [{step}/{len(valid_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(valid_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "            )\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96cb54",
   "metadata": {},
   "source": [
    "## 推論関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89565ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    predictions = []\n",
    "\n",
    "    test_dataset = BaseDataset(test, CFG.model, include_labels=False)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    for fold in range(CFG.n_splits): #5\n",
    "    #for fold in [1,3,4,5]: #5\n",
    "        LOGGER.info(f\"========== model: bert-base-uncased fold: {fold} inference ==========\")\n",
    "        model = BaseModel(CFG.model)\n",
    "        model.to(device)\n",
    "        model.load_state_dict(torch.load(OUTPUT_DIR + f\"{CFG.version}_fold{fold}_best.pth\")[\"model\"])\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        \n",
    "        for i, (input_ids, attention_mask) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(input_ids, attention_mask)\n",
    "            preds.append(y_preds.to(\"cpu\").numpy())\n",
    "            \n",
    "        preds = np.concatenate(preds)\n",
    "        predictions.append(preds)\n",
    "        \n",
    "    predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca0e69",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27a668f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train, fold):\n",
    "\n",
    "    LOGGER.info(f\"=*========= fold: {fold} training ==========\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Data Loader\n",
    "    # ====================================================\n",
    "    trn_idx = train[train[\"fold\"] != fold].index\n",
    "    val_idx = train[train[\"fold\"] == fold].index\n",
    "    \n",
    "    train_folds = train.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = train.loc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = BaseDataset(train_folds, CFG.tokenizer) #, CFG.model) #\n",
    "    valid_dataset = BaseDataset(valid_folds, CFG.tokenizer) #, CFG.model) #\n",
    "\n",
    "    #print(\"DataLoader\")\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size, #16\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers, #4, \n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=CFG.batch_size, #16\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers, #4,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    #print(\"Model\")\n",
    "    # ====================================================\n",
    "    # Model\n",
    "    # ====================================================\n",
    "    model = BaseModel(CFG.model)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = T.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    #criterion = nn.BCEWithLogitsLoss()\n",
    "    # ====================================================\n",
    "    # Loop\n",
    "    # ====================================================\n",
    "    best_score = -1\n",
    "    best_loss = np.inf\n",
    "    #print(\"Loop\")\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, device)\n",
    "        \n",
    "        # eval\n",
    "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
    "        valid_labels = valid_folds[TARGET].values\n",
    "        \n",
    "        # scoring\n",
    "        score = fbeta_score(valid_labels, np.where(preds < init_border, 0, 1), beta=7.0)\n",
    "        LOGGER.info(f\"score1 = {score}, thresh={init_border}\")\n",
    "        \n",
    "        border = opt_fbeta_threshold(valid_labels, preds) ##最適化\n",
    "        \n",
    "        score = fbeta_score(valid_labels, np.where(preds < border, 0, 1), beta=7.0)\n",
    "        LOGGER.info(f\"score2 = {score},  thresh={border}\")\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        LOGGER.info(\n",
    "            f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\"\n",
    "        )\n",
    "        LOGGER.info(f\"Epoch {epoch+1} - Score: {score}\")\n",
    "\n",
    "        #if score >= best_score: ##\n",
    "        if avg_val_loss <= best_loss:\n",
    "        #if True:\n",
    "            best_loss = avg_val_loss\n",
    "            best_score = score\n",
    "            LOGGER.info(f\"Epoch {epoch+1} - Save Best Score: {best_score:.4f} - Best Loss: {best_loss:.4f} Model\")\n",
    "            torch.save(\n",
    "                {\"model\": model.state_dict(), \"preds\": preds}, OUTPUT_DIR + f\"{CFG.version}_fold{fold}_best.pth\"\n",
    "            )\n",
    "\n",
    "    check_point = torch.load(OUTPUT_DIR + f\"{CFG.version}_fold{fold}_best.pth\")\n",
    "\n",
    "    valid_folds[\"preds\"] = check_point[\"preds\"]\n",
    "\n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b707a",
   "metadata": {},
   "source": [
    "## メイン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "883abd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(result_df):\n",
    "    preds = result_df[\"preds\"].values\n",
    "    labels = result_df[\"judgement\"].values\n",
    "    \n",
    "    score = fbeta_score(labels, np.where(preds < border, 0, 1), beta=7.0)\n",
    "    LOGGER.info(f\"Score1: {score:<.5f}\")\n",
    "    \n",
    "    best_border = opt_fbeta_threshold(labels, preds)\n",
    "    LOGGER.info(f\"best border: {best_border:<.5f}\")\n",
    "    \n",
    "    score = fbeta_score(labels, np.where(preds < best_border, 0, 1), beta=7.0)\n",
    "    LOGGER.info(f\"Score2: {score:<.5f}\")\n",
    "    \n",
    "    return score, best_border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88058068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize, minimize_scalar\n",
    "def opt_fbeta_threshold(y_true, y_pred):\n",
    "    \"\"\"fbeta score計算時のthresholdを最適化\"\"\"\n",
    "    def opt_(x): \n",
    "        return -fbeta_score(y_true, y_pred >= x, beta=7)\n",
    "    \n",
    "    #result = minimize(opt_, x0=np.array([0.1]), method='Powell')\n",
    "    result = minimize_scalar(opt_, bounds=(0, 0.5), method='bounded')\n",
    "    \n",
    "    best_threshold = result['x'].item()\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4c60009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_confusion_matrix(\n",
    "        y_true,\n",
    "        pred_label,\n",
    "        height=.6,\n",
    "        labels=None):\n",
    "    \"\"\"混合行列をプロット \n",
    "    (https://www.guruguru.science/competitions/11/discussions/2fb11851-67d0-4e96-a4b1-5629b944f363/)\"\"\"\n",
    "    \n",
    "    conf = confusion_matrix(y_true=y_true,\n",
    "                            y_pred=pred_label,\n",
    "                            normalize='true')\n",
    "\n",
    "    n_labels = len(conf)\n",
    "    size = n_labels * height\n",
    "    fig, ax = plt.subplots(figsize=(size * 4, size * 3))\n",
    "    sns.heatmap(conf, cmap='Blues', ax=ax, annot=True, fmt='.2f')\n",
    "    ax.set_ylabel('Label')\n",
    "    ax.set_xlabel('Predict')\n",
    "\n",
    "    if labels is not None:\n",
    "        ax.set_yticklabels(labels)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.tick_params('y', labelrotation=0)\n",
    "        ax.tick_params('x', labelrotation=90)\n",
    "\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7d7b360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=*========= fold: 0 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/4343] Elapsed 0m 0s (remain 47m 44s) Loss: 0.7081 \n",
      "Epoch: [1][100/4343] Elapsed 0m 10s (remain 7m 20s) Loss: 0.1202 \n",
      "Epoch: [1][200/4343] Elapsed 0m 20s (remain 6m 54s) Loss: 0.1147 \n",
      "Epoch: [1][300/4343] Elapsed 0m 29s (remain 6m 39s) Loss: 0.1171 \n",
      "Epoch: [1][400/4343] Elapsed 0m 39s (remain 6m 28s) Loss: 0.1089 \n",
      "Epoch: [1][500/4343] Elapsed 0m 49s (remain 6m 16s) Loss: 0.1028 \n",
      "Epoch: [1][600/4343] Elapsed 0m 58s (remain 6m 5s) Loss: 0.1003 \n",
      "Epoch: [1][700/4343] Elapsed 1m 8s (remain 5m 54s) Loss: 0.0951 \n",
      "Epoch: [1][800/4343] Elapsed 1m 17s (remain 5m 44s) Loss: 0.0932 \n",
      "Epoch: [1][900/4343] Elapsed 1m 27s (remain 5m 34s) Loss: 0.0933 \n",
      "Epoch: [1][1000/4343] Elapsed 1m 37s (remain 5m 24s) Loss: 0.0894 \n",
      "Epoch: [1][1100/4343] Elapsed 1m 46s (remain 5m 14s) Loss: 0.0868 \n",
      "Epoch: [1][1200/4343] Elapsed 1m 56s (remain 5m 4s) Loss: 0.0863 \n",
      "Epoch: [1][1300/4343] Elapsed 2m 6s (remain 4m 54s) Loss: 0.0865 \n",
      "Epoch: [1][1400/4343] Elapsed 2m 15s (remain 4m 45s) Loss: 0.0871 \n",
      "Epoch: [1][1500/4343] Elapsed 2m 25s (remain 4m 35s) Loss: 0.0859 \n",
      "Epoch: [1][1600/4343] Elapsed 2m 34s (remain 4m 25s) Loss: 0.0854 \n",
      "Epoch: [1][1700/4343] Elapsed 2m 44s (remain 4m 15s) Loss: 0.0843 \n",
      "Epoch: [1][1800/4343] Elapsed 2m 54s (remain 4m 5s) Loss: 0.0818 \n",
      "Epoch: [1][1900/4343] Elapsed 3m 3s (remain 3m 55s) Loss: 0.0809 \n",
      "Epoch: [1][2000/4343] Elapsed 3m 13s (remain 3m 46s) Loss: 0.0806 \n",
      "Epoch: [1][2100/4343] Elapsed 3m 22s (remain 3m 36s) Loss: 0.0797 \n",
      "Epoch: [1][2200/4343] Elapsed 3m 32s (remain 3m 26s) Loss: 0.0787 \n",
      "Epoch: [1][2300/4343] Elapsed 3m 42s (remain 3m 17s) Loss: 0.0769 \n",
      "Epoch: [1][2400/4343] Elapsed 3m 51s (remain 3m 7s) Loss: 0.0770 \n",
      "Epoch: [1][2500/4343] Elapsed 4m 1s (remain 2m 57s) Loss: 0.0756 \n",
      "Epoch: [1][2600/4343] Elapsed 4m 10s (remain 2m 48s) Loss: 0.0757 \n",
      "Epoch: [1][2700/4343] Elapsed 4m 20s (remain 2m 38s) Loss: 0.0748 \n",
      "Epoch: [1][2800/4343] Elapsed 4m 30s (remain 2m 28s) Loss: 0.0737 \n",
      "Epoch: [1][2900/4343] Elapsed 4m 39s (remain 2m 18s) Loss: 0.0723 \n",
      "Epoch: [1][3000/4343] Elapsed 4m 49s (remain 2m 9s) Loss: 0.0716 \n",
      "Epoch: [1][3100/4343] Elapsed 4m 58s (remain 1m 59s) Loss: 0.0707 \n",
      "Epoch: [1][3200/4343] Elapsed 5m 8s (remain 1m 50s) Loss: 0.0696 \n",
      "Epoch: [1][3300/4343] Elapsed 5m 17s (remain 1m 40s) Loss: 0.0690 \n",
      "Epoch: [1][3400/4343] Elapsed 5m 27s (remain 1m 30s) Loss: 0.0686 \n",
      "Epoch: [1][3500/4343] Elapsed 5m 37s (remain 1m 21s) Loss: 0.0677 \n",
      "Epoch: [1][3600/4343] Elapsed 5m 46s (remain 1m 11s) Loss: 0.0677 \n",
      "Epoch: [1][3700/4343] Elapsed 5m 56s (remain 1m 1s) Loss: 0.0673 \n",
      "Epoch: [1][3800/4343] Elapsed 6m 6s (remain 0m 52s) Loss: 0.0679 \n",
      "Epoch: [1][3900/4343] Elapsed 6m 15s (remain 0m 42s) Loss: 0.0671 \n",
      "Epoch: [1][4000/4343] Elapsed 6m 25s (remain 0m 32s) Loss: 0.0671 \n",
      "Epoch: [1][4100/4343] Elapsed 6m 34s (remain 0m 23s) Loss: 0.0664 \n",
      "Epoch: [1][4200/4343] Elapsed 6m 44s (remain 0m 13s) Loss: 0.0657 \n",
      "Epoch: [1][4300/4343] Elapsed 6m 54s (remain 0m 4s) Loss: 0.0658 \n",
      "Epoch: [1][4342/4343] Elapsed 6m 58s (remain 0m 0s) Loss: 0.0657 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 2m 54s) Loss: 0.0011 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.0520 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.0590 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0480 \n",
      "EVAL: [400/1086] Elapsed 0m 10s (remain 0m 17s) Loss: 0.0557 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.0495 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 12s) Loss: 0.0534 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0508 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0529 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.0539 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.0513 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8281249999999999, thresh=0.023282372444280715\n",
      "score2 = 0.8670072766682149,  thresh=0.0132469846281435\n",
      "Epoch 1 - avg_train_loss: 0.0657  avg_val_loss: 0.0508  time: 445s\n",
      "Epoch 1 - Score: 0.8670072766682149\n",
      "Epoch 1 - Save Best Score: 0.8670 - Best Loss: 0.0508 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 27s (remain 0m 0s) Loss: 0.0508 \n",
      "Epoch: [2][0/4343] Elapsed 0m 0s (remain 18m 49s) Loss: 0.0014 \n",
      "Epoch: [2][100/4343] Elapsed 0m 9s (remain 6m 54s) Loss: 0.0287 \n",
      "Epoch: [2][200/4343] Elapsed 0m 19s (remain 6m 46s) Loss: 0.0394 \n",
      "Epoch: [2][300/4343] Elapsed 0m 29s (remain 6m 34s) Loss: 0.0397 \n",
      "Epoch: [2][400/4343] Elapsed 0m 38s (remain 6m 22s) Loss: 0.0384 \n",
      "Epoch: [2][500/4343] Elapsed 0m 48s (remain 6m 12s) Loss: 0.0415 \n",
      "Epoch: [2][600/4343] Elapsed 0m 58s (remain 6m 2s) Loss: 0.0417 \n",
      "Epoch: [2][700/4343] Elapsed 1m 7s (remain 5m 52s) Loss: 0.0403 \n",
      "Epoch: [2][800/4343] Elapsed 1m 17s (remain 5m 42s) Loss: 0.0433 \n",
      "Epoch: [2][900/4343] Elapsed 1m 26s (remain 5m 32s) Loss: 0.0416 \n",
      "Epoch: [2][1000/4343] Elapsed 1m 36s (remain 5m 22s) Loss: 0.0415 \n",
      "Epoch: [2][1100/4343] Elapsed 1m 46s (remain 5m 12s) Loss: 0.0406 \n",
      "Epoch: [2][1200/4343] Elapsed 1m 55s (remain 5m 2s) Loss: 0.0418 \n",
      "Epoch: [2][1300/4343] Elapsed 2m 5s (remain 4m 53s) Loss: 0.0404 \n",
      "Epoch: [2][1400/4343] Elapsed 2m 15s (remain 4m 43s) Loss: 0.0403 \n",
      "Epoch: [2][1500/4343] Elapsed 2m 24s (remain 4m 33s) Loss: 0.0410 \n",
      "Epoch: [2][1600/4343] Elapsed 2m 34s (remain 4m 24s) Loss: 0.0402 \n",
      "Epoch: [2][1700/4343] Elapsed 2m 43s (remain 4m 14s) Loss: 0.0384 \n",
      "Epoch: [2][1800/4343] Elapsed 2m 53s (remain 4m 4s) Loss: 0.0387 \n",
      "Epoch: [2][1900/4343] Elapsed 3m 2s (remain 3m 55s) Loss: 0.0392 \n",
      "Epoch: [2][2000/4343] Elapsed 3m 12s (remain 3m 45s) Loss: 0.0397 \n",
      "Epoch: [2][2100/4343] Elapsed 3m 22s (remain 3m 35s) Loss: 0.0401 \n",
      "Epoch: [2][2200/4343] Elapsed 3m 31s (remain 3m 26s) Loss: 0.0397 \n",
      "Epoch: [2][2300/4343] Elapsed 3m 41s (remain 3m 16s) Loss: 0.0395 \n",
      "Epoch: [2][2400/4343] Elapsed 3m 50s (remain 3m 6s) Loss: 0.0399 \n",
      "Epoch: [2][2500/4343] Elapsed 4m 0s (remain 2m 57s) Loss: 0.0403 \n",
      "Epoch: [2][2600/4343] Elapsed 4m 10s (remain 2m 47s) Loss: 0.0397 \n",
      "Epoch: [2][2700/4343] Elapsed 4m 19s (remain 2m 37s) Loss: 0.0390 \n",
      "Epoch: [2][2800/4343] Elapsed 4m 29s (remain 2m 28s) Loss: 0.0385 \n",
      "Epoch: [2][2900/4343] Elapsed 4m 38s (remain 2m 18s) Loss: 0.0390 \n",
      "Epoch: [2][3000/4343] Elapsed 4m 48s (remain 2m 8s) Loss: 0.0383 \n",
      "Epoch: [2][3100/4343] Elapsed 4m 57s (remain 1m 59s) Loss: 0.0388 \n",
      "Epoch: [2][3200/4343] Elapsed 5m 7s (remain 1m 49s) Loss: 0.0388 \n",
      "Epoch: [2][3300/4343] Elapsed 5m 17s (remain 1m 40s) Loss: 0.0385 \n",
      "Epoch: [2][3400/4343] Elapsed 5m 26s (remain 1m 30s) Loss: 0.0387 \n",
      "Epoch: [2][3500/4343] Elapsed 5m 36s (remain 1m 20s) Loss: 0.0389 \n",
      "Epoch: [2][3600/4343] Elapsed 5m 45s (remain 1m 11s) Loss: 0.0384 \n",
      "Epoch: [2][3700/4343] Elapsed 5m 55s (remain 1m 1s) Loss: 0.0382 \n",
      "Epoch: [2][3800/4343] Elapsed 6m 5s (remain 0m 52s) Loss: 0.0379 \n",
      "Epoch: [2][3900/4343] Elapsed 6m 14s (remain 0m 42s) Loss: 0.0379 \n",
      "Epoch: [2][4000/4343] Elapsed 6m 24s (remain 0m 32s) Loss: 0.0382 \n",
      "Epoch: [2][4100/4343] Elapsed 6m 33s (remain 0m 23s) Loss: 0.0382 \n",
      "Epoch: [2][4200/4343] Elapsed 6m 43s (remain 0m 13s) Loss: 0.0390 \n",
      "Epoch: [2][4300/4343] Elapsed 6m 52s (remain 0m 4s) Loss: 0.0385 \n",
      "Epoch: [2][4342/4343] Elapsed 6m 56s (remain 0m 0s) Loss: 0.0384 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 2m 53s) Loss: 0.0011 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.0378 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.0429 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0360 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.0395 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.0370 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0431 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0449 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0476 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.0479 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.0444 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8556316116988177, thresh=0.023282372444280715\n",
      "score2 = 0.8792225821378991,  thresh=0.010550906942656556\n",
      "Epoch 2 - avg_train_loss: 0.0384  avg_val_loss: 0.0449  time: 444s\n",
      "Epoch 2 - Score: 0.8792225821378991\n",
      "Epoch 2 - Save Best Score: 0.8792 - Best Loss: 0.0449 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.0449 \n",
      "Epoch: [3][0/4343] Elapsed 0m 0s (remain 19m 9s) Loss: 0.0019 \n",
      "Epoch: [3][100/4343] Elapsed 0m 9s (remain 6m 55s) Loss: 0.0184 \n",
      "Epoch: [3][200/4343] Elapsed 0m 19s (remain 6m 42s) Loss: 0.0126 \n",
      "Epoch: [3][300/4343] Elapsed 0m 29s (remain 6m 31s) Loss: 0.0181 \n",
      "Epoch: [3][400/4343] Elapsed 0m 38s (remain 6m 21s) Loss: 0.0194 \n",
      "Epoch: [3][500/4343] Elapsed 0m 48s (remain 6m 11s) Loss: 0.0228 \n",
      "Epoch: [3][600/4343] Elapsed 0m 58s (remain 6m 1s) Loss: 0.0220 \n",
      "Epoch: [3][700/4343] Elapsed 1m 7s (remain 5m 51s) Loss: 0.0222 \n",
      "Epoch: [3][800/4343] Elapsed 1m 17s (remain 5m 41s) Loss: 0.0228 \n",
      "Epoch: [3][900/4343] Elapsed 1m 26s (remain 5m 32s) Loss: 0.0226 \n",
      "Epoch: [3][1000/4343] Elapsed 1m 36s (remain 5m 22s) Loss: 0.0239 \n",
      "Epoch: [3][1100/4343] Elapsed 1m 46s (remain 5m 12s) Loss: 0.0241 \n",
      "Epoch: [3][1200/4343] Elapsed 1m 55s (remain 5m 2s) Loss: 0.0235 \n",
      "Epoch: [3][1300/4343] Elapsed 2m 5s (remain 4m 53s) Loss: 0.0231 \n",
      "Epoch: [3][1400/4343] Elapsed 2m 15s (remain 4m 43s) Loss: 0.0228 \n",
      "Epoch: [3][1500/4343] Elapsed 2m 24s (remain 4m 33s) Loss: 0.0232 \n",
      "Epoch: [3][1600/4343] Elapsed 2m 34s (remain 4m 24s) Loss: 0.0247 \n",
      "Epoch: [3][1700/4343] Elapsed 2m 43s (remain 4m 14s) Loss: 0.0248 \n",
      "Epoch: [3][1800/4343] Elapsed 2m 53s (remain 4m 4s) Loss: 0.0251 \n",
      "Epoch: [3][1900/4343] Elapsed 3m 3s (remain 3m 55s) Loss: 0.0260 \n",
      "Epoch: [3][2000/4343] Elapsed 3m 12s (remain 3m 45s) Loss: 0.0258 \n",
      "Epoch: [3][2100/4343] Elapsed 3m 22s (remain 3m 36s) Loss: 0.0256 \n",
      "Epoch: [3][2200/4343] Elapsed 3m 32s (remain 3m 26s) Loss: 0.0254 \n",
      "Epoch: [3][2300/4343] Elapsed 3m 41s (remain 3m 16s) Loss: 0.0263 \n",
      "Epoch: [3][2400/4343] Elapsed 3m 51s (remain 3m 7s) Loss: 0.0267 \n",
      "Epoch: [3][2500/4343] Elapsed 4m 0s (remain 2m 57s) Loss: 0.0271 \n",
      "Epoch: [3][2600/4343] Elapsed 4m 10s (remain 2m 47s) Loss: 0.0270 \n",
      "Epoch: [3][2700/4343] Elapsed 4m 20s (remain 2m 38s) Loss: 0.0270 \n",
      "Epoch: [3][2800/4343] Elapsed 4m 29s (remain 2m 28s) Loss: 0.0276 \n",
      "Epoch: [3][2900/4343] Elapsed 4m 39s (remain 2m 18s) Loss: 0.0271 \n",
      "Epoch: [3][3000/4343] Elapsed 4m 49s (remain 2m 9s) Loss: 0.0265 \n",
      "Epoch: [3][3100/4343] Elapsed 4m 58s (remain 1m 59s) Loss: 0.0264 \n",
      "Epoch: [3][3200/4343] Elapsed 5m 8s (remain 1m 50s) Loss: 0.0265 \n",
      "Epoch: [3][3300/4343] Elapsed 5m 17s (remain 1m 40s) Loss: 0.0263 \n",
      "Epoch: [3][3400/4343] Elapsed 5m 27s (remain 1m 30s) Loss: 0.0260 \n",
      "Epoch: [3][3500/4343] Elapsed 5m 37s (remain 1m 21s) Loss: 0.0266 \n",
      "Epoch: [3][3600/4343] Elapsed 5m 46s (remain 1m 11s) Loss: 0.0269 \n",
      "Epoch: [3][3700/4343] Elapsed 5m 56s (remain 1m 1s) Loss: 0.0265 \n",
      "Epoch: [3][3800/4343] Elapsed 6m 6s (remain 0m 52s) Loss: 0.0272 \n",
      "Epoch: [3][3900/4343] Elapsed 6m 15s (remain 0m 42s) Loss: 0.0281 \n",
      "Epoch: [3][4000/4343] Elapsed 6m 25s (remain 0m 32s) Loss: 0.0281 \n",
      "Epoch: [3][4100/4343] Elapsed 6m 34s (remain 0m 23s) Loss: 0.0276 \n",
      "Epoch: [3][4200/4343] Elapsed 6m 44s (remain 0m 13s) Loss: 0.0274 \n",
      "Epoch: [3][4300/4343] Elapsed 6m 54s (remain 0m 4s) Loss: 0.0273 \n",
      "Epoch: [3][4342/4343] Elapsed 6m 58s (remain 0m 0s) Loss: 0.0271 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 2m 54s) Loss: 0.0003 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.0272 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.0597 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0442 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.0490 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.0515 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0609 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0596 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0606 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.0621 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.0582 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8354153653966273, thresh=0.023282372444280715\n",
      "score2 = 0.8315029808597426,  thresh=0.10137670235796244\n",
      "Epoch 3 - avg_train_loss: 0.0271  avg_val_loss: 0.0586  time: 445s\n",
      "Epoch 3 - Score: 0.8315029808597426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.0586 \n",
      "Epoch: [4][0/4343] Elapsed 0m 0s (remain 16m 35s) Loss: 0.0003 \n",
      "Epoch: [4][100/4343] Elapsed 0m 9s (remain 6m 53s) Loss: 0.0082 \n",
      "Epoch: [4][200/4343] Elapsed 0m 19s (remain 6m 41s) Loss: 0.0149 \n",
      "Epoch: [4][300/4343] Elapsed 0m 29s (remain 6m 30s) Loss: 0.0145 \n",
      "Epoch: [4][400/4343] Elapsed 0m 38s (remain 6m 20s) Loss: 0.0126 \n",
      "Epoch: [4][500/4343] Elapsed 0m 48s (remain 6m 10s) Loss: 0.0138 \n",
      "Epoch: [4][600/4343] Elapsed 0m 57s (remain 6m 0s) Loss: 0.0173 \n",
      "Epoch: [4][700/4343] Elapsed 1m 7s (remain 5m 51s) Loss: 0.0167 \n",
      "Epoch: [4][800/4343] Elapsed 1m 17s (remain 5m 41s) Loss: 0.0168 \n",
      "Epoch: [4][900/4343] Elapsed 1m 26s (remain 5m 31s) Loss: 0.0164 \n",
      "Epoch: [4][1000/4343] Elapsed 1m 36s (remain 5m 21s) Loss: 0.0168 \n",
      "Epoch: [4][1100/4343] Elapsed 1m 46s (remain 5m 12s) Loss: 0.0165 \n",
      "Epoch: [4][1200/4343] Elapsed 1m 55s (remain 5m 2s) Loss: 0.0174 \n",
      "Epoch: [4][1300/4343] Elapsed 2m 5s (remain 4m 52s) Loss: 0.0181 \n",
      "Epoch: [4][1400/4343] Elapsed 2m 14s (remain 4m 43s) Loss: 0.0170 \n",
      "Epoch: [4][1500/4343] Elapsed 2m 24s (remain 4m 33s) Loss: 0.0180 \n",
      "Epoch: [4][1600/4343] Elapsed 2m 34s (remain 4m 23s) Loss: 0.0185 \n",
      "Epoch: [4][1700/4343] Elapsed 2m 43s (remain 4m 14s) Loss: 0.0182 \n",
      "Epoch: [4][1800/4343] Elapsed 2m 53s (remain 4m 4s) Loss: 0.0181 \n",
      "Epoch: [4][1900/4343] Elapsed 3m 2s (remain 3m 55s) Loss: 0.0179 \n",
      "Epoch: [4][2000/4343] Elapsed 3m 12s (remain 3m 45s) Loss: 0.0180 \n",
      "Epoch: [4][2100/4343] Elapsed 3m 22s (remain 3m 35s) Loss: 0.0179 \n",
      "Epoch: [4][2200/4343] Elapsed 3m 31s (remain 3m 26s) Loss: 0.0177 \n",
      "Epoch: [4][2300/4343] Elapsed 3m 41s (remain 3m 16s) Loss: 0.0180 \n",
      "Epoch: [4][2400/4343] Elapsed 3m 51s (remain 3m 6s) Loss: 0.0181 \n",
      "Epoch: [4][2500/4343] Elapsed 4m 0s (remain 2m 57s) Loss: 0.0180 \n",
      "Epoch: [4][2600/4343] Elapsed 4m 10s (remain 2m 47s) Loss: 0.0179 \n",
      "Epoch: [4][2700/4343] Elapsed 4m 19s (remain 2m 37s) Loss: 0.0177 \n",
      "Epoch: [4][2800/4343] Elapsed 4m 29s (remain 2m 28s) Loss: 0.0178 \n",
      "Epoch: [4][2900/4343] Elapsed 4m 39s (remain 2m 18s) Loss: 0.0182 \n",
      "Epoch: [4][3000/4343] Elapsed 4m 48s (remain 2m 9s) Loss: 0.0186 \n",
      "Epoch: [4][3100/4343] Elapsed 4m 58s (remain 1m 59s) Loss: 0.0182 \n",
      "Epoch: [4][3200/4343] Elapsed 5m 7s (remain 1m 49s) Loss: 0.0189 \n",
      "Epoch: [4][3300/4343] Elapsed 5m 17s (remain 1m 40s) Loss: 0.0191 \n",
      "Epoch: [4][3400/4343] Elapsed 5m 27s (remain 1m 30s) Loss: 0.0188 \n",
      "Epoch: [4][3500/4343] Elapsed 5m 36s (remain 1m 21s) Loss: 0.0183 \n",
      "Epoch: [4][3600/4343] Elapsed 5m 46s (remain 1m 11s) Loss: 0.0183 \n",
      "Epoch: [4][3700/4343] Elapsed 5m 56s (remain 1m 1s) Loss: 0.0179 \n",
      "Epoch: [4][3800/4343] Elapsed 6m 5s (remain 0m 52s) Loss: 0.0177 \n",
      "Epoch: [4][3900/4343] Elapsed 6m 15s (remain 0m 42s) Loss: 0.0179 \n",
      "Epoch: [4][4000/4343] Elapsed 6m 24s (remain 0m 32s) Loss: 0.0180 \n",
      "Epoch: [4][4100/4343] Elapsed 6m 34s (remain 0m 23s) Loss: 0.0182 \n",
      "Epoch: [4][4200/4343] Elapsed 6m 44s (remain 0m 13s) Loss: 0.0182 \n",
      "Epoch: [4][4300/4343] Elapsed 6m 53s (remain 0m 4s) Loss: 0.0188 \n",
      "Epoch: [4][4342/4343] Elapsed 6m 57s (remain 0m 0s) Loss: 0.0187 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 2m 58s) Loss: 0.0011 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.0381 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.0457 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0372 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.0441 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.0408 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0507 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0519 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0548 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.0563 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.0521 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8137715179968701, thresh=0.023282372444280715\n",
      "score2 = 0.8658666261582865,  thresh=0.0018956184300904487\n",
      "Epoch 4 - avg_train_loss: 0.0187  avg_val_loss: 0.0520  time: 445s\n",
      "Epoch 4 - Score: 0.8658666261582865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.0520 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 result ==========\n",
      "Score1: 0.85563\n",
      "best border: 0.01055\n",
      "Score2: 0.87922\n",
      "=*========= fold: 1 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/4343] Elapsed 0m 0s (remain 18m 0s) Loss: 0.7411 \n",
      "Epoch: [1][100/4343] Elapsed 0m 9s (remain 6m 51s) Loss: 0.1548 \n",
      "Epoch: [1][200/4343] Elapsed 0m 19s (remain 6m 38s) Loss: 0.1355 \n",
      "Epoch: [1][300/4343] Elapsed 0m 28s (remain 6m 28s) Loss: 0.1205 \n",
      "Epoch: [1][400/4343] Elapsed 0m 38s (remain 6m 18s) Loss: 0.1135 \n",
      "Epoch: [1][500/4343] Elapsed 0m 48s (remain 6m 8s) Loss: 0.1053 \n",
      "Epoch: [1][600/4343] Elapsed 0m 57s (remain 5m 59s) Loss: 0.1013 \n",
      "Epoch: [1][700/4343] Elapsed 1m 7s (remain 5m 49s) Loss: 0.0973 \n",
      "Epoch: [1][800/4343] Elapsed 1m 16s (remain 5m 39s) Loss: 0.0920 \n",
      "Epoch: [1][900/4343] Elapsed 1m 26s (remain 5m 30s) Loss: 0.0922 \n",
      "Epoch: [1][1000/4343] Elapsed 1m 36s (remain 5m 20s) Loss: 0.0887 \n",
      "Epoch: [1][1100/4343] Elapsed 1m 45s (remain 5m 11s) Loss: 0.0867 \n",
      "Epoch: [1][1200/4343] Elapsed 1m 55s (remain 5m 1s) Loss: 0.0855 \n",
      "Epoch: [1][1300/4343] Elapsed 2m 4s (remain 4m 52s) Loss: 0.0823 \n",
      "Epoch: [1][1400/4343] Elapsed 2m 14s (remain 4m 42s) Loss: 0.0809 \n",
      "Epoch: [1][1500/4343] Elapsed 2m 24s (remain 4m 33s) Loss: 0.0805 \n",
      "Epoch: [1][1600/4343] Elapsed 2m 33s (remain 4m 23s) Loss: 0.0790 \n",
      "Epoch: [1][1700/4343] Elapsed 2m 43s (remain 4m 13s) Loss: 0.0790 \n",
      "Epoch: [1][1800/4343] Elapsed 2m 53s (remain 4m 4s) Loss: 0.0778 \n",
      "Epoch: [1][1900/4343] Elapsed 3m 2s (remain 3m 54s) Loss: 0.0759 \n",
      "Epoch: [1][2000/4343] Elapsed 3m 12s (remain 3m 45s) Loss: 0.0742 \n",
      "Epoch: [1][2100/4343] Elapsed 3m 22s (remain 3m 35s) Loss: 0.0744 \n",
      "Epoch: [1][2200/4343] Elapsed 3m 31s (remain 3m 25s) Loss: 0.0732 \n",
      "Epoch: [1][2300/4343] Elapsed 3m 41s (remain 3m 16s) Loss: 0.0735 \n",
      "Epoch: [1][2400/4343] Elapsed 3m 50s (remain 3m 6s) Loss: 0.0725 \n",
      "Epoch: [1][2500/4343] Elapsed 4m 0s (remain 2m 57s) Loss: 0.0704 \n",
      "Epoch: [1][2600/4343] Elapsed 4m 10s (remain 2m 47s) Loss: 0.0698 \n",
      "Epoch: [1][2700/4343] Elapsed 4m 19s (remain 2m 37s) Loss: 0.0704 \n",
      "Epoch: [1][2800/4343] Elapsed 4m 29s (remain 2m 28s) Loss: 0.0690 \n",
      "Epoch: [1][2900/4343] Elapsed 4m 39s (remain 2m 18s) Loss: 0.0683 \n",
      "Epoch: [1][3000/4343] Elapsed 4m 48s (remain 2m 9s) Loss: 0.0681 \n",
      "Epoch: [1][3100/4343] Elapsed 4m 58s (remain 1m 59s) Loss: 0.0666 \n",
      "Epoch: [1][3200/4343] Elapsed 5m 7s (remain 1m 49s) Loss: 0.0663 \n",
      "Epoch: [1][3300/4343] Elapsed 5m 17s (remain 1m 40s) Loss: 0.0655 \n",
      "Epoch: [1][3400/4343] Elapsed 5m 27s (remain 1m 30s) Loss: 0.0660 \n",
      "Epoch: [1][3500/4343] Elapsed 5m 36s (remain 1m 21s) Loss: 0.0662 \n",
      "Epoch: [1][3600/4343] Elapsed 5m 46s (remain 1m 11s) Loss: 0.0652 \n",
      "Epoch: [1][3700/4343] Elapsed 5m 56s (remain 1m 1s) Loss: 0.0648 \n",
      "Epoch: [1][3800/4343] Elapsed 6m 5s (remain 0m 52s) Loss: 0.0644 \n",
      "Epoch: [1][3900/4343] Elapsed 6m 15s (remain 0m 42s) Loss: 0.0651 \n",
      "Epoch: [1][4000/4343] Elapsed 6m 24s (remain 0m 32s) Loss: 0.0649 \n",
      "Epoch: [1][4100/4343] Elapsed 6m 34s (remain 0m 23s) Loss: 0.0644 \n",
      "Epoch: [1][4200/4343] Elapsed 6m 44s (remain 0m 13s) Loss: 0.0642 \n",
      "Epoch: [1][4300/4343] Elapsed 6m 53s (remain 0m 4s) Loss: 0.0639 \n",
      "Epoch: [1][4342/4343] Elapsed 6m 57s (remain 0m 0s) Loss: 0.0637 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 10s) Loss: 0.0054 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.0535 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.0459 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0483 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.0530 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.0478 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0467 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0449 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0439 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.0448 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.0466 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.9005019191024505, thresh=0.023282372444280715\n",
      "score2 = 0.8469403326720586,  thresh=0.058920625865344925\n",
      "Epoch 1 - avg_train_loss: 0.0637  avg_val_loss: 0.0467  time: 445s\n",
      "Epoch 1 - Score: 0.8469403326720586\n",
      "Epoch 1 - Save Best Score: 0.8469 - Best Loss: 0.0467 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.0467 \n",
      "Epoch: [2][0/4343] Elapsed 0m 0s (remain 17m 29s) Loss: 0.0397 \n",
      "Epoch: [2][100/4343] Elapsed 0m 9s (remain 6m 54s) Loss: 0.0457 \n",
      "Epoch: [2][200/4343] Elapsed 0m 19s (remain 6m 41s) Loss: 0.0401 \n",
      "Epoch: [2][300/4343] Elapsed 0m 29s (remain 6m 30s) Loss: 0.0383 \n",
      "Epoch: [2][400/4343] Elapsed 0m 38s (remain 6m 20s) Loss: 0.0368 \n",
      "Epoch: [2][500/4343] Elapsed 0m 48s (remain 6m 10s) Loss: 0.0377 \n",
      "Epoch: [2][600/4343] Elapsed 0m 57s (remain 6m 0s) Loss: 0.0367 \n",
      "Epoch: [2][700/4343] Elapsed 1m 7s (remain 5m 51s) Loss: 0.0361 \n",
      "Epoch: [2][800/4343] Elapsed 1m 17s (remain 5m 41s) Loss: 0.0368 \n",
      "Epoch: [2][900/4343] Elapsed 1m 26s (remain 5m 31s) Loss: 0.0376 \n",
      "Epoch: [2][1000/4343] Elapsed 1m 36s (remain 5m 21s) Loss: 0.0381 \n",
      "Epoch: [2][1100/4343] Elapsed 1m 46s (remain 5m 12s) Loss: 0.0401 \n",
      "Epoch: [2][1200/4343] Elapsed 1m 55s (remain 5m 2s) Loss: 0.0394 \n",
      "Epoch: [2][1300/4343] Elapsed 2m 5s (remain 4m 52s) Loss: 0.0393 \n",
      "Epoch: [2][1400/4343] Elapsed 2m 14s (remain 4m 43s) Loss: 0.0384 \n",
      "Epoch: [2][1500/4343] Elapsed 2m 24s (remain 4m 33s) Loss: 0.0381 \n",
      "Epoch: [2][1600/4343] Elapsed 2m 34s (remain 4m 23s) Loss: 0.0381 \n",
      "Epoch: [2][1700/4343] Elapsed 2m 43s (remain 4m 14s) Loss: 0.0385 \n",
      "Epoch: [2][1800/4343] Elapsed 2m 53s (remain 4m 4s) Loss: 0.0377 \n",
      "Epoch: [2][1900/4343] Elapsed 3m 2s (remain 3m 55s) Loss: 0.0364 \n",
      "Epoch: [2][2000/4343] Elapsed 3m 12s (remain 3m 45s) Loss: 0.0362 \n",
      "Epoch: [2][2100/4343] Elapsed 3m 22s (remain 3m 35s) Loss: 0.0362 \n",
      "Epoch: [2][2200/4343] Elapsed 3m 31s (remain 3m 26s) Loss: 0.0362 \n",
      "Epoch: [2][2300/4343] Elapsed 3m 41s (remain 3m 16s) Loss: 0.0364 \n",
      "Epoch: [2][2400/4343] Elapsed 3m 51s (remain 3m 6s) Loss: 0.0367 \n",
      "Epoch: [2][2500/4343] Elapsed 4m 0s (remain 2m 57s) Loss: 0.0362 \n",
      "Epoch: [2][2600/4343] Elapsed 4m 10s (remain 2m 47s) Loss: 0.0367 \n",
      "Epoch: [2][2700/4343] Elapsed 4m 19s (remain 2m 38s) Loss: 0.0365 \n",
      "Epoch: [2][2800/4343] Elapsed 4m 29s (remain 2m 28s) Loss: 0.0358 \n",
      "Epoch: [2][2900/4343] Elapsed 4m 39s (remain 2m 18s) Loss: 0.0352 \n",
      "Epoch: [2][3000/4343] Elapsed 4m 48s (remain 2m 9s) Loss: 0.0351 \n",
      "Epoch: [2][3100/4343] Elapsed 4m 58s (remain 1m 59s) Loss: 0.0352 \n",
      "Epoch: [2][3200/4343] Elapsed 5m 8s (remain 1m 49s) Loss: 0.0352 \n",
      "Epoch: [2][3300/4343] Elapsed 5m 17s (remain 1m 40s) Loss: 0.0351 \n",
      "Epoch: [2][3400/4343] Elapsed 5m 27s (remain 1m 30s) Loss: 0.0352 \n",
      "Epoch: [2][3500/4343] Elapsed 5m 36s (remain 1m 21s) Loss: 0.0351 \n",
      "Epoch: [2][3600/4343] Elapsed 5m 46s (remain 1m 11s) Loss: 0.0354 \n",
      "Epoch: [2][3700/4343] Elapsed 5m 56s (remain 1m 1s) Loss: 0.0357 \n",
      "Epoch: [2][3800/4343] Elapsed 6m 5s (remain 0m 52s) Loss: 0.0356 \n",
      "Epoch: [2][3900/4343] Elapsed 6m 15s (remain 0m 42s) Loss: 0.0353 \n",
      "Epoch: [2][4000/4343] Elapsed 6m 24s (remain 0m 32s) Loss: 0.0349 \n",
      "Epoch: [2][4100/4343] Elapsed 6m 34s (remain 0m 23s) Loss: 0.0354 \n",
      "Epoch: [2][4200/4343] Elapsed 6m 44s (remain 0m 13s) Loss: 0.0360 \n",
      "Epoch: [2][4300/4343] Elapsed 6m 53s (remain 0m 4s) Loss: 0.0358 \n",
      "Epoch: [2][4342/4343] Elapsed 6m 57s (remain 0m 0s) Loss: 0.0355 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 11s) Loss: 0.0006 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.0600 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.0463 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0562 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.0662 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.0599 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0535 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0515 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0493 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.0493 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.0514 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8107682619647355, thresh=0.023282372444280715\n",
      "score2 = 0.8182533438237607,  thresh=0.016057012542063424\n",
      "Epoch 2 - avg_train_loss: 0.0355  avg_val_loss: 0.0508  time: 445s\n",
      "Epoch 2 - Score: 0.8182533438237607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.0508 \n",
      "Epoch: [3][0/4343] Elapsed 0m 0s (remain 17m 33s) Loss: 0.0008 \n",
      "Epoch: [3][100/4343] Elapsed 0m 9s (remain 6m 53s) Loss: 0.0345 \n",
      "Epoch: [3][200/4343] Elapsed 0m 19s (remain 6m 41s) Loss: 0.0267 \n",
      "Epoch: [3][300/4343] Elapsed 0m 29s (remain 6m 30s) Loss: 0.0254 \n",
      "Epoch: [3][400/4343] Elapsed 0m 38s (remain 6m 20s) Loss: 0.0227 \n",
      "Epoch: [3][500/4343] Elapsed 0m 48s (remain 6m 10s) Loss: 0.0249 \n",
      "Epoch: [3][600/4343] Elapsed 0m 57s (remain 6m 0s) Loss: 0.0279 \n",
      "Epoch: [3][700/4343] Elapsed 1m 7s (remain 5m 51s) Loss: 0.0282 \n",
      "Epoch: [3][800/4343] Elapsed 1m 17s (remain 5m 41s) Loss: 0.0271 \n",
      "Epoch: [3][900/4343] Elapsed 1m 26s (remain 5m 31s) Loss: 0.0264 \n",
      "Epoch: [3][1000/4343] Elapsed 1m 36s (remain 5m 21s) Loss: 0.0316 \n",
      "Epoch: [3][1100/4343] Elapsed 1m 46s (remain 5m 12s) Loss: 0.0335 \n",
      "Epoch: [3][1200/4343] Elapsed 1m 55s (remain 5m 2s) Loss: 0.0340 \n",
      "Epoch: [3][1300/4343] Elapsed 2m 5s (remain 4m 52s) Loss: 0.0349 \n",
      "Epoch: [3][1400/4343] Elapsed 2m 14s (remain 4m 43s) Loss: 0.0359 \n",
      "Epoch: [3][1500/4343] Elapsed 2m 24s (remain 4m 33s) Loss: 0.0363 \n",
      "Epoch: [3][1600/4343] Elapsed 2m 34s (remain 4m 23s) Loss: 0.0357 \n",
      "Epoch: [3][1700/4343] Elapsed 2m 43s (remain 4m 14s) Loss: 0.0373 \n",
      "Epoch: [3][1800/4343] Elapsed 2m 53s (remain 4m 4s) Loss: 0.0371 \n",
      "Epoch: [3][1900/4343] Elapsed 3m 2s (remain 3m 55s) Loss: 0.0365 \n",
      "Epoch: [3][2000/4343] Elapsed 3m 12s (remain 3m 45s) Loss: 0.0362 \n",
      "Epoch: [3][2100/4343] Elapsed 3m 22s (remain 3m 35s) Loss: 0.0359 \n",
      "Epoch: [3][2200/4343] Elapsed 3m 31s (remain 3m 26s) Loss: 0.0357 \n",
      "Epoch: [3][2300/4343] Elapsed 3m 41s (remain 3m 16s) Loss: 0.0355 \n",
      "Epoch: [3][2400/4343] Elapsed 3m 51s (remain 3m 6s) Loss: 0.0359 \n",
      "Epoch: [3][2500/4343] Elapsed 4m 0s (remain 2m 57s) Loss: 0.0357 \n",
      "Epoch: [3][2600/4343] Elapsed 4m 10s (remain 2m 47s) Loss: 0.0351 \n",
      "Epoch: [3][2700/4343] Elapsed 4m 19s (remain 2m 37s) Loss: 0.0344 \n",
      "Epoch: [3][2800/4343] Elapsed 4m 29s (remain 2m 28s) Loss: 0.0340 \n",
      "Epoch: [3][2900/4343] Elapsed 4m 39s (remain 2m 18s) Loss: 0.0338 \n",
      "Epoch: [3][3000/4343] Elapsed 4m 48s (remain 2m 9s) Loss: 0.0336 \n",
      "Epoch: [3][3100/4343] Elapsed 4m 58s (remain 1m 59s) Loss: 0.0328 \n",
      "Epoch: [3][3200/4343] Elapsed 5m 8s (remain 1m 49s) Loss: 0.0330 \n",
      "Epoch: [3][3300/4343] Elapsed 5m 17s (remain 1m 40s) Loss: 0.0328 \n",
      "Epoch: [3][3400/4343] Elapsed 5m 27s (remain 1m 30s) Loss: 0.0324 \n",
      "Epoch: [3][3500/4343] Elapsed 5m 36s (remain 1m 21s) Loss: 0.0327 \n",
      "Epoch: [3][3600/4343] Elapsed 5m 46s (remain 1m 11s) Loss: 0.0320 \n",
      "Epoch: [3][3700/4343] Elapsed 5m 56s (remain 1m 1s) Loss: 0.0330 \n",
      "Epoch: [3][3800/4343] Elapsed 6m 5s (remain 0m 52s) Loss: 0.0336 \n",
      "Epoch: [3][3900/4343] Elapsed 6m 15s (remain 0m 42s) Loss: 0.0338 \n",
      "Epoch: [3][4000/4343] Elapsed 6m 24s (remain 0m 32s) Loss: 0.0342 \n",
      "Epoch: [3][4100/4343] Elapsed 6m 34s (remain 0m 23s) Loss: 0.0343 \n",
      "Epoch: [3][4200/4343] Elapsed 6m 44s (remain 0m 13s) Loss: 0.0338 \n",
      "Epoch: [3][4300/4343] Elapsed 6m 53s (remain 0m 4s) Loss: 0.0338 \n",
      "Epoch: [3][4342/4343] Elapsed 6m 57s (remain 0m 0s) Loss: 0.0342 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 12s) Loss: 0.0010 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.1773 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.1612 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.1492 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.1750 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.1580 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.1420 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.1408 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.1358 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.1350 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.1407 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.15246348900657997, thresh=0.023282372444280715\n",
      "score2 = 0.1205206492045637,  thresh=0.17733922702865548\n",
      "Epoch 3 - avg_train_loss: 0.0342  avg_val_loss: 0.1425  time: 445s\n",
      "Epoch 3 - Score: 0.1205206492045637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.1425 \n",
      "Epoch: [4][0/4343] Elapsed 0m 0s (remain 17m 30s) Loss: 0.0021 \n",
      "Epoch: [4][100/4343] Elapsed 0m 9s (remain 6m 53s) Loss: 0.1659 \n",
      "Epoch: [4][200/4343] Elapsed 0m 19s (remain 6m 41s) Loss: 0.1211 \n",
      "Epoch: [4][300/4343] Elapsed 0m 29s (remain 6m 30s) Loss: 0.0904 \n",
      "Epoch: [4][400/4343] Elapsed 0m 38s (remain 6m 20s) Loss: 0.0743 \n",
      "Epoch: [4][500/4343] Elapsed 0m 48s (remain 6m 10s) Loss: 0.0672 \n",
      "Epoch: [4][600/4343] Elapsed 0m 57s (remain 6m 0s) Loss: 0.0616 \n",
      "Epoch: [4][700/4343] Elapsed 1m 7s (remain 5m 50s) Loss: 0.0564 \n",
      "Epoch: [4][800/4343] Elapsed 1m 17s (remain 5m 41s) Loss: 0.0537 \n",
      "Epoch: [4][900/4343] Elapsed 1m 26s (remain 5m 31s) Loss: 0.0515 \n",
      "Epoch: [4][1000/4343] Elapsed 1m 36s (remain 5m 21s) Loss: 0.0508 \n",
      "Epoch: [4][1100/4343] Elapsed 1m 46s (remain 5m 12s) Loss: 0.0486 \n",
      "Epoch: [4][1200/4343] Elapsed 1m 55s (remain 5m 2s) Loss: 0.0484 \n",
      "Epoch: [4][1300/4343] Elapsed 2m 5s (remain 4m 52s) Loss: 0.0510 \n",
      "Epoch: [4][1400/4343] Elapsed 2m 14s (remain 4m 43s) Loss: 0.0525 \n",
      "Epoch: [4][1500/4343] Elapsed 2m 24s (remain 4m 33s) Loss: 0.0542 \n",
      "Epoch: [4][1600/4343] Elapsed 2m 34s (remain 4m 23s) Loss: 0.0545 \n",
      "Epoch: [4][1700/4343] Elapsed 2m 43s (remain 4m 14s) Loss: 0.0538 \n",
      "Epoch: [4][1800/4343] Elapsed 2m 53s (remain 4m 4s) Loss: 0.0544 \n",
      "Epoch: [4][1900/4343] Elapsed 3m 2s (remain 3m 54s) Loss: 0.0533 \n",
      "Epoch: [4][2000/4343] Elapsed 3m 12s (remain 3m 45s) Loss: 0.0525 \n",
      "Epoch: [4][2100/4343] Elapsed 3m 22s (remain 3m 35s) Loss: 0.0514 \n",
      "Epoch: [4][2200/4343] Elapsed 3m 31s (remain 3m 26s) Loss: 0.0499 \n",
      "Epoch: [4][2300/4343] Elapsed 3m 41s (remain 3m 16s) Loss: 0.0492 \n",
      "Epoch: [4][2400/4343] Elapsed 3m 50s (remain 3m 6s) Loss: 0.0485 \n",
      "Epoch: [4][2500/4343] Elapsed 4m 0s (remain 2m 57s) Loss: 0.0476 \n",
      "Epoch: [4][2600/4343] Elapsed 4m 10s (remain 2m 47s) Loss: 0.0471 \n",
      "Epoch: [4][2700/4343] Elapsed 4m 19s (remain 2m 37s) Loss: 0.0461 \n",
      "Epoch: [4][2800/4343] Elapsed 4m 29s (remain 2m 28s) Loss: 0.0451 \n",
      "Epoch: [4][2900/4343] Elapsed 4m 39s (remain 2m 18s) Loss: 0.0447 \n",
      "Epoch: [4][3000/4343] Elapsed 4m 48s (remain 2m 9s) Loss: 0.0435 \n",
      "Epoch: [4][3100/4343] Elapsed 4m 58s (remain 1m 59s) Loss: 0.0457 \n",
      "Epoch: [4][3200/4343] Elapsed 5m 7s (remain 1m 49s) Loss: 0.0456 \n",
      "Epoch: [4][3300/4343] Elapsed 5m 17s (remain 1m 40s) Loss: 0.0462 \n",
      "Epoch: [4][3400/4343] Elapsed 5m 27s (remain 1m 30s) Loss: 0.0464 \n",
      "Epoch: [4][3500/4343] Elapsed 5m 36s (remain 1m 20s) Loss: 0.0462 \n",
      "Epoch: [4][3600/4343] Elapsed 5m 46s (remain 1m 11s) Loss: 0.0462 \n",
      "Epoch: [4][3700/4343] Elapsed 5m 55s (remain 1m 1s) Loss: 0.0463 \n",
      "Epoch: [4][3800/4343] Elapsed 6m 5s (remain 0m 52s) Loss: 0.0463 \n",
      "Epoch: [4][3900/4343] Elapsed 6m 15s (remain 0m 42s) Loss: 0.0459 \n",
      "Epoch: [4][4000/4343] Elapsed 6m 24s (remain 0m 32s) Loss: 0.0455 \n",
      "Epoch: [4][4100/4343] Elapsed 6m 34s (remain 0m 23s) Loss: 0.0450 \n",
      "Epoch: [4][4200/4343] Elapsed 6m 44s (remain 0m 13s) Loss: 0.0450 \n",
      "Epoch: [4][4300/4343] Elapsed 6m 53s (remain 0m 4s) Loss: 0.0453 \n",
      "Epoch: [4][4342/4343] Elapsed 6m 57s (remain 0m 0s) Loss: 0.0450 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 12s) Loss: 0.0009 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.0574 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.0502 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0628 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.0708 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.0606 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0540 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0511 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0500 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.0530 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.0541 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8345031976290749, thresh=0.023282372444280715\n",
      "score2 = 0.8351545426162973,  thresh=0.027870357653506476\n",
      "Epoch 4 - avg_train_loss: 0.0450  avg_val_loss: 0.0527  time: 445s\n",
      "Epoch 4 - Score: 0.8351545426162973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.0527 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 result ==========\n",
      "Score1: 0.90050\n",
      "best border: 0.05892\n",
      "Score2: 0.84694\n",
      "=*========= fold: 2 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/4343] Elapsed 0m 0s (remain 18m 51s) Loss: 0.8152 \n",
      "Epoch: [1][100/4343] Elapsed 0m 9s (remain 6m 51s) Loss: 0.2090 \n",
      "Epoch: [1][200/4343] Elapsed 0m 19s (remain 6m 39s) Loss: 0.1680 \n",
      "Epoch: [1][300/4343] Elapsed 0m 28s (remain 6m 29s) Loss: 0.1361 \n",
      "Epoch: [1][400/4343] Elapsed 0m 38s (remain 6m 19s) Loss: 0.1175 \n",
      "Epoch: [1][500/4343] Elapsed 0m 48s (remain 6m 9s) Loss: 0.1078 \n",
      "Epoch: [1][600/4343] Elapsed 0m 57s (remain 5m 59s) Loss: 0.1014 \n",
      "Epoch: [1][700/4343] Elapsed 1m 7s (remain 5m 49s) Loss: 0.1048 \n",
      "Epoch: [1][800/4343] Elapsed 1m 16s (remain 5m 40s) Loss: 0.0986 \n",
      "Epoch: [1][900/4343] Elapsed 1m 26s (remain 5m 30s) Loss: 0.1022 \n",
      "Epoch: [1][1000/4343] Elapsed 1m 36s (remain 5m 20s) Loss: 0.1008 \n",
      "Epoch: [1][1100/4343] Elapsed 1m 45s (remain 5m 11s) Loss: 0.0981 \n",
      "Epoch: [1][1200/4343] Elapsed 1m 55s (remain 5m 1s) Loss: 0.0976 \n",
      "Epoch: [1][1300/4343] Elapsed 2m 4s (remain 4m 51s) Loss: 0.0942 \n",
      "Epoch: [1][1400/4343] Elapsed 2m 14s (remain 4m 42s) Loss: 0.0935 \n",
      "Epoch: [1][1500/4343] Elapsed 2m 24s (remain 4m 32s) Loss: 0.0920 \n",
      "Epoch: [1][1600/4343] Elapsed 2m 33s (remain 4m 23s) Loss: 0.0909 \n",
      "Epoch: [1][1700/4343] Elapsed 2m 43s (remain 4m 13s) Loss: 0.0883 \n",
      "Epoch: [1][1800/4343] Elapsed 2m 52s (remain 4m 3s) Loss: 0.0876 \n",
      "Epoch: [1][1900/4343] Elapsed 3m 2s (remain 3m 54s) Loss: 0.0880 \n",
      "Epoch: [1][2000/4343] Elapsed 3m 11s (remain 3m 44s) Loss: 0.0865 \n",
      "Epoch: [1][2100/4343] Elapsed 3m 21s (remain 3m 35s) Loss: 0.0856 \n",
      "Epoch: [1][2200/4343] Elapsed 3m 31s (remain 3m 25s) Loss: 0.0835 \n",
      "Epoch: [1][2300/4343] Elapsed 3m 40s (remain 3m 15s) Loss: 0.0835 \n",
      "Epoch: [1][2400/4343] Elapsed 3m 50s (remain 3m 6s) Loss: 0.0827 \n",
      "Epoch: [1][2500/4343] Elapsed 3m 59s (remain 2m 56s) Loss: 0.0815 \n",
      "Epoch: [1][2600/4343] Elapsed 4m 9s (remain 2m 47s) Loss: 0.0816 \n",
      "Epoch: [1][2700/4343] Elapsed 4m 19s (remain 2m 37s) Loss: 0.0803 \n",
      "Epoch: [1][2800/4343] Elapsed 4m 28s (remain 2m 27s) Loss: 0.0795 \n",
      "Epoch: [1][2900/4343] Elapsed 4m 38s (remain 2m 18s) Loss: 0.0789 \n",
      "Epoch: [1][3000/4343] Elapsed 4m 47s (remain 2m 8s) Loss: 0.0781 \n",
      "Epoch: [1][3100/4343] Elapsed 4m 57s (remain 1m 59s) Loss: 0.0778 \n",
      "Epoch: [1][3200/4343] Elapsed 5m 7s (remain 1m 49s) Loss: 0.0764 \n",
      "Epoch: [1][3300/4343] Elapsed 5m 16s (remain 1m 39s) Loss: 0.0767 \n",
      "Epoch: [1][3400/4343] Elapsed 5m 26s (remain 1m 30s) Loss: 0.0751 \n",
      "Epoch: [1][3500/4343] Elapsed 5m 35s (remain 1m 20s) Loss: 0.0744 \n",
      "Epoch: [1][3600/4343] Elapsed 5m 45s (remain 1m 11s) Loss: 0.0738 \n",
      "Epoch: [1][3700/4343] Elapsed 5m 54s (remain 1m 1s) Loss: 0.0728 \n",
      "Epoch: [1][3800/4343] Elapsed 6m 4s (remain 0m 51s) Loss: 0.0724 \n",
      "Epoch: [1][3900/4343] Elapsed 6m 14s (remain 0m 42s) Loss: 0.0722 \n",
      "Epoch: [1][4000/4343] Elapsed 6m 23s (remain 0m 32s) Loss: 0.0709 \n",
      "Epoch: [1][4100/4343] Elapsed 6m 33s (remain 0m 23s) Loss: 0.0704 \n",
      "Epoch: [1][4200/4343] Elapsed 6m 42s (remain 0m 13s) Loss: 0.0696 \n",
      "Epoch: [1][4300/4343] Elapsed 6m 52s (remain 0m 4s) Loss: 0.0687 \n",
      "Epoch: [1][4342/4343] Elapsed 6m 56s (remain 0m 0s) Loss: 0.0687 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 15s) Loss: 0.0011 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.0742 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.0493 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0445 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 17s) Loss: 0.0476 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.0471 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0468 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0520 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0516 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.0528 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.0508 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8255451713395641, thresh=0.023282372444280715\n",
      "score2 = 0.8464047212300049,  thresh=0.019357529155368074\n",
      "Epoch 1 - avg_train_loss: 0.0687  avg_val_loss: 0.0491  time: 443s\n",
      "Epoch 1 - Score: 0.8464047212300049\n",
      "Epoch 1 - Save Best Score: 0.8464 - Best Loss: 0.0491 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.0491 \n",
      "Epoch: [2][0/4343] Elapsed 0m 0s (remain 18m 44s) Loss: 0.0015 \n",
      "Epoch: [2][100/4343] Elapsed 0m 9s (remain 6m 53s) Loss: 0.0426 \n",
      "Epoch: [2][200/4343] Elapsed 0m 19s (remain 6m 40s) Loss: 0.0427 \n",
      "Epoch: [2][300/4343] Elapsed 0m 29s (remain 6m 29s) Loss: 0.0359 \n",
      "Epoch: [2][400/4343] Elapsed 0m 38s (remain 6m 19s) Loss: 0.0365 \n",
      "Epoch: [2][500/4343] Elapsed 0m 48s (remain 6m 9s) Loss: 0.0359 \n",
      "Epoch: [2][600/4343] Elapsed 0m 57s (remain 5m 59s) Loss: 0.0351 \n",
      "Epoch: [2][700/4343] Elapsed 1m 7s (remain 5m 49s) Loss: 0.0348 \n",
      "Epoch: [2][800/4343] Elapsed 1m 16s (remain 5m 40s) Loss: 0.0367 \n",
      "Epoch: [2][900/4343] Elapsed 1m 26s (remain 5m 30s) Loss: 0.0393 \n",
      "Epoch: [2][1000/4343] Elapsed 1m 36s (remain 5m 20s) Loss: 0.0391 \n",
      "Epoch: [2][1100/4343] Elapsed 1m 45s (remain 5m 11s) Loss: 0.0389 \n",
      "Epoch: [2][1200/4343] Elapsed 1m 55s (remain 5m 1s) Loss: 0.0386 \n",
      "Epoch: [2][1300/4343] Elapsed 2m 4s (remain 4m 52s) Loss: 0.0372 \n",
      "Epoch: [2][1400/4343] Elapsed 2m 14s (remain 4m 42s) Loss: 0.0366 \n",
      "Epoch: [2][1500/4343] Elapsed 2m 24s (remain 4m 32s) Loss: 0.0350 \n",
      "Epoch: [2][1600/4343] Elapsed 2m 33s (remain 4m 23s) Loss: 0.0365 \n",
      "Epoch: [2][1700/4343] Elapsed 2m 43s (remain 4m 13s) Loss: 0.0368 \n",
      "Epoch: [2][1800/4343] Elapsed 2m 52s (remain 4m 3s) Loss: 0.0369 \n",
      "Epoch: [2][1900/4343] Elapsed 3m 2s (remain 3m 54s) Loss: 0.0369 \n",
      "Epoch: [2][2000/4343] Elapsed 3m 12s (remain 3m 44s) Loss: 0.0380 \n",
      "Epoch: [2][2100/4343] Elapsed 3m 21s (remain 3m 35s) Loss: 0.0378 \n",
      "Epoch: [2][2200/4343] Elapsed 3m 31s (remain 3m 25s) Loss: 0.0379 \n",
      "Epoch: [2][2300/4343] Elapsed 3m 40s (remain 3m 15s) Loss: 0.0371 \n",
      "Epoch: [2][2400/4343] Elapsed 3m 50s (remain 3m 6s) Loss: 0.0374 \n",
      "Epoch: [2][2500/4343] Elapsed 3m 59s (remain 2m 56s) Loss: 0.0376 \n",
      "Epoch: [2][2600/4343] Elapsed 4m 9s (remain 2m 47s) Loss: 0.0378 \n",
      "Epoch: [2][2700/4343] Elapsed 4m 19s (remain 2m 37s) Loss: 0.0377 \n",
      "Epoch: [2][2800/4343] Elapsed 4m 28s (remain 2m 27s) Loss: 0.0378 \n",
      "Epoch: [2][2900/4343] Elapsed 4m 38s (remain 2m 18s) Loss: 0.0383 \n",
      "Epoch: [2][3000/4343] Elapsed 4m 47s (remain 2m 8s) Loss: 0.0383 \n",
      "Epoch: [2][3100/4343] Elapsed 4m 57s (remain 1m 59s) Loss: 0.0383 \n",
      "Epoch: [2][3200/4343] Elapsed 5m 7s (remain 1m 49s) Loss: 0.0376 \n",
      "Epoch: [2][3300/4343] Elapsed 5m 16s (remain 1m 39s) Loss: 0.0375 \n",
      "Epoch: [2][3400/4343] Elapsed 5m 26s (remain 1m 30s) Loss: 0.0374 \n",
      "Epoch: [2][3500/4343] Elapsed 5m 35s (remain 1m 20s) Loss: 0.0380 \n",
      "Epoch: [2][3600/4343] Elapsed 5m 45s (remain 1m 11s) Loss: 0.0387 \n",
      "Epoch: [2][3700/4343] Elapsed 5m 55s (remain 1m 1s) Loss: 0.0383 \n",
      "Epoch: [2][3800/4343] Elapsed 6m 4s (remain 0m 51s) Loss: 0.0381 \n",
      "Epoch: [2][3900/4343] Elapsed 6m 14s (remain 0m 42s) Loss: 0.0380 \n",
      "Epoch: [2][4000/4343] Elapsed 6m 23s (remain 0m 32s) Loss: 0.0381 \n",
      "Epoch: [2][4100/4343] Elapsed 6m 33s (remain 0m 23s) Loss: 0.0381 \n",
      "Epoch: [2][4200/4343] Elapsed 6m 42s (remain 0m 13s) Loss: 0.0386 \n",
      "Epoch: [2][4300/4343] Elapsed 6m 52s (remain 0m 4s) Loss: 0.0384 \n",
      "Epoch: [2][4342/4343] Elapsed 6m 56s (remain 0m 0s) Loss: 0.0386 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 12s) Loss: 0.0011 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.0626 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.0441 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0418 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.0465 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.0440 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0440 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0467 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0491 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.0490 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.0486 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8847515002308046, thresh=0.023282372444280715\n",
      "score2 = 0.8672758246863869,  thresh=0.06531067092868038\n",
      "Epoch 2 - avg_train_loss: 0.0386  avg_val_loss: 0.0465  time: 443s\n",
      "Epoch 2 - Score: 0.8672758246863869\n",
      "Epoch 2 - Save Best Score: 0.8673 - Best Loss: 0.0465 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.0465 \n",
      "Epoch: [3][0/4343] Elapsed 0m 0s (remain 20m 13s) Loss: 0.0011 \n",
      "Epoch: [3][100/4343] Elapsed 0m 9s (remain 6m 54s) Loss: 0.0290 \n",
      "Epoch: [3][200/4343] Elapsed 0m 19s (remain 6m 40s) Loss: 0.0338 \n",
      "Epoch: [3][300/4343] Elapsed 0m 29s (remain 6m 29s) Loss: 0.0368 \n",
      "Epoch: [3][400/4343] Elapsed 0m 38s (remain 6m 19s) Loss: 0.0350 \n",
      "Epoch: [3][500/4343] Elapsed 0m 48s (remain 6m 9s) Loss: 0.0332 \n",
      "Epoch: [3][600/4343] Elapsed 0m 57s (remain 6m 0s) Loss: 0.0366 \n",
      "Epoch: [3][700/4343] Elapsed 1m 7s (remain 5m 50s) Loss: 0.0342 \n",
      "Epoch: [3][800/4343] Elapsed 1m 16s (remain 5m 40s) Loss: 0.0334 \n",
      "Epoch: [3][900/4343] Elapsed 1m 26s (remain 5m 30s) Loss: 0.0321 \n",
      "Epoch: [3][1000/4343] Elapsed 1m 36s (remain 5m 21s) Loss: 0.0304 \n",
      "Epoch: [3][1100/4343] Elapsed 1m 45s (remain 5m 11s) Loss: 0.0324 \n",
      "Epoch: [3][1200/4343] Elapsed 1m 55s (remain 5m 1s) Loss: 0.0309 \n",
      "Epoch: [3][1300/4343] Elapsed 2m 4s (remain 4m 52s) Loss: 0.0301 \n",
      "Epoch: [3][1400/4343] Elapsed 2m 14s (remain 4m 42s) Loss: 0.0314 \n",
      "Epoch: [3][1500/4343] Elapsed 2m 24s (remain 4m 32s) Loss: 0.0310 \n",
      "Epoch: [3][1600/4343] Elapsed 2m 33s (remain 4m 23s) Loss: 0.0297 \n",
      "Epoch: [3][1700/4343] Elapsed 2m 43s (remain 4m 13s) Loss: 0.0285 \n",
      "Epoch: [3][1800/4343] Elapsed 2m 52s (remain 4m 4s) Loss: 0.0273 \n",
      "Epoch: [3][1900/4343] Elapsed 3m 2s (remain 3m 54s) Loss: 0.0270 \n",
      "Epoch: [3][2000/4343] Elapsed 3m 12s (remain 3m 44s) Loss: 0.0276 \n",
      "Epoch: [3][2100/4343] Elapsed 3m 21s (remain 3m 35s) Loss: 0.0269 \n",
      "Epoch: [3][2200/4343] Elapsed 3m 31s (remain 3m 25s) Loss: 0.0273 \n",
      "Epoch: [3][2300/4343] Elapsed 3m 40s (remain 3m 15s) Loss: 0.0271 \n",
      "Epoch: [3][2400/4343] Elapsed 3m 50s (remain 3m 6s) Loss: 0.0268 \n",
      "Epoch: [3][2500/4343] Elapsed 3m 59s (remain 2m 56s) Loss: 0.0271 \n",
      "Epoch: [3][2600/4343] Elapsed 4m 9s (remain 2m 47s) Loss: 0.0274 \n",
      "Epoch: [3][2700/4343] Elapsed 4m 19s (remain 2m 37s) Loss: 0.0270 \n",
      "Epoch: [3][2800/4343] Elapsed 4m 28s (remain 2m 27s) Loss: 0.0268 \n",
      "Epoch: [3][2900/4343] Elapsed 4m 38s (remain 2m 18s) Loss: 0.0269 \n",
      "Epoch: [3][3000/4343] Elapsed 4m 47s (remain 2m 8s) Loss: 0.0265 \n",
      "Epoch: [3][3100/4343] Elapsed 4m 57s (remain 1m 59s) Loss: 0.0261 \n",
      "Epoch: [3][3200/4343] Elapsed 5m 7s (remain 1m 49s) Loss: 0.0263 \n",
      "Epoch: [3][3300/4343] Elapsed 5m 16s (remain 1m 39s) Loss: 0.0261 \n",
      "Epoch: [3][3400/4343] Elapsed 5m 26s (remain 1m 30s) Loss: 0.0259 \n",
      "Epoch: [3][3500/4343] Elapsed 5m 35s (remain 1m 20s) Loss: 0.0261 \n",
      "Epoch: [3][3600/4343] Elapsed 5m 45s (remain 1m 11s) Loss: 0.0255 \n",
      "Epoch: [3][3700/4343] Elapsed 5m 55s (remain 1m 1s) Loss: 0.0257 \n",
      "Epoch: [3][3800/4343] Elapsed 6m 4s (remain 0m 51s) Loss: 0.0259 \n",
      "Epoch: [3][3900/4343] Elapsed 6m 14s (remain 0m 42s) Loss: 0.0261 \n",
      "Epoch: [3][4000/4343] Elapsed 6m 23s (remain 0m 32s) Loss: 0.0258 \n",
      "Epoch: [3][4100/4343] Elapsed 6m 33s (remain 0m 23s) Loss: 0.0262 \n",
      "Epoch: [3][4200/4343] Elapsed 6m 43s (remain 0m 13s) Loss: 0.0257 \n",
      "Epoch: [3][4300/4343] Elapsed 6m 52s (remain 0m 4s) Loss: 0.0256 \n",
      "Epoch: [3][4342/4343] Elapsed 6m 56s (remain 0m 0s) Loss: 0.0257 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 19s) Loss: 0.0003 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.0513 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.0418 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0369 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 17s) Loss: 0.0432 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.0457 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 12s) Loss: 0.0458 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0493 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0512 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.0505 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.0494 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8877566774741392, thresh=0.023282372444280715\n",
      "score2 = 0.8923029174425824,  thresh=0.049739736929296945\n",
      "Epoch 3 - avg_train_loss: 0.0257  avg_val_loss: 0.0487  time: 444s\n",
      "Epoch 3 - Score: 0.8923029174425824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.0487 \n",
      "Epoch: [4][0/4343] Elapsed 0m 0s (remain 18m 13s) Loss: 0.0442 \n",
      "Epoch: [4][100/4343] Elapsed 0m 9s (remain 6m 53s) Loss: 0.0108 \n",
      "Epoch: [4][200/4343] Elapsed 0m 19s (remain 6m 40s) Loss: 0.0122 \n",
      "Epoch: [4][300/4343] Elapsed 0m 29s (remain 6m 29s) Loss: 0.0109 \n",
      "Epoch: [4][400/4343] Elapsed 0m 38s (remain 6m 19s) Loss: 0.0133 \n",
      "Epoch: [4][500/4343] Elapsed 0m 48s (remain 6m 9s) Loss: 0.0148 \n",
      "Epoch: [4][600/4343] Elapsed 0m 57s (remain 5m 59s) Loss: 0.0152 \n",
      "Epoch: [4][700/4343] Elapsed 1m 7s (remain 5m 49s) Loss: 0.0142 \n",
      "Epoch: [4][800/4343] Elapsed 1m 16s (remain 5m 40s) Loss: 0.0153 \n",
      "Epoch: [4][900/4343] Elapsed 1m 26s (remain 5m 30s) Loss: 0.0165 \n",
      "Epoch: [4][1000/4343] Elapsed 1m 36s (remain 5m 20s) Loss: 0.0154 \n",
      "Epoch: [4][1100/4343] Elapsed 1m 45s (remain 5m 11s) Loss: 0.0158 \n",
      "Epoch: [4][1200/4343] Elapsed 1m 55s (remain 5m 1s) Loss: 0.0162 \n",
      "Epoch: [4][1300/4343] Elapsed 2m 4s (remain 4m 52s) Loss: 0.0165 \n",
      "Epoch: [4][1400/4343] Elapsed 2m 14s (remain 4m 42s) Loss: 0.0158 \n",
      "Epoch: [4][1500/4343] Elapsed 2m 24s (remain 4m 32s) Loss: 0.0158 \n",
      "Epoch: [4][1600/4343] Elapsed 2m 33s (remain 4m 23s) Loss: 0.0171 \n",
      "Epoch: [4][1700/4343] Elapsed 2m 43s (remain 4m 13s) Loss: 0.0169 \n",
      "Epoch: [4][1800/4343] Elapsed 2m 52s (remain 4m 3s) Loss: 0.0165 \n",
      "Epoch: [4][1900/4343] Elapsed 3m 2s (remain 3m 54s) Loss: 0.0158 \n",
      "Epoch: [4][2000/4343] Elapsed 3m 12s (remain 3m 44s) Loss: 0.0163 \n",
      "Epoch: [4][2100/4343] Elapsed 3m 21s (remain 3m 35s) Loss: 0.0163 \n",
      "Epoch: [4][2200/4343] Elapsed 3m 31s (remain 3m 25s) Loss: 0.0168 \n",
      "Epoch: [4][2300/4343] Elapsed 3m 40s (remain 3m 15s) Loss: 0.0172 \n",
      "Epoch: [4][2400/4343] Elapsed 3m 50s (remain 3m 6s) Loss: 0.0168 \n",
      "Epoch: [4][2500/4343] Elapsed 3m 59s (remain 2m 56s) Loss: 0.0168 \n",
      "Epoch: [4][2600/4343] Elapsed 4m 9s (remain 2m 47s) Loss: 0.0171 \n",
      "Epoch: [4][2700/4343] Elapsed 4m 19s (remain 2m 37s) Loss: 0.0169 \n",
      "Epoch: [4][2800/4343] Elapsed 4m 28s (remain 2m 27s) Loss: 0.0167 \n",
      "Epoch: [4][2900/4343] Elapsed 4m 38s (remain 2m 18s) Loss: 0.0169 \n",
      "Epoch: [4][3000/4343] Elapsed 4m 47s (remain 2m 8s) Loss: 0.0165 \n",
      "Epoch: [4][3100/4343] Elapsed 4m 57s (remain 1m 59s) Loss: 0.0175 \n",
      "Epoch: [4][3200/4343] Elapsed 5m 7s (remain 1m 49s) Loss: 0.0173 \n",
      "Epoch: [4][3300/4343] Elapsed 5m 16s (remain 1m 39s) Loss: 0.0183 \n",
      "Epoch: [4][3400/4343] Elapsed 5m 26s (remain 1m 30s) Loss: 0.0188 \n",
      "Epoch: [4][3500/4343] Elapsed 5m 35s (remain 1m 20s) Loss: 0.0187 \n",
      "Epoch: [4][3600/4343] Elapsed 5m 45s (remain 1m 11s) Loss: 0.0184 \n",
      "Epoch: [4][3700/4343] Elapsed 5m 55s (remain 1m 1s) Loss: 0.0187 \n",
      "Epoch: [4][3800/4343] Elapsed 6m 4s (remain 0m 51s) Loss: 0.0188 \n",
      "Epoch: [4][3900/4343] Elapsed 6m 14s (remain 0m 42s) Loss: 0.0195 \n",
      "Epoch: [4][4000/4343] Elapsed 6m 23s (remain 0m 32s) Loss: 0.0196 \n",
      "Epoch: [4][4100/4343] Elapsed 6m 33s (remain 0m 23s) Loss: 0.0193 \n",
      "Epoch: [4][4200/4343] Elapsed 6m 42s (remain 0m 13s) Loss: 0.0191 \n",
      "Epoch: [4][4300/4343] Elapsed 6m 52s (remain 0m 4s) Loss: 0.0195 \n",
      "Epoch: [4][4342/4343] Elapsed 6m 56s (remain 0m 0s) Loss: 0.0198 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 19s) Loss: 0.0028 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.0565 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.0492 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0431 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.0429 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.0418 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0399 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0435 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0447 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.0461 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.0450 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.859643638637074, thresh=0.023282372444280715\n",
      "score2 = 0.860450563204005,  thresh=0.031225795555332475\n",
      "Epoch 4 - avg_train_loss: 0.0198  avg_val_loss: 0.0446  time: 443s\n",
      "Epoch 4 - Score: 0.860450563204005\n",
      "Epoch 4 - Save Best Score: 0.8605 - Best Loss: 0.0446 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.0446 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 result ==========\n",
      "Score1: 0.85964\n",
      "best border: 0.03123\n",
      "Score2: 0.86045\n",
      "=*========= fold: 3 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/4343] Elapsed 0m 0s (remain 18m 11s) Loss: 0.8129 \n",
      "Epoch: [1][100/4343] Elapsed 0m 9s (remain 6m 51s) Loss: 0.1385 \n",
      "Epoch: [1][200/4343] Elapsed 0m 19s (remain 6m 38s) Loss: 0.1257 \n",
      "Epoch: [1][300/4343] Elapsed 0m 28s (remain 6m 28s) Loss: 0.1356 \n",
      "Epoch: [1][400/4343] Elapsed 0m 38s (remain 6m 18s) Loss: 0.1236 \n",
      "Epoch: [1][500/4343] Elapsed 0m 48s (remain 6m 8s) Loss: 0.1211 \n",
      "Epoch: [1][600/4343] Elapsed 0m 57s (remain 5m 59s) Loss: 0.1167 \n",
      "Epoch: [1][700/4343] Elapsed 1m 7s (remain 5m 49s) Loss: 0.1175 \n",
      "Epoch: [1][800/4343] Elapsed 1m 16s (remain 5m 39s) Loss: 0.1116 \n",
      "Epoch: [1][900/4343] Elapsed 1m 26s (remain 5m 30s) Loss: 0.1069 \n",
      "Epoch: [1][1000/4343] Elapsed 1m 36s (remain 5m 20s) Loss: 0.1027 \n",
      "Epoch: [1][1100/4343] Elapsed 1m 45s (remain 5m 11s) Loss: 0.0989 \n",
      "Epoch: [1][1200/4343] Elapsed 1m 55s (remain 5m 1s) Loss: 0.0954 \n",
      "Epoch: [1][1300/4343] Elapsed 2m 4s (remain 4m 51s) Loss: 0.0932 \n",
      "Epoch: [1][1400/4343] Elapsed 2m 14s (remain 4m 42s) Loss: 0.0912 \n",
      "Epoch: [1][1500/4343] Elapsed 2m 23s (remain 4m 32s) Loss: 0.0902 \n",
      "Epoch: [1][1600/4343] Elapsed 2m 33s (remain 4m 23s) Loss: 0.0894 \n",
      "Epoch: [1][1700/4343] Elapsed 2m 43s (remain 4m 13s) Loss: 0.0903 \n",
      "Epoch: [1][1800/4343] Elapsed 2m 52s (remain 4m 3s) Loss: 0.0887 \n",
      "Epoch: [1][1900/4343] Elapsed 3m 2s (remain 3m 54s) Loss: 0.0871 \n",
      "Epoch: [1][2000/4343] Elapsed 3m 11s (remain 3m 44s) Loss: 0.0843 \n",
      "Epoch: [1][2100/4343] Elapsed 3m 21s (remain 3m 35s) Loss: 0.0825 \n",
      "Epoch: [1][2200/4343] Elapsed 3m 31s (remain 3m 25s) Loss: 0.0815 \n",
      "Epoch: [1][2300/4343] Elapsed 3m 40s (remain 3m 15s) Loss: 0.0786 \n",
      "Epoch: [1][2400/4343] Elapsed 3m 50s (remain 3m 6s) Loss: 0.0776 \n",
      "Epoch: [1][2500/4343] Elapsed 3m 59s (remain 2m 56s) Loss: 0.0774 \n",
      "Epoch: [1][2600/4343] Elapsed 4m 9s (remain 2m 47s) Loss: 0.0769 \n",
      "Epoch: [1][2700/4343] Elapsed 4m 19s (remain 2m 37s) Loss: 0.0761 \n",
      "Epoch: [1][2800/4343] Elapsed 4m 28s (remain 2m 27s) Loss: 0.0748 \n",
      "Epoch: [1][2900/4343] Elapsed 4m 38s (remain 2m 18s) Loss: 0.0744 \n",
      "Epoch: [1][3000/4343] Elapsed 4m 47s (remain 2m 8s) Loss: 0.0745 \n",
      "Epoch: [1][3100/4343] Elapsed 4m 57s (remain 1m 59s) Loss: 0.0758 \n",
      "Epoch: [1][3200/4343] Elapsed 5m 7s (remain 1m 49s) Loss: 0.0761 \n",
      "Epoch: [1][3300/4343] Elapsed 5m 16s (remain 1m 39s) Loss: 0.0785 \n",
      "Epoch: [1][3400/4343] Elapsed 5m 26s (remain 1m 30s) Loss: 0.0792 \n",
      "Epoch: [1][3500/4343] Elapsed 5m 35s (remain 1m 20s) Loss: 0.0803 \n",
      "Epoch: [1][3600/4343] Elapsed 5m 45s (remain 1m 11s) Loss: 0.0806 \n",
      "Epoch: [1][3700/4343] Elapsed 5m 54s (remain 1m 1s) Loss: 0.0807 \n",
      "Epoch: [1][3800/4343] Elapsed 6m 4s (remain 0m 51s) Loss: 0.0806 \n",
      "Epoch: [1][3900/4343] Elapsed 6m 14s (remain 0m 42s) Loss: 0.0813 \n",
      "Epoch: [1][4000/4343] Elapsed 6m 23s (remain 0m 32s) Loss: 0.0831 \n",
      "Epoch: [1][4100/4343] Elapsed 6m 33s (remain 0m 23s) Loss: 0.0841 \n",
      "Epoch: [1][4200/4343] Elapsed 6m 42s (remain 0m 13s) Loss: 0.0838 \n",
      "Epoch: [1][4300/4343] Elapsed 6m 52s (remain 0m 4s) Loss: 0.0842 \n",
      "Epoch: [1][4342/4343] Elapsed 6m 56s (remain 0m 0s) Loss: 0.0845 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 16s) Loss: 0.0218 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.1182 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.1263 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.1138 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 17s) Loss: 0.1114 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.1114 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.1039 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.1039 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.1001 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.1030 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.1062 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.0, thresh=0.023282372444280715\n",
      "score2 = 0.0,  thresh=0.49999553788108364\n",
      "Epoch 1 - avg_train_loss: 0.0845  avg_val_loss: 0.1092  time: 443s\n",
      "Epoch 1 - Score: 0.0\n",
      "Epoch 1 - Save Best Score: 0.0000 - Best Loss: 0.1092 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.1092 \n",
      "Epoch: [2][0/4343] Elapsed 0m 0s (remain 17m 49s) Loss: 0.0183 \n",
      "Epoch: [2][100/4343] Elapsed 0m 9s (remain 6m 55s) Loss: 0.1008 \n",
      "Epoch: [2][200/4343] Elapsed 0m 19s (remain 6m 42s) Loss: 0.1075 \n",
      "Epoch: [2][300/4343] Elapsed 0m 29s (remain 6m 31s) Loss: 0.1089 \n",
      "Epoch: [2][400/4343] Elapsed 0m 38s (remain 6m 20s) Loss: 0.1075 \n",
      "Epoch: [2][500/4343] Elapsed 0m 48s (remain 6m 10s) Loss: 0.1002 \n",
      "Epoch: [2][600/4343] Elapsed 0m 57s (remain 6m 0s) Loss: 0.0931 \n",
      "Epoch: [2][700/4343] Elapsed 1m 7s (remain 5m 51s) Loss: 0.0948 \n",
      "Epoch: [2][800/4343] Elapsed 1m 17s (remain 5m 41s) Loss: 0.0964 \n",
      "Epoch: [2][900/4343] Elapsed 1m 26s (remain 5m 31s) Loss: 0.0982 \n",
      "Epoch: [2][1000/4343] Elapsed 1m 36s (remain 5m 21s) Loss: 0.0993 \n",
      "Epoch: [2][1100/4343] Elapsed 1m 46s (remain 5m 12s) Loss: 0.1034 \n",
      "Epoch: [2][1200/4343] Elapsed 1m 55s (remain 5m 2s) Loss: 0.1034 \n",
      "Epoch: [2][1300/4343] Elapsed 2m 5s (remain 4m 53s) Loss: 0.1023 \n",
      "Epoch: [2][1400/4343] Elapsed 2m 14s (remain 4m 43s) Loss: 0.1037 \n",
      "Epoch: [2][1500/4343] Elapsed 2m 24s (remain 4m 33s) Loss: 0.1042 \n",
      "Epoch: [2][1600/4343] Elapsed 2m 34s (remain 4m 24s) Loss: 0.1023 \n",
      "Epoch: [2][1700/4343] Elapsed 2m 43s (remain 4m 14s) Loss: 0.1001 \n",
      "Epoch: [2][1800/4343] Elapsed 2m 53s (remain 4m 4s) Loss: 0.0973 \n",
      "Epoch: [2][1900/4343] Elapsed 3m 3s (remain 3m 55s) Loss: 0.0953 \n",
      "Epoch: [2][2000/4343] Elapsed 3m 12s (remain 3m 45s) Loss: 0.0934 \n",
      "Epoch: [2][2100/4343] Elapsed 3m 22s (remain 3m 35s) Loss: 0.0920 \n",
      "Epoch: [2][2200/4343] Elapsed 3m 31s (remain 3m 26s) Loss: 0.0885 \n",
      "Epoch: [2][2300/4343] Elapsed 3m 41s (remain 3m 16s) Loss: 0.0888 \n",
      "Epoch: [2][2400/4343] Elapsed 3m 51s (remain 3m 6s) Loss: 0.0879 \n",
      "Epoch: [2][2500/4343] Elapsed 4m 0s (remain 2m 57s) Loss: 0.0875 \n",
      "Epoch: [2][2600/4343] Elapsed 4m 10s (remain 2m 47s) Loss: 0.0860 \n",
      "Epoch: [2][2700/4343] Elapsed 4m 19s (remain 2m 38s) Loss: 0.0837 \n",
      "Epoch: [2][2800/4343] Elapsed 4m 29s (remain 2m 28s) Loss: 0.0823 \n",
      "Epoch: [2][2900/4343] Elapsed 4m 39s (remain 2m 18s) Loss: 0.0808 \n",
      "Epoch: [2][3000/4343] Elapsed 4m 48s (remain 2m 9s) Loss: 0.0790 \n",
      "Epoch: [2][3100/4343] Elapsed 4m 58s (remain 1m 59s) Loss: 0.0791 \n",
      "Epoch: [2][3200/4343] Elapsed 5m 8s (remain 1m 49s) Loss: 0.0782 \n",
      "Epoch: [2][3300/4343] Elapsed 5m 17s (remain 1m 40s) Loss: 0.0770 \n",
      "Epoch: [2][3400/4343] Elapsed 5m 27s (remain 1m 30s) Loss: 0.0758 \n",
      "Epoch: [2][3500/4343] Elapsed 5m 36s (remain 1m 21s) Loss: 0.0745 \n",
      "Epoch: [2][3600/4343] Elapsed 5m 46s (remain 1m 11s) Loss: 0.0736 \n",
      "Epoch: [2][3700/4343] Elapsed 5m 56s (remain 1m 1s) Loss: 0.0721 \n",
      "Epoch: [2][3800/4343] Elapsed 6m 5s (remain 0m 52s) Loss: 0.0720 \n",
      "Epoch: [2][3900/4343] Elapsed 6m 15s (remain 0m 42s) Loss: 0.0720 \n",
      "Epoch: [2][4000/4343] Elapsed 6m 25s (remain 0m 32s) Loss: 0.0727 \n",
      "Epoch: [2][4100/4343] Elapsed 6m 34s (remain 0m 23s) Loss: 0.0722 \n",
      "Epoch: [2][4200/4343] Elapsed 6m 44s (remain 0m 13s) Loss: 0.0720 \n",
      "Epoch: [2][4300/4343] Elapsed 6m 53s (remain 0m 4s) Loss: 0.0714 \n",
      "Epoch: [2][4342/4343] Elapsed 6m 57s (remain 0m 0s) Loss: 0.0712 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 14s) Loss: 0.0008 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.0639 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.0669 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0582 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.0580 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.0583 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0535 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0549 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0531 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.0542 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.0555 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8735564110156945, thresh=0.023282372444280715\n",
      "score2 = 0.8811866647084741,  thresh=0.015273631093868614\n",
      "Epoch 2 - avg_train_loss: 0.0712  avg_val_loss: 0.0553  time: 445s\n",
      "Epoch 2 - Score: 0.8811866647084741\n",
      "Epoch 2 - Save Best Score: 0.8812 - Best Loss: 0.0553 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.0553 \n",
      "Epoch: [3][0/4343] Elapsed 0m 0s (remain 19m 59s) Loss: 0.0010 \n",
      "Epoch: [3][100/4343] Elapsed 0m 9s (remain 6m 55s) Loss: 0.0406 \n",
      "Epoch: [3][200/4343] Elapsed 0m 19s (remain 6m 42s) Loss: 0.0440 \n",
      "Epoch: [3][300/4343] Elapsed 0m 29s (remain 6m 31s) Loss: 0.0404 \n",
      "Epoch: [3][400/4343] Elapsed 0m 38s (remain 6m 21s) Loss: 0.0396 \n",
      "Epoch: [3][500/4343] Elapsed 0m 48s (remain 6m 11s) Loss: 0.0402 \n",
      "Epoch: [3][600/4343] Elapsed 0m 58s (remain 6m 1s) Loss: 0.0434 \n",
      "Epoch: [3][700/4343] Elapsed 1m 7s (remain 5m 51s) Loss: 0.0475 \n",
      "Epoch: [3][800/4343] Elapsed 1m 17s (remain 5m 41s) Loss: 0.0521 \n",
      "Epoch: [3][900/4343] Elapsed 1m 26s (remain 5m 32s) Loss: 0.0516 \n",
      "Epoch: [3][1000/4343] Elapsed 1m 36s (remain 5m 22s) Loss: 0.0509 \n",
      "Epoch: [3][1100/4343] Elapsed 1m 46s (remain 5m 12s) Loss: 0.0509 \n",
      "Epoch: [3][1200/4343] Elapsed 1m 55s (remain 5m 2s) Loss: 0.0494 \n",
      "Epoch: [3][1300/4343] Elapsed 2m 5s (remain 4m 53s) Loss: 0.0487 \n",
      "Epoch: [3][1400/4343] Elapsed 2m 15s (remain 4m 43s) Loss: 0.0490 \n",
      "Epoch: [3][1500/4343] Elapsed 2m 24s (remain 4m 33s) Loss: 0.0491 \n",
      "Epoch: [3][1600/4343] Elapsed 2m 34s (remain 4m 24s) Loss: 0.0496 \n",
      "Epoch: [3][1700/4343] Elapsed 2m 43s (remain 4m 14s) Loss: 0.0496 \n",
      "Epoch: [3][1800/4343] Elapsed 2m 53s (remain 4m 5s) Loss: 0.0494 \n",
      "Epoch: [3][1900/4343] Elapsed 3m 3s (remain 3m 55s) Loss: 0.0498 \n",
      "Epoch: [3][2000/4343] Elapsed 3m 12s (remain 3m 45s) Loss: 0.0511 \n",
      "Epoch: [3][2100/4343] Elapsed 3m 22s (remain 3m 36s) Loss: 0.0502 \n",
      "Epoch: [3][2200/4343] Elapsed 3m 32s (remain 3m 26s) Loss: 0.0506 \n",
      "Epoch: [3][2300/4343] Elapsed 3m 41s (remain 3m 16s) Loss: 0.0509 \n",
      "Epoch: [3][2400/4343] Elapsed 3m 51s (remain 3m 7s) Loss: 0.0502 \n",
      "Epoch: [3][2500/4343] Elapsed 4m 0s (remain 2m 57s) Loss: 0.0500 \n",
      "Epoch: [3][2600/4343] Elapsed 4m 10s (remain 2m 47s) Loss: 0.0492 \n",
      "Epoch: [3][2700/4343] Elapsed 4m 20s (remain 2m 38s) Loss: 0.0482 \n",
      "Epoch: [3][2800/4343] Elapsed 4m 29s (remain 2m 28s) Loss: 0.0482 \n",
      "Epoch: [3][2900/4343] Elapsed 4m 39s (remain 2m 18s) Loss: 0.0482 \n",
      "Epoch: [3][3000/4343] Elapsed 4m 49s (remain 2m 9s) Loss: 0.0483 \n",
      "Epoch: [3][3100/4343] Elapsed 4m 58s (remain 1m 59s) Loss: 0.0482 \n",
      "Epoch: [3][3200/4343] Elapsed 5m 8s (remain 1m 50s) Loss: 0.0477 \n",
      "Epoch: [3][3300/4343] Elapsed 5m 17s (remain 1m 40s) Loss: 0.0478 \n",
      "Epoch: [3][3400/4343] Elapsed 5m 27s (remain 1m 30s) Loss: 0.0482 \n",
      "Epoch: [3][3500/4343] Elapsed 5m 37s (remain 1m 21s) Loss: 0.0482 \n",
      "Epoch: [3][3600/4343] Elapsed 5m 46s (remain 1m 11s) Loss: 0.0477 \n",
      "Epoch: [3][3700/4343] Elapsed 5m 56s (remain 1m 1s) Loss: 0.0480 \n",
      "Epoch: [3][3800/4343] Elapsed 6m 6s (remain 0m 52s) Loss: 0.0476 \n",
      "Epoch: [3][3900/4343] Elapsed 6m 15s (remain 0m 42s) Loss: 0.0474 \n",
      "Epoch: [3][4000/4343] Elapsed 6m 25s (remain 0m 32s) Loss: 0.0470 \n",
      "Epoch: [3][4100/4343] Elapsed 6m 35s (remain 0m 23s) Loss: 0.0468 \n",
      "Epoch: [3][4200/4343] Elapsed 6m 44s (remain 0m 13s) Loss: 0.0467 \n",
      "Epoch: [3][4300/4343] Elapsed 6m 54s (remain 0m 4s) Loss: 0.0471 \n",
      "Epoch: [3][4342/4343] Elapsed 6m 58s (remain 0m 0s) Loss: 0.0473 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 16s) Loss: 0.0007 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.1168 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.0875 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0752 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.0754 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.0720 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0656 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0726 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0713 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.0728 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.0721 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.7554225878833208, thresh=0.023282372444280715\n",
      "score2 = 0.7446968557243869,  thresh=0.050804882689202344\n",
      "Epoch 3 - avg_train_loss: 0.0473  avg_val_loss: 0.0721  time: 445s\n",
      "Epoch 3 - Score: 0.7446968557243869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.0721 \n",
      "Epoch: [4][0/4343] Elapsed 0m 0s (remain 17m 49s) Loss: 0.0009 \n",
      "Epoch: [4][100/4343] Elapsed 0m 9s (remain 6m 53s) Loss: 0.0473 \n",
      "Epoch: [4][200/4343] Elapsed 0m 19s (remain 6m 41s) Loss: 0.0648 \n",
      "Epoch: [4][300/4343] Elapsed 0m 29s (remain 6m 30s) Loss: 0.0587 \n",
      "Epoch: [4][400/4343] Elapsed 0m 38s (remain 6m 20s) Loss: 0.0545 \n",
      "Epoch: [4][500/4343] Elapsed 0m 48s (remain 6m 10s) Loss: 0.0511 \n",
      "Epoch: [4][600/4343] Elapsed 0m 57s (remain 6m 0s) Loss: 0.0496 \n",
      "Epoch: [4][700/4343] Elapsed 1m 7s (remain 5m 51s) Loss: 0.0476 \n",
      "Epoch: [4][800/4343] Elapsed 1m 17s (remain 5m 41s) Loss: 0.0448 \n",
      "Epoch: [4][900/4343] Elapsed 1m 26s (remain 5m 31s) Loss: 0.0444 \n",
      "Epoch: [4][1000/4343] Elapsed 1m 36s (remain 5m 22s) Loss: 0.0429 \n",
      "Epoch: [4][1100/4343] Elapsed 1m 46s (remain 5m 12s) Loss: 0.0432 \n",
      "Epoch: [4][1200/4343] Elapsed 1m 55s (remain 5m 2s) Loss: 0.0413 \n",
      "Epoch: [4][1300/4343] Elapsed 2m 5s (remain 4m 52s) Loss: 0.0413 \n",
      "Epoch: [4][1400/4343] Elapsed 2m 14s (remain 4m 43s) Loss: 0.0399 \n",
      "Epoch: [4][1500/4343] Elapsed 2m 24s (remain 4m 33s) Loss: 0.0408 \n",
      "Epoch: [4][1600/4343] Elapsed 2m 34s (remain 4m 24s) Loss: 0.0404 \n",
      "Epoch: [4][1700/4343] Elapsed 2m 43s (remain 4m 14s) Loss: 0.0387 \n",
      "Epoch: [4][1800/4343] Elapsed 2m 53s (remain 4m 4s) Loss: 0.0381 \n",
      "Epoch: [4][1900/4343] Elapsed 3m 3s (remain 3m 55s) Loss: 0.0374 \n",
      "Epoch: [4][2000/4343] Elapsed 3m 12s (remain 3m 45s) Loss: 0.0376 \n",
      "Epoch: [4][2100/4343] Elapsed 3m 22s (remain 3m 35s) Loss: 0.0367 \n",
      "Epoch: [4][2200/4343] Elapsed 3m 31s (remain 3m 26s) Loss: 0.0369 \n",
      "Epoch: [4][2300/4343] Elapsed 3m 41s (remain 3m 16s) Loss: 0.0367 \n",
      "Epoch: [4][2400/4343] Elapsed 3m 51s (remain 3m 6s) Loss: 0.0376 \n",
      "Epoch: [4][2500/4343] Elapsed 4m 0s (remain 2m 57s) Loss: 0.0373 \n",
      "Epoch: [4][2600/4343] Elapsed 4m 10s (remain 2m 47s) Loss: 0.0372 \n",
      "Epoch: [4][2700/4343] Elapsed 4m 20s (remain 2m 38s) Loss: 0.0376 \n",
      "Epoch: [4][2800/4343] Elapsed 4m 29s (remain 2m 28s) Loss: 0.0374 \n",
      "Epoch: [4][2900/4343] Elapsed 4m 39s (remain 2m 18s) Loss: 0.0379 \n",
      "Epoch: [4][3000/4343] Elapsed 4m 48s (remain 2m 9s) Loss: 0.0373 \n",
      "Epoch: [4][3100/4343] Elapsed 4m 58s (remain 1m 59s) Loss: 0.0369 \n",
      "Epoch: [4][3200/4343] Elapsed 5m 8s (remain 1m 49s) Loss: 0.0389 \n",
      "Epoch: [4][3300/4343] Elapsed 5m 17s (remain 1m 40s) Loss: 0.0409 \n",
      "Epoch: [4][3400/4343] Elapsed 5m 27s (remain 1m 30s) Loss: 0.0413 \n",
      "Epoch: [4][3500/4343] Elapsed 5m 36s (remain 1m 21s) Loss: 0.0408 \n",
      "Epoch: [4][3600/4343] Elapsed 5m 46s (remain 1m 11s) Loss: 0.0404 \n",
      "Epoch: [4][3700/4343] Elapsed 5m 56s (remain 1m 1s) Loss: 0.0421 \n",
      "Epoch: [4][3800/4343] Elapsed 6m 5s (remain 0m 52s) Loss: 0.0423 \n",
      "Epoch: [4][3900/4343] Elapsed 6m 15s (remain 0m 42s) Loss: 0.0423 \n",
      "Epoch: [4][4000/4343] Elapsed 6m 25s (remain 0m 32s) Loss: 0.0420 \n",
      "Epoch: [4][4100/4343] Elapsed 6m 34s (remain 0m 23s) Loss: 0.0418 \n",
      "Epoch: [4][4200/4343] Elapsed 6m 44s (remain 0m 13s) Loss: 0.0418 \n",
      "Epoch: [4][4300/4343] Elapsed 6m 53s (remain 0m 4s) Loss: 0.0422 \n",
      "Epoch: [4][4342/4343] Elapsed 6m 57s (remain 0m 0s) Loss: 0.0420 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 14s) Loss: 0.0016 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.1103 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.0899 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0718 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.0684 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.0668 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0586 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0628 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0603 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.0627 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.0618 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.7240734973528495, thresh=0.023282372444280715\n",
      "score2 = 0.7109374999999999,  thresh=0.07294496276038648\n",
      "Epoch 4 - avg_train_loss: 0.0420  avg_val_loss: 0.0610  time: 445s\n",
      "Epoch 4 - Score: 0.7109374999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.0610 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 result ==========\n",
      "Score1: 0.87356\n",
      "best border: 0.01527\n",
      "Score2: 0.88119\n",
      "=*========= fold: 4 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/4343] Elapsed 0m 0s (remain 18m 27s) Loss: 0.6240 \n",
      "Epoch: [1][100/4343] Elapsed 0m 9s (remain 6m 51s) Loss: 0.1141 \n",
      "Epoch: [1][200/4343] Elapsed 0m 19s (remain 6m 38s) Loss: 0.1299 \n",
      "Epoch: [1][300/4343] Elapsed 0m 28s (remain 6m 28s) Loss: 0.1319 \n",
      "Epoch: [1][400/4343] Elapsed 0m 38s (remain 6m 18s) Loss: 0.1325 \n",
      "Epoch: [1][500/4343] Elapsed 0m 48s (remain 6m 8s) Loss: 0.1298 \n",
      "Epoch: [1][600/4343] Elapsed 0m 57s (remain 5m 59s) Loss: 0.1300 \n",
      "Epoch: [1][700/4343] Elapsed 1m 7s (remain 5m 49s) Loss: 0.1244 \n",
      "Epoch: [1][800/4343] Elapsed 1m 16s (remain 5m 39s) Loss: 0.1234 \n",
      "Epoch: [1][900/4343] Elapsed 1m 26s (remain 5m 30s) Loss: 0.1207 \n",
      "Epoch: [1][1000/4343] Elapsed 1m 36s (remain 5m 20s) Loss: 0.1248 \n",
      "Epoch: [1][1100/4343] Elapsed 1m 45s (remain 5m 11s) Loss: 0.1248 \n",
      "Epoch: [1][1200/4343] Elapsed 1m 55s (remain 5m 1s) Loss: 0.1214 \n",
      "Epoch: [1][1300/4343] Elapsed 2m 4s (remain 4m 52s) Loss: 0.1198 \n",
      "Epoch: [1][1400/4343] Elapsed 2m 14s (remain 4m 42s) Loss: 0.1197 \n",
      "Epoch: [1][1500/4343] Elapsed 2m 24s (remain 4m 33s) Loss: 0.1187 \n",
      "Epoch: [1][1600/4343] Elapsed 2m 33s (remain 4m 23s) Loss: 0.1173 \n",
      "Epoch: [1][1700/4343] Elapsed 2m 43s (remain 4m 13s) Loss: 0.1168 \n",
      "Epoch: [1][1800/4343] Elapsed 2m 53s (remain 4m 4s) Loss: 0.1163 \n",
      "Epoch: [1][1900/4343] Elapsed 3m 2s (remain 3m 54s) Loss: 0.1146 \n",
      "Epoch: [1][2000/4343] Elapsed 3m 12s (remain 3m 44s) Loss: 0.1139 \n",
      "Epoch: [1][2100/4343] Elapsed 3m 21s (remain 3m 35s) Loss: 0.1151 \n",
      "Epoch: [1][2200/4343] Elapsed 3m 31s (remain 3m 25s) Loss: 0.1140 \n",
      "Epoch: [1][2300/4343] Elapsed 3m 41s (remain 3m 16s) Loss: 0.1126 \n",
      "Epoch: [1][2400/4343] Elapsed 3m 50s (remain 3m 6s) Loss: 0.1145 \n",
      "Epoch: [1][2500/4343] Elapsed 4m 0s (remain 2m 56s) Loss: 0.1132 \n",
      "Epoch: [1][2600/4343] Elapsed 4m 9s (remain 2m 47s) Loss: 0.1124 \n",
      "Epoch: [1][2700/4343] Elapsed 4m 19s (remain 2m 37s) Loss: 0.1138 \n",
      "Epoch: [1][2800/4343] Elapsed 4m 29s (remain 2m 28s) Loss: 0.1134 \n",
      "Epoch: [1][2900/4343] Elapsed 4m 38s (remain 2m 18s) Loss: 0.1131 \n",
      "Epoch: [1][3000/4343] Elapsed 4m 48s (remain 2m 8s) Loss: 0.1127 \n",
      "Epoch: [1][3100/4343] Elapsed 4m 57s (remain 1m 59s) Loss: 0.1131 \n",
      "Epoch: [1][3200/4343] Elapsed 5m 7s (remain 1m 49s) Loss: 0.1126 \n",
      "Epoch: [1][3300/4343] Elapsed 5m 17s (remain 1m 40s) Loss: 0.1134 \n",
      "Epoch: [1][3400/4343] Elapsed 5m 26s (remain 1m 30s) Loss: 0.1138 \n",
      "Epoch: [1][3500/4343] Elapsed 5m 36s (remain 1m 20s) Loss: 0.1146 \n",
      "Epoch: [1][3600/4343] Elapsed 5m 46s (remain 1m 11s) Loss: 0.1142 \n",
      "Epoch: [1][3700/4343] Elapsed 5m 55s (remain 1m 1s) Loss: 0.1146 \n",
      "Epoch: [1][3800/4343] Elapsed 6m 5s (remain 0m 52s) Loss: 0.1139 \n",
      "Epoch: [1][3900/4343] Elapsed 6m 14s (remain 0m 42s) Loss: 0.1137 \n",
      "Epoch: [1][4000/4343] Elapsed 6m 24s (remain 0m 32s) Loss: 0.1123 \n",
      "Epoch: [1][4100/4343] Elapsed 6m 34s (remain 0m 23s) Loss: 0.1120 \n",
      "Epoch: [1][4200/4343] Elapsed 6m 43s (remain 0m 13s) Loss: 0.1125 \n",
      "Epoch: [1][4300/4343] Elapsed 6m 53s (remain 0m 4s) Loss: 0.1122 \n",
      "Epoch: [1][4342/4343] Elapsed 6m 57s (remain 0m 0s) Loss: 0.1122 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 15s) Loss: 0.0153 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.1212 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.1208 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.1083 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.1072 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.1076 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.1064 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0990 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.1066 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.1076 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.1097 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.3937007874015748, thresh=0.023282372444280715\n",
      "score2 = 0.39382482671707625,  thresh=0.4934285039048734\n",
      "Epoch 1 - avg_train_loss: 0.1122  avg_val_loss: 0.1123  time: 444s\n",
      "Epoch 1 - Score: 0.39382482671707625\n",
      "Epoch 1 - Save Best Score: 0.3938 - Best Loss: 0.1123 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.1123 \n",
      "Epoch: [2][0/4343] Elapsed 0m 0s (remain 18m 0s) Loss: 0.0171 \n",
      "Epoch: [2][100/4343] Elapsed 0m 9s (remain 6m 55s) Loss: 0.0796 \n",
      "Epoch: [2][200/4343] Elapsed 0m 19s (remain 6m 41s) Loss: 0.0890 \n",
      "Epoch: [2][300/4343] Elapsed 0m 29s (remain 6m 30s) Loss: 0.0954 \n",
      "Epoch: [2][400/4343] Elapsed 0m 38s (remain 6m 20s) Loss: 0.1040 \n",
      "Epoch: [2][500/4343] Elapsed 0m 48s (remain 6m 10s) Loss: 0.1181 \n",
      "Epoch: [2][600/4343] Elapsed 0m 57s (remain 6m 0s) Loss: 0.1152 \n",
      "Epoch: [2][700/4343] Elapsed 1m 7s (remain 5m 50s) Loss: 0.1118 \n",
      "Epoch: [2][800/4343] Elapsed 1m 17s (remain 5m 41s) Loss: 0.1105 \n",
      "Epoch: [2][900/4343] Elapsed 1m 26s (remain 5m 31s) Loss: 0.1118 \n",
      "Epoch: [2][1000/4343] Elapsed 1m 36s (remain 5m 21s) Loss: 0.1122 \n",
      "Epoch: [2][1100/4343] Elapsed 1m 45s (remain 5m 11s) Loss: 0.1111 \n",
      "Epoch: [2][1200/4343] Elapsed 1m 55s (remain 5m 2s) Loss: 0.1130 \n",
      "Epoch: [2][1300/4343] Elapsed 2m 5s (remain 4m 52s) Loss: 0.1138 \n",
      "Epoch: [2][1400/4343] Elapsed 2m 14s (remain 4m 43s) Loss: 0.1126 \n",
      "Epoch: [2][1500/4343] Elapsed 2m 24s (remain 4m 33s) Loss: 0.1116 \n",
      "Epoch: [2][1600/4343] Elapsed 2m 33s (remain 4m 23s) Loss: 0.1107 \n",
      "Epoch: [2][1700/4343] Elapsed 2m 43s (remain 4m 14s) Loss: 0.1107 \n",
      "Epoch: [2][1800/4343] Elapsed 2m 53s (remain 4m 4s) Loss: 0.1097 \n",
      "Epoch: [2][1900/4343] Elapsed 3m 2s (remain 3m 54s) Loss: 0.1077 \n",
      "Epoch: [2][2000/4343] Elapsed 3m 12s (remain 3m 45s) Loss: 0.1060 \n",
      "Epoch: [2][2100/4343] Elapsed 3m 22s (remain 3m 35s) Loss: 0.1042 \n",
      "Epoch: [2][2200/4343] Elapsed 3m 31s (remain 3m 25s) Loss: 0.1044 \n",
      "Epoch: [2][2300/4343] Elapsed 3m 41s (remain 3m 16s) Loss: 0.1047 \n",
      "Epoch: [2][2400/4343] Elapsed 3m 50s (remain 3m 6s) Loss: 0.1043 \n",
      "Epoch: [2][2500/4343] Elapsed 4m 0s (remain 2m 57s) Loss: 0.1036 \n",
      "Epoch: [2][2600/4343] Elapsed 4m 10s (remain 2m 47s) Loss: 0.1027 \n",
      "Epoch: [2][2700/4343] Elapsed 4m 19s (remain 2m 37s) Loss: 0.1009 \n",
      "Epoch: [2][2800/4343] Elapsed 4m 29s (remain 2m 28s) Loss: 0.1002 \n",
      "Epoch: [2][2900/4343] Elapsed 4m 38s (remain 2m 18s) Loss: 0.1009 \n",
      "Epoch: [2][3000/4343] Elapsed 4m 48s (remain 2m 9s) Loss: 0.1011 \n",
      "Epoch: [2][3100/4343] Elapsed 4m 58s (remain 1m 59s) Loss: 0.1011 \n",
      "Epoch: [2][3200/4343] Elapsed 5m 7s (remain 1m 49s) Loss: 0.1017 \n",
      "Epoch: [2][3300/4343] Elapsed 5m 17s (remain 1m 40s) Loss: 0.1023 \n",
      "Epoch: [2][3400/4343] Elapsed 5m 26s (remain 1m 30s) Loss: 0.1029 \n",
      "Epoch: [2][3500/4343] Elapsed 5m 36s (remain 1m 20s) Loss: 0.1037 \n",
      "Epoch: [2][3600/4343] Elapsed 5m 46s (remain 1m 11s) Loss: 0.1040 \n",
      "Epoch: [2][3700/4343] Elapsed 5m 55s (remain 1m 1s) Loss: 0.1049 \n",
      "Epoch: [2][3800/4343] Elapsed 6m 5s (remain 0m 52s) Loss: 0.1046 \n",
      "Epoch: [2][3900/4343] Elapsed 6m 14s (remain 0m 42s) Loss: 0.1045 \n",
      "Epoch: [2][4000/4343] Elapsed 6m 24s (remain 0m 32s) Loss: 0.1043 \n",
      "Epoch: [2][4100/4343] Elapsed 6m 34s (remain 0m 23s) Loss: 0.1046 \n",
      "Epoch: [2][4200/4343] Elapsed 6m 43s (remain 0m 13s) Loss: 0.1046 \n",
      "Epoch: [2][4300/4343] Elapsed 6m 53s (remain 0m 4s) Loss: 0.1045 \n",
      "Epoch: [2][4342/4343] Elapsed 6m 57s (remain 0m 0s) Loss: 0.1044 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 21s) Loss: 0.0166 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.0895 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.1061 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.1008 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.1064 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.1064 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.1010 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0983 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.1003 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.1056 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.1106 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.0, thresh=0.023282372444280715\n",
      "score2 = 0.0,  thresh=0.49999553788108364\n",
      "Epoch 2 - avg_train_loss: 0.1044  avg_val_loss: 0.1123  time: 444s\n",
      "Epoch 2 - Score: 0.0\n",
      "Epoch 2 - Save Best Score: 0.0000 - Best Loss: 0.1123 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.1123 \n",
      "Epoch: [3][0/4343] Elapsed 0m 0s (remain 20m 12s) Loss: 0.0206 \n",
      "Epoch: [3][100/4343] Elapsed 0m 9s (remain 6m 55s) Loss: 0.1370 \n",
      "Epoch: [3][200/4343] Elapsed 0m 19s (remain 6m 41s) Loss: 0.1100 \n",
      "Epoch: [3][300/4343] Elapsed 0m 29s (remain 6m 31s) Loss: 0.1037 \n",
      "Epoch: [3][400/4343] Elapsed 0m 38s (remain 6m 21s) Loss: 0.1105 \n",
      "Epoch: [3][500/4343] Elapsed 0m 48s (remain 6m 11s) Loss: 0.0999 \n",
      "Epoch: [3][600/4343] Elapsed 0m 58s (remain 6m 1s) Loss: 0.0984 \n",
      "Epoch: [3][700/4343] Elapsed 1m 7s (remain 5m 51s) Loss: 0.0964 \n",
      "Epoch: [3][800/4343] Elapsed 1m 17s (remain 5m 41s) Loss: 0.1000 \n",
      "Epoch: [3][900/4343] Elapsed 1m 26s (remain 5m 31s) Loss: 0.1025 \n",
      "Epoch: [3][1000/4343] Elapsed 1m 36s (remain 5m 22s) Loss: 0.1038 \n",
      "Epoch: [3][1100/4343] Elapsed 1m 46s (remain 5m 12s) Loss: 0.1093 \n",
      "Epoch: [3][1200/4343] Elapsed 1m 55s (remain 5m 2s) Loss: 0.1073 \n",
      "Epoch: [3][1300/4343] Elapsed 2m 5s (remain 4m 53s) Loss: 0.1086 \n",
      "Epoch: [3][1400/4343] Elapsed 2m 14s (remain 4m 43s) Loss: 0.1068 \n",
      "Epoch: [3][1500/4343] Elapsed 2m 24s (remain 4m 33s) Loss: 0.1077 \n",
      "Epoch: [3][1600/4343] Elapsed 2m 34s (remain 4m 24s) Loss: 0.1057 \n",
      "Epoch: [3][1700/4343] Elapsed 2m 43s (remain 4m 14s) Loss: 0.1059 \n",
      "Epoch: [3][1800/4343] Elapsed 2m 53s (remain 4m 4s) Loss: 0.1068 \n",
      "Epoch: [3][1900/4343] Elapsed 3m 3s (remain 3m 55s) Loss: 0.1072 \n",
      "Epoch: [3][2000/4343] Elapsed 3m 12s (remain 3m 45s) Loss: 0.1063 \n",
      "Epoch: [3][2100/4343] Elapsed 3m 22s (remain 3m 35s) Loss: 0.1064 \n",
      "Epoch: [3][2200/4343] Elapsed 3m 31s (remain 3m 26s) Loss: 0.1054 \n",
      "Epoch: [3][2300/4343] Elapsed 3m 41s (remain 3m 16s) Loss: 0.1058 \n",
      "Epoch: [3][2400/4343] Elapsed 3m 51s (remain 3m 6s) Loss: 0.1061 \n",
      "Epoch: [3][2500/4343] Elapsed 4m 0s (remain 2m 57s) Loss: 0.1070 \n",
      "Epoch: [3][2600/4343] Elapsed 4m 10s (remain 2m 47s) Loss: 0.1076 \n",
      "Epoch: [3][2700/4343] Elapsed 4m 19s (remain 2m 38s) Loss: 0.1076 \n",
      "Epoch: [3][2800/4343] Elapsed 4m 29s (remain 2m 28s) Loss: 0.1084 \n",
      "Epoch: [3][2900/4343] Elapsed 4m 39s (remain 2m 18s) Loss: 0.1083 \n",
      "Epoch: [3][3000/4343] Elapsed 4m 48s (remain 2m 9s) Loss: 0.1090 \n",
      "Epoch: [3][3100/4343] Elapsed 4m 58s (remain 1m 59s) Loss: 0.1096 \n",
      "Epoch: [3][3200/4343] Elapsed 5m 8s (remain 1m 49s) Loss: 0.1104 \n",
      "Epoch: [3][3300/4343] Elapsed 5m 17s (remain 1m 40s) Loss: 0.1094 \n",
      "Epoch: [3][3400/4343] Elapsed 5m 27s (remain 1m 30s) Loss: 0.1109 \n",
      "Epoch: [3][3500/4343] Elapsed 5m 36s (remain 1m 21s) Loss: 0.1104 \n",
      "Epoch: [3][3600/4343] Elapsed 5m 46s (remain 1m 11s) Loss: 0.1105 \n",
      "Epoch: [3][3700/4343] Elapsed 5m 56s (remain 1m 1s) Loss: 0.1106 \n",
      "Epoch: [3][3800/4343] Elapsed 6m 5s (remain 0m 52s) Loss: 0.1107 \n",
      "Epoch: [3][3900/4343] Elapsed 6m 15s (remain 0m 42s) Loss: 0.1108 \n",
      "Epoch: [3][4000/4343] Elapsed 6m 25s (remain 0m 32s) Loss: 0.1113 \n",
      "Epoch: [3][4100/4343] Elapsed 6m 34s (remain 0m 23s) Loss: 0.1119 \n",
      "Epoch: [3][4200/4343] Elapsed 6m 44s (remain 0m 13s) Loss: 0.1120 \n",
      "Epoch: [3][4300/4343] Elapsed 6m 53s (remain 0m 4s) Loss: 0.1117 \n",
      "Epoch: [3][4342/4343] Elapsed 6m 57s (remain 0m 0s) Loss: 0.1115 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 17s) Loss: 0.0190 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.0895 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.1055 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.1004 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.1058 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.1058 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.1006 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0980 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0999 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.1050 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.1098 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.0, thresh=0.023282372444280715\n",
      "score2 = 0.0,  thresh=0.49999553788108364\n",
      "Epoch 3 - avg_train_loss: 0.1115  avg_val_loss: 0.1115  time: 445s\n",
      "Epoch 3 - Score: 0.0\n",
      "Epoch 3 - Save Best Score: 0.0000 - Best Loss: 0.1115 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.1115 \n",
      "Epoch: [4][0/4343] Elapsed 0m 0s (remain 18m 2s) Loss: 0.0207 \n",
      "Epoch: [4][100/4343] Elapsed 0m 9s (remain 6m 54s) Loss: 0.1135 \n",
      "Epoch: [4][200/4343] Elapsed 0m 19s (remain 6m 41s) Loss: 0.0944 \n",
      "Epoch: [4][300/4343] Elapsed 0m 29s (remain 6m 30s) Loss: 0.0989 \n",
      "Epoch: [4][400/4343] Elapsed 0m 38s (remain 6m 20s) Loss: 0.1125 \n",
      "Epoch: [4][500/4343] Elapsed 0m 48s (remain 6m 10s) Loss: 0.1131 \n",
      "Epoch: [4][600/4343] Elapsed 0m 57s (remain 6m 0s) Loss: 0.1110 \n",
      "Epoch: [4][700/4343] Elapsed 1m 7s (remain 5m 50s) Loss: 0.1047 \n",
      "Epoch: [4][800/4343] Elapsed 1m 17s (remain 5m 41s) Loss: 0.1050 \n",
      "Epoch: [4][900/4343] Elapsed 1m 26s (remain 5m 31s) Loss: 0.1035 \n",
      "Epoch: [4][1000/4343] Elapsed 1m 36s (remain 5m 21s) Loss: 0.1038 \n",
      "Epoch: [4][1100/4343] Elapsed 1m 46s (remain 5m 12s) Loss: 0.1046 \n",
      "Epoch: [4][1200/4343] Elapsed 1m 55s (remain 5m 2s) Loss: 0.1029 \n",
      "Epoch: [4][1300/4343] Elapsed 2m 5s (remain 4m 52s) Loss: 0.1020 \n",
      "Epoch: [4][1400/4343] Elapsed 2m 14s (remain 4m 43s) Loss: 0.1029 \n",
      "Epoch: [4][1500/4343] Elapsed 2m 24s (remain 4m 33s) Loss: 0.1019 \n",
      "Epoch: [4][1600/4343] Elapsed 2m 34s (remain 4m 23s) Loss: 0.1014 \n",
      "Epoch: [4][1700/4343] Elapsed 2m 43s (remain 4m 14s) Loss: 0.1006 \n",
      "Epoch: [4][1800/4343] Elapsed 2m 53s (remain 4m 4s) Loss: 0.1016 \n",
      "Epoch: [4][1900/4343] Elapsed 3m 2s (remain 3m 55s) Loss: 0.1022 \n",
      "Epoch: [4][2000/4343] Elapsed 3m 12s (remain 3m 45s) Loss: 0.1012 \n",
      "Epoch: [4][2100/4343] Elapsed 3m 22s (remain 3m 35s) Loss: 0.1011 \n",
      "Epoch: [4][2200/4343] Elapsed 3m 31s (remain 3m 26s) Loss: 0.1004 \n",
      "Epoch: [4][2300/4343] Elapsed 3m 41s (remain 3m 16s) Loss: 0.1007 \n",
      "Epoch: [4][2400/4343] Elapsed 3m 51s (remain 3m 6s) Loss: 0.0994 \n",
      "Epoch: [4][2500/4343] Elapsed 4m 0s (remain 2m 57s) Loss: 0.1000 \n",
      "Epoch: [4][2600/4343] Elapsed 4m 10s (remain 2m 47s) Loss: 0.0990 \n",
      "Epoch: [4][2700/4343] Elapsed 4m 19s (remain 2m 38s) Loss: 0.0978 \n",
      "Epoch: [4][2800/4343] Elapsed 4m 29s (remain 2m 28s) Loss: 0.0971 \n",
      "Epoch: [4][2900/4343] Elapsed 4m 39s (remain 2m 18s) Loss: 0.0956 \n",
      "Epoch: [4][3000/4343] Elapsed 4m 48s (remain 2m 9s) Loss: 0.0949 \n",
      "Epoch: [4][3100/4343] Elapsed 4m 58s (remain 1m 59s) Loss: 0.0950 \n",
      "Epoch: [4][3200/4343] Elapsed 5m 7s (remain 1m 49s) Loss: 0.0952 \n",
      "Epoch: [4][3300/4343] Elapsed 5m 17s (remain 1m 40s) Loss: 0.0947 \n",
      "Epoch: [4][3400/4343] Elapsed 5m 27s (remain 1m 30s) Loss: 0.0956 \n",
      "Epoch: [4][3500/4343] Elapsed 5m 36s (remain 1m 21s) Loss: 0.0960 \n",
      "Epoch: [4][3600/4343] Elapsed 5m 46s (remain 1m 11s) Loss: 0.0953 \n",
      "Epoch: [4][3700/4343] Elapsed 5m 56s (remain 1m 1s) Loss: 0.0957 \n",
      "Epoch: [4][3800/4343] Elapsed 6m 5s (remain 0m 52s) Loss: 0.0960 \n",
      "Epoch: [4][3900/4343] Elapsed 6m 15s (remain 0m 42s) Loss: 0.0964 \n",
      "Epoch: [4][4000/4343] Elapsed 6m 24s (remain 0m 32s) Loss: 0.0964 \n",
      "Epoch: [4][4100/4343] Elapsed 6m 34s (remain 0m 23s) Loss: 0.0959 \n",
      "Epoch: [4][4200/4343] Elapsed 6m 44s (remain 0m 13s) Loss: 0.0961 \n",
      "Epoch: [4][4300/4343] Elapsed 6m 53s (remain 0m 4s) Loss: 0.0959 \n",
      "Epoch: [4][4342/4343] Elapsed 6m 57s (remain 0m 0s) Loss: 0.0959 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 3m 21s) Loss: 0.0130 \n",
      "EVAL: [100/1086] Elapsed 0m 2s (remain 0m 25s) Loss: 0.0851 \n",
      "EVAL: [200/1086] Elapsed 0m 5s (remain 0m 22s) Loss: 0.0936 \n",
      "EVAL: [300/1086] Elapsed 0m 7s (remain 0m 19s) Loss: 0.0868 \n",
      "EVAL: [400/1086] Elapsed 0m 9s (remain 0m 16s) Loss: 0.0929 \n",
      "EVAL: [500/1086] Elapsed 0m 12s (remain 0m 14s) Loss: 0.0904 \n",
      "EVAL: [600/1086] Elapsed 0m 14s (remain 0m 11s) Loss: 0.0873 \n",
      "EVAL: [700/1086] Elapsed 0m 17s (remain 0m 9s) Loss: 0.0823 \n",
      "EVAL: [800/1086] Elapsed 0m 19s (remain 0m 7s) Loss: 0.0844 \n",
      "EVAL: [900/1086] Elapsed 0m 22s (remain 0m 4s) Loss: 0.0839 \n",
      "EVAL: [1000/1086] Elapsed 0m 24s (remain 0m 2s) Loss: 0.0876 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.36421219319081555, thresh=0.023282372444280715\n",
      "score2 = 0.36421219319081555,  thresh=0.49999553788108364\n",
      "Epoch 4 - avg_train_loss: 0.0959  avg_val_loss: 0.0894  time: 445s\n",
      "Epoch 4 - Score: 0.36421219319081555\n",
      "Epoch 4 - Save Best Score: 0.3642 - Best Loss: 0.0894 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 26s (remain 0m 0s) Loss: 0.0894 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 4 result ==========\n",
      "Score1: 0.36421\n",
      "best border: 0.50000\n",
      "Score2: 0.36421\n",
      "========== CV ==========\n",
      "Score1: 0.77443\n",
      "best border: 0.01511\n",
      "Score2: 0.77987\n"
     ]
    }
   ],
   "source": [
    "##main\n",
    "\n",
    "oof_df = pd.DataFrame()\n",
    "for fold in range(CFG.n_splits): \n",
    "    _oof_df = train_loop(train, fold)\n",
    "    oof_df = pd.concat([oof_df, _oof_df])\n",
    "    LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "    get_result(_oof_df)\n",
    "        \n",
    "# CV result\n",
    "LOGGER.info(f\"========== CV ==========\")\n",
    "score, best_border = get_result(oof_df)\n",
    "    \n",
    "# Save OOF result\n",
    "oof_df.to_csv(OUTPUT_DIR + f\"oof_df_{CFG.version}_{score:<.5f}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df60abde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.023282372444280715, 0.015106569612934143)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "border, best_border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db831d56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: bert-base-uncased fold: 0 inference ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076ae03f4509442c9072c74996534f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: bert-base-uncased fold: 1 inference ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c047b9e33870473b852e48db75ef1e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: bert-base-uncased fold: 2 inference ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37dce7a0e274958bb4124c06c0ad96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: bert-base-uncased fold: 3 inference ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044a92dcd16840a7a0efe08398e87395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: bert-base-uncased fold: 4 inference ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5756272f7784cecaa0d51220163c385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inference\n",
    "predictions = inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37ba564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ensemble\n",
    "test[\"judgement\"] = predictions\n",
    "test.to_csv(OUTPUT_DIR + f\"predictions_{CFG.version}.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae54f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "predictions2 = np.where(predictions < border, 0, 1)\n",
    "sub[\"judgement\"] = predictions2\n",
    "sub.to_csv(OUTPUT_DIR + f\"submission_{CFG.version}.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-flesh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-amendment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
