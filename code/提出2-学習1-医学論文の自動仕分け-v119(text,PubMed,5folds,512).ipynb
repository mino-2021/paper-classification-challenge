{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e22c03b",
   "metadata": {},
   "source": [
    "## 医学論文の自動仕分けチャレンジ 推論1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c63c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/gdrive')\n",
    "\n",
    "#!cp /gdrive/MyDrive/Datasets/signate-471/train.csv .\n",
    "#!cp /gdrive/MyDrive/Datasets/signate-471/test.csv .\n",
    "#!cp /gdrive/MyDrive/Datasets/signate-471/sample_submit.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c56ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b63231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers as T\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3a8515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    batch_size = 5 #16\n",
    "    num_workers = 3 #4\n",
    "    max_length =  512 #256 #72\n",
    "    n_splits = 5\n",
    "    version = 119\n",
    "    model = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\" \n",
    "    \n",
    "    epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85045998",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../input/\"\n",
    "OUTPUT_DIR = \"../output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58bbf96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2c6927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c7e7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
    "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "594765c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 471\n",
    "seed_torch(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6107bd10",
   "metadata": {},
   "source": [
    "## データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b73b316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
    "test = pd.read_csv(DATA_DIR + \"test.csv\")\n",
    "sub = pd.read_csv(DATA_DIR + \"sample_submit.csv\", header=None)\n",
    "sub.columns = [\"id\", \"judgement\"]\n",
    "TARGET = \"judgement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "176e2360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023282372444280715\n"
     ]
    }
   ],
   "source": [
    "# この値を境に、モデルの出力を 0 と 1 にします。\n",
    "border = len(train[train[TARGET] == 1]) / len(train[TARGET])\n",
    "print(border)\n",
    "init_border = border"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7eab26",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e95fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # preprocess\n",
    "train[\"text\"] = train[\"title\"] + \" \" + train[\"abstract\"].fillna(\"\")\n",
    "test[\"text\"] = test[\"title\"] + \" \" + test[\"abstract\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "856008b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(train):\n",
    "\n",
    "    # 交差検証 用の番号を振ります。\n",
    "    Fold = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=seed) #5\n",
    "    for n, (train_index, val_index) in enumerate(Fold.split(train, train[TARGET])):\n",
    "        train.loc[val_index, \"fold\"] = int(n)\n",
    "    train[\"fold\"] = train[\"fold\"].astype(np.uint8)\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d675f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(test):\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ca7c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_train_data(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a92114",
   "metadata": {},
   "source": [
    "## データセット定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76116aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, df, model_name, include_labels=True):\n",
    "        #tokenizer = T.BertTokenizer.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.df = df\n",
    "        self.include_labels = include_labels\n",
    "\n",
    "        self.title = df[\"text\"].tolist()\n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.title,\n",
    "            padding = 'max_length',            \n",
    "            max_length = CFG.max_length,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        if self.include_labels:\n",
    "            self.labels = df[TARGET].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][idx])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][idx])\n",
    "\n",
    "        if self.include_labels:\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "            return input_ids, attention_mask, label\n",
    "\n",
    "        return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ebc85",
   "metadata": {},
   "source": [
    "## モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "774b39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        #self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model = T.BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model(input_ids=input_ids, attention_mask=attention_mask) #,labels=labels)\n",
    "        logits = out.logits\n",
    "        #out = self.sigmoid(out.logits).squeeze()\n",
    "        out = self.sigmoid(logits).squeeze()\n",
    "        #out = logits.squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f5491",
   "metadata": {},
   "source": [
    "## ツール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da98b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1cea44",
   "metadata": {},
   "source": [
    "## 学習補助関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf0c100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    start = end = time.time()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        batch_size = labels.size(0)\n",
    "        \n",
    "        y_preds = model(input_ids, attention_mask)\n",
    "        \n",
    "        loss = criterion(y_preds, labels)\n",
    "        \n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0 or step == (len(train_loader) - 1):\n",
    "            print(\n",
    "                f\"Epoch: [{epoch + 1}][{step}/{len(train_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(train_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "            )\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ba8db",
   "metadata": {},
   "source": [
    "## 評価 補助関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6a3064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    start = end = time.time()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(valid_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # record score\n",
    "        preds.append(y_preds.to(\"cpu\").numpy())\n",
    "\n",
    "        if step % 100 == 0 or step == (len(valid_loader) - 1):\n",
    "            print(\n",
    "                f\"EVAL: [{step}/{len(valid_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(valid_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "            )\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96cb54",
   "metadata": {},
   "source": [
    "## 推論関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89565ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    predictions = []\n",
    "\n",
    "    test_dataset = BaseDataset(test, CFG.model, include_labels=False)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    for fold in range(CFG.n_splits): #5\n",
    "    #for fold in [1,3,4,5]: #5\n",
    "        LOGGER.info(f\"========== model: bert-base-uncased fold: {fold} inference ==========\")\n",
    "        model = BaseModel(CFG.model)\n",
    "        model.to(device)\n",
    "        model.load_state_dict(torch.load(OUTPUT_DIR + f\"{CFG.version}_fold{fold}_best.pth\")[\"model\"])\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        \n",
    "        for i, (input_ids, attention_mask) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(input_ids, attention_mask)\n",
    "            preds.append(y_preds.to(\"cpu\").numpy())\n",
    "            \n",
    "        preds = np.concatenate(preds)\n",
    "        predictions.append(preds)\n",
    "        \n",
    "    predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca0e69",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27a668f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train, fold):\n",
    "\n",
    "    LOGGER.info(f\"=*========= fold: {fold} training ==========\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Data Loader\n",
    "    # ====================================================\n",
    "    trn_idx = train[train[\"fold\"] != fold].index\n",
    "    val_idx = train[train[\"fold\"] == fold].index\n",
    "    \n",
    "    train_folds = train.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = train.loc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = BaseDataset(train_folds, CFG.model)\n",
    "    valid_dataset = BaseDataset(valid_folds, CFG.model)\n",
    "    #print(\"DataLoader\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size, #16\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers, #4, \n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=CFG.batch_size, #16\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers, #4,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    #print(\"Model\")\n",
    "    # ====================================================\n",
    "    # Model\n",
    "    # ====================================================\n",
    "    model = BaseModel(CFG.model)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = T.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    #criterion = nn.BCEWithLogitsLoss()\n",
    "    # ====================================================\n",
    "    # Loop\n",
    "    # ====================================================\n",
    "    best_score = -1\n",
    "    best_loss = np.inf\n",
    "    #print(\"Loop\")\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, device)\n",
    "        \n",
    "        # eval\n",
    "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
    "        valid_labels = valid_folds[TARGET].values\n",
    "        \n",
    "        # scoring\n",
    "        score = fbeta_score(valid_labels, np.where(preds < init_border, 0, 1), beta=7.0)\n",
    "        LOGGER.info(f\"score1 = {score}, thresh={init_border}\")\n",
    "        \n",
    "        border = opt_fbeta_threshold(valid_labels, preds) ##最適化\n",
    "        \n",
    "        score = fbeta_score(valid_labels, np.where(preds < border, 0, 1), beta=7.0)\n",
    "        LOGGER.info(f\"score2 = {score},  thresh={border}\")\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        LOGGER.info(\n",
    "            f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\"\n",
    "        )\n",
    "        LOGGER.info(f\"Epoch {epoch+1} - Score: {score}\")\n",
    "\n",
    "        #if score >= best_score: ##\n",
    "        if avg_val_loss <= best_loss:\n",
    "        #if True:\n",
    "            best_loss = avg_val_loss\n",
    "            best_score = score\n",
    "            LOGGER.info(f\"Epoch {epoch+1} - Save Best Score: {best_score:.4f} - Best Loss: {best_loss:.4f} Model\")\n",
    "            torch.save(\n",
    "                {\"model\": model.state_dict(), \"preds\": preds}, OUTPUT_DIR + f\"{CFG.version}_fold{fold}_best.pth\"\n",
    "            )\n",
    "\n",
    "    check_point = torch.load(OUTPUT_DIR + f\"{CFG.version}_fold{fold}_best.pth\")\n",
    "\n",
    "    valid_folds[\"preds\"] = check_point[\"preds\"]\n",
    "\n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b707a",
   "metadata": {},
   "source": [
    "## メイン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "883abd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(result_df):\n",
    "    preds = result_df[\"preds\"].values\n",
    "    labels = result_df[\"judgement\"].values\n",
    "    \n",
    "    score = fbeta_score(labels, np.where(preds < border, 0, 1), beta=7.0)\n",
    "    LOGGER.info(f\"Score1: {score:<.5f}\")\n",
    "    \n",
    "    best_border = opt_fbeta_threshold(labels, preds)\n",
    "    LOGGER.info(f\"best border: {best_border:<.5f}\")\n",
    "    \n",
    "    score = fbeta_score(labels, np.where(preds < best_border, 0, 1), beta=7.0)\n",
    "    LOGGER.info(f\"Score2: {score:<.5f}\")\n",
    "    \n",
    "    return score, best_border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88058068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize, minimize_scalar\n",
    "def opt_fbeta_threshold(y_true, y_pred):\n",
    "    \"\"\"fbeta score計算時のthresholdを最適化\"\"\"\n",
    "    def opt_(x): \n",
    "        return -fbeta_score(y_true, y_pred >= x, beta=7)\n",
    "    \n",
    "    #result = minimize(opt_, x0=np.array([0.1]), method='Powell')\n",
    "    result = minimize_scalar(opt_, bounds=(0, 0.5), method='bounded')\n",
    "    \n",
    "    best_threshold = result['x'].item()\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4c60009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_confusion_matrix(\n",
    "        y_true,\n",
    "        pred_label,\n",
    "        height=.6,\n",
    "        labels=None):\n",
    "    \"\"\"混合行列をプロット \n",
    "    (https://www.guruguru.science/competitions/11/discussions/2fb11851-67d0-4e96-a4b1-5629b944f363/)\"\"\"\n",
    "    \n",
    "    conf = confusion_matrix(y_true=y_true,\n",
    "                            y_pred=pred_label,\n",
    "                            normalize='true')\n",
    "\n",
    "    n_labels = len(conf)\n",
    "    size = n_labels * height\n",
    "    fig, ax = plt.subplots(figsize=(size * 4, size * 3))\n",
    "    sns.heatmap(conf, cmap='Blues', ax=ax, annot=True, fmt='.2f')\n",
    "    ax.set_ylabel('Label')\n",
    "    ax.set_xlabel('Predict')\n",
    "\n",
    "    if labels is not None:\n",
    "        ax.set_yticklabels(labels)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.tick_params('y', labelrotation=0)\n",
    "        ax.tick_params('x', labelrotation=90)\n",
    "\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7d7b360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=*========= fold: 0 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/4343] Elapsed 0m 0s (remain 57m 28s) Loss: 0.6753 \n",
      "Epoch: [1][100/4343] Elapsed 0m 18s (remain 13m 1s) Loss: 0.1201 \n",
      "Epoch: [1][200/4343] Elapsed 0m 36s (remain 12m 31s) Loss: 0.1145 \n",
      "Epoch: [1][300/4343] Elapsed 0m 54s (remain 12m 14s) Loss: 0.1187 \n",
      "Epoch: [1][400/4343] Elapsed 1m 12s (remain 11m 54s) Loss: 0.1097 \n",
      "Epoch: [1][500/4343] Elapsed 1m 30s (remain 11m 37s) Loss: 0.1036 \n",
      "Epoch: [1][600/4343] Elapsed 1m 49s (remain 11m 20s) Loss: 0.1004 \n",
      "Epoch: [1][700/4343] Elapsed 2m 7s (remain 11m 1s) Loss: 0.0946 \n",
      "Epoch: [1][800/4343] Elapsed 2m 25s (remain 10m 41s) Loss: 0.0919 \n",
      "Epoch: [1][900/4343] Elapsed 2m 43s (remain 10m 22s) Loss: 0.0923 \n",
      "Epoch: [1][1000/4343] Elapsed 3m 0s (remain 10m 4s) Loss: 0.0898 \n",
      "Epoch: [1][1100/4343] Elapsed 3m 18s (remain 9m 45s) Loss: 0.0862 \n",
      "Epoch: [1][1200/4343] Elapsed 3m 36s (remain 9m 26s) Loss: 0.0863 \n",
      "Epoch: [1][1300/4343] Elapsed 3m 54s (remain 9m 8s) Loss: 0.0846 \n",
      "Epoch: [1][1400/4343] Elapsed 4m 12s (remain 8m 50s) Loss: 0.0847 \n",
      "Epoch: [1][1500/4343] Elapsed 4m 30s (remain 8m 31s) Loss: 0.0824 \n",
      "Epoch: [1][1600/4343] Elapsed 4m 48s (remain 8m 13s) Loss: 0.0813 \n",
      "Epoch: [1][1700/4343] Elapsed 5m 6s (remain 7m 55s) Loss: 0.0802 \n",
      "Epoch: [1][1800/4343] Elapsed 5m 24s (remain 7m 37s) Loss: 0.0784 \n",
      "Epoch: [1][1900/4343] Elapsed 5m 41s (remain 7m 19s) Loss: 0.0773 \n",
      "Epoch: [1][2000/4343] Elapsed 5m 59s (remain 7m 1s) Loss: 0.0773 \n",
      "Epoch: [1][2100/4343] Elapsed 6m 17s (remain 6m 43s) Loss: 0.0765 \n",
      "Epoch: [1][2200/4343] Elapsed 6m 35s (remain 6m 25s) Loss: 0.0755 \n",
      "Epoch: [1][2300/4343] Elapsed 6m 53s (remain 6m 7s) Loss: 0.0736 \n",
      "Epoch: [1][2400/4343] Elapsed 7m 11s (remain 5m 49s) Loss: 0.0734 \n",
      "Epoch: [1][2500/4343] Elapsed 7m 29s (remain 5m 31s) Loss: 0.0716 \n",
      "Epoch: [1][2600/4343] Elapsed 7m 47s (remain 5m 13s) Loss: 0.0716 \n",
      "Epoch: [1][2700/4343] Elapsed 8m 5s (remain 4m 54s) Loss: 0.0709 \n",
      "Epoch: [1][2800/4343] Elapsed 8m 23s (remain 4m 36s) Loss: 0.0699 \n",
      "Epoch: [1][2900/4343] Elapsed 8m 41s (remain 4m 18s) Loss: 0.0684 \n",
      "Epoch: [1][3000/4343] Elapsed 8m 58s (remain 4m 0s) Loss: 0.0673 \n",
      "Epoch: [1][3100/4343] Elapsed 9m 16s (remain 3m 43s) Loss: 0.0660 \n",
      "Epoch: [1][3200/4343] Elapsed 9m 34s (remain 3m 25s) Loss: 0.0649 \n",
      "Epoch: [1][3300/4343] Elapsed 9m 52s (remain 3m 7s) Loss: 0.0640 \n",
      "Epoch: [1][3400/4343] Elapsed 10m 10s (remain 2m 49s) Loss: 0.0643 \n",
      "Epoch: [1][3500/4343] Elapsed 10m 28s (remain 2m 31s) Loss: 0.0633 \n",
      "Epoch: [1][3600/4343] Elapsed 10m 46s (remain 2m 13s) Loss: 0.0634 \n",
      "Epoch: [1][3700/4343] Elapsed 11m 4s (remain 1m 55s) Loss: 0.0632 \n",
      "Epoch: [1][3800/4343] Elapsed 11m 22s (remain 1m 37s) Loss: 0.0636 \n",
      "Epoch: [1][3900/4343] Elapsed 11m 39s (remain 1m 19s) Loss: 0.0631 \n",
      "Epoch: [1][4000/4343] Elapsed 11m 57s (remain 1m 1s) Loss: 0.0629 \n",
      "Epoch: [1][4100/4343] Elapsed 12m 15s (remain 0m 43s) Loss: 0.0622 \n",
      "Epoch: [1][4200/4343] Elapsed 12m 33s (remain 0m 25s) Loss: 0.0617 \n",
      "Epoch: [1][4300/4343] Elapsed 12m 51s (remain 0m 7s) Loss: 0.0619 \n",
      "Epoch: [1][4342/4343] Elapsed 12m 58s (remain 0m 0s) Loss: 0.0620 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 7s) Loss: 0.0012 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0559 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0646 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.0518 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0656 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0597 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0641 \n",
      "EVAL: [700/1086] Elapsed 0m 36s (remain 0m 20s) Loss: 0.0629 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 14s) Loss: 0.0636 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0658 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0616 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.7509881422924901, thresh=0.023282372444280715\n",
      "score2 = 0.7734806629834254,  thresh=0.017227239527402384\n",
      "Epoch 1 - avg_train_loss: 0.0620  avg_val_loss: 0.0614  time: 836s\n",
      "Epoch 1 - Score: 0.7734806629834254\n",
      "Epoch 1 - Save Best Score: 0.7735 - Best Loss: 0.0614 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0614 \n",
      "Epoch: [2][0/4343] Elapsed 0m 0s (remain 25m 11s) Loss: 0.0014 \n",
      "Epoch: [2][100/4343] Elapsed 0m 18s (remain 12m 46s) Loss: 0.0364 \n",
      "Epoch: [2][200/4343] Elapsed 0m 36s (remain 12m 24s) Loss: 0.0450 \n",
      "Epoch: [2][300/4343] Elapsed 0m 54s (remain 12m 5s) Loss: 0.0417 \n",
      "Epoch: [2][400/4343] Elapsed 1m 11s (remain 11m 47s) Loss: 0.0407 \n",
      "Epoch: [2][500/4343] Elapsed 1m 29s (remain 11m 28s) Loss: 0.0428 \n",
      "Epoch: [2][600/4343] Elapsed 1m 47s (remain 11m 10s) Loss: 0.0421 \n",
      "Epoch: [2][700/4343] Elapsed 2m 5s (remain 10m 52s) Loss: 0.0411 \n",
      "Epoch: [2][800/4343] Elapsed 2m 23s (remain 10m 34s) Loss: 0.0433 \n",
      "Epoch: [2][900/4343] Elapsed 2m 41s (remain 10m 16s) Loss: 0.0431 \n",
      "Epoch: [2][1000/4343] Elapsed 2m 59s (remain 9m 58s) Loss: 0.0437 \n",
      "Epoch: [2][1100/4343] Elapsed 3m 17s (remain 9m 40s) Loss: 0.0421 \n",
      "Epoch: [2][1200/4343] Elapsed 3m 34s (remain 9m 22s) Loss: 0.0443 \n",
      "Epoch: [2][1300/4343] Elapsed 3m 52s (remain 9m 4s) Loss: 0.0444 \n",
      "Epoch: [2][1400/4343] Elapsed 4m 10s (remain 8m 46s) Loss: 0.0440 \n",
      "Epoch: [2][1500/4343] Elapsed 4m 28s (remain 8m 28s) Loss: 0.0433 \n",
      "Epoch: [2][1600/4343] Elapsed 4m 46s (remain 8m 10s) Loss: 0.0441 \n",
      "Epoch: [2][1700/4343] Elapsed 5m 4s (remain 7m 52s) Loss: 0.0426 \n",
      "Epoch: [2][1800/4343] Elapsed 5m 22s (remain 7m 34s) Loss: 0.0420 \n",
      "Epoch: [2][1900/4343] Elapsed 5m 40s (remain 7m 16s) Loss: 0.0426 \n",
      "Epoch: [2][2000/4343] Elapsed 5m 58s (remain 6m 59s) Loss: 0.0431 \n",
      "Epoch: [2][2100/4343] Elapsed 6m 15s (remain 6m 41s) Loss: 0.0427 \n",
      "Epoch: [2][2200/4343] Elapsed 6m 33s (remain 6m 23s) Loss: 0.0426 \n",
      "Epoch: [2][2300/4343] Elapsed 6m 51s (remain 6m 5s) Loss: 0.0430 \n",
      "Epoch: [2][2400/4343] Elapsed 7m 9s (remain 5m 47s) Loss: 0.0436 \n",
      "Epoch: [2][2500/4343] Elapsed 7m 27s (remain 5m 29s) Loss: 0.0431 \n",
      "Epoch: [2][2600/4343] Elapsed 7m 45s (remain 5m 11s) Loss: 0.0428 \n",
      "Epoch: [2][2700/4343] Elapsed 8m 3s (remain 4m 53s) Loss: 0.0424 \n",
      "Epoch: [2][2800/4343] Elapsed 8m 21s (remain 4m 35s) Loss: 0.0421 \n",
      "Epoch: [2][2900/4343] Elapsed 8m 38s (remain 4m 17s) Loss: 0.0426 \n",
      "Epoch: [2][3000/4343] Elapsed 8m 56s (remain 4m 0s) Loss: 0.0421 \n",
      "Epoch: [2][3100/4343] Elapsed 9m 14s (remain 3m 42s) Loss: 0.0424 \n",
      "Epoch: [2][3200/4343] Elapsed 9m 32s (remain 3m 24s) Loss: 0.0420 \n",
      "Epoch: [2][3300/4343] Elapsed 9m 50s (remain 3m 6s) Loss: 0.0417 \n",
      "Epoch: [2][3400/4343] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0419 \n",
      "Epoch: [2][3500/4343] Elapsed 10m 26s (remain 2m 30s) Loss: 0.0414 \n",
      "Epoch: [2][3600/4343] Elapsed 10m 44s (remain 2m 12s) Loss: 0.0412 \n",
      "Epoch: [2][3700/4343] Elapsed 11m 2s (remain 1m 54s) Loss: 0.0409 \n",
      "Epoch: [2][3800/4343] Elapsed 11m 19s (remain 1m 36s) Loss: 0.0405 \n",
      "Epoch: [2][3900/4343] Elapsed 11m 37s (remain 1m 19s) Loss: 0.0404 \n",
      "Epoch: [2][4000/4343] Elapsed 11m 55s (remain 1m 1s) Loss: 0.0403 \n",
      "Epoch: [2][4100/4343] Elapsed 12m 13s (remain 0m 43s) Loss: 0.0401 \n",
      "Epoch: [2][4200/4343] Elapsed 12m 31s (remain 0m 25s) Loss: 0.0407 \n",
      "Epoch: [2][4300/4343] Elapsed 12m 49s (remain 0m 7s) Loss: 0.0400 \n",
      "Epoch: [2][4342/4343] Elapsed 12m 56s (remain 0m 0s) Loss: 0.0398 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 2s) Loss: 0.0009 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0253 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0357 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.0288 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0361 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0335 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0395 \n",
      "EVAL: [700/1086] Elapsed 0m 36s (remain 0m 20s) Loss: 0.0415 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 14s) Loss: 0.0428 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0448 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0415 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8250825082508251, thresh=0.023282372444280715\n",
      "score2 = 0.9033353922174182,  thresh=0.0038147669210738446\n",
      "Epoch 2 - avg_train_loss: 0.0398  avg_val_loss: 0.0441  time: 834s\n",
      "Epoch 2 - Score: 0.9033353922174182\n",
      "Epoch 2 - Save Best Score: 0.9033 - Best Loss: 0.0441 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0441 \n",
      "Epoch: [3][0/4343] Elapsed 0m 0s (remain 26m 56s) Loss: 0.0008 \n",
      "Epoch: [3][100/4343] Elapsed 0m 18s (remain 12m 46s) Loss: 0.0147 \n",
      "Epoch: [3][200/4343] Elapsed 0m 36s (remain 12m 24s) Loss: 0.0135 \n",
      "Epoch: [3][300/4343] Elapsed 0m 54s (remain 12m 5s) Loss: 0.0238 \n",
      "Epoch: [3][400/4343] Elapsed 1m 11s (remain 11m 47s) Loss: 0.0276 \n",
      "Epoch: [3][500/4343] Elapsed 1m 29s (remain 11m 28s) Loss: 0.0275 \n",
      "Epoch: [3][600/4343] Elapsed 1m 47s (remain 11m 10s) Loss: 0.0262 \n",
      "Epoch: [3][700/4343] Elapsed 2m 5s (remain 10m 52s) Loss: 0.0271 \n",
      "Epoch: [3][800/4343] Elapsed 2m 23s (remain 10m 34s) Loss: 0.0284 \n",
      "Epoch: [3][900/4343] Elapsed 2m 41s (remain 10m 16s) Loss: 0.0279 \n",
      "Epoch: [3][1000/4343] Elapsed 2m 59s (remain 9m 58s) Loss: 0.0289 \n",
      "Epoch: [3][1100/4343] Elapsed 3m 17s (remain 9m 40s) Loss: 0.0295 \n",
      "Epoch: [3][1200/4343] Elapsed 3m 35s (remain 9m 22s) Loss: 0.0286 \n",
      "Epoch: [3][1300/4343] Elapsed 3m 52s (remain 9m 4s) Loss: 0.0295 \n",
      "Epoch: [3][1400/4343] Elapsed 4m 10s (remain 8m 46s) Loss: 0.0301 \n",
      "Epoch: [3][1500/4343] Elapsed 4m 28s (remain 8m 28s) Loss: 0.0300 \n",
      "Epoch: [3][1600/4343] Elapsed 4m 46s (remain 8m 10s) Loss: 0.0305 \n",
      "Epoch: [3][1700/4343] Elapsed 5m 4s (remain 7m 52s) Loss: 0.0317 \n",
      "Epoch: [3][1800/4343] Elapsed 5m 22s (remain 7m 34s) Loss: 0.0318 \n",
      "Epoch: [3][1900/4343] Elapsed 5m 40s (remain 7m 17s) Loss: 0.0323 \n",
      "Epoch: [3][2000/4343] Elapsed 5m 58s (remain 6m 59s) Loss: 0.0325 \n",
      "Epoch: [3][2100/4343] Elapsed 6m 15s (remain 6m 41s) Loss: 0.0321 \n",
      "Epoch: [3][2200/4343] Elapsed 6m 33s (remain 6m 23s) Loss: 0.0329 \n",
      "Epoch: [3][2300/4343] Elapsed 6m 51s (remain 6m 5s) Loss: 0.0333 \n",
      "Epoch: [3][2400/4343] Elapsed 7m 9s (remain 5m 47s) Loss: 0.0336 \n",
      "Epoch: [3][2500/4343] Elapsed 7m 27s (remain 5m 29s) Loss: 0.0338 \n",
      "Epoch: [3][2600/4343] Elapsed 7m 45s (remain 5m 11s) Loss: 0.0341 \n",
      "Epoch: [3][2700/4343] Elapsed 8m 3s (remain 4m 53s) Loss: 0.0340 \n",
      "Epoch: [3][2800/4343] Elapsed 8m 21s (remain 4m 35s) Loss: 0.0343 \n",
      "Epoch: [3][2900/4343] Elapsed 8m 39s (remain 4m 17s) Loss: 0.0338 \n",
      "Epoch: [3][3000/4343] Elapsed 8m 56s (remain 4m 0s) Loss: 0.0333 \n",
      "Epoch: [3][3100/4343] Elapsed 9m 14s (remain 3m 42s) Loss: 0.0327 \n",
      "Epoch: [3][3200/4343] Elapsed 9m 32s (remain 3m 24s) Loss: 0.0328 \n",
      "Epoch: [3][3300/4343] Elapsed 9m 50s (remain 3m 6s) Loss: 0.0322 \n",
      "Epoch: [3][3400/4343] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0315 \n",
      "Epoch: [3][3500/4343] Elapsed 10m 26s (remain 2m 30s) Loss: 0.0315 \n",
      "Epoch: [3][3600/4343] Elapsed 10m 44s (remain 2m 12s) Loss: 0.0319 \n",
      "Epoch: [3][3700/4343] Elapsed 11m 2s (remain 1m 54s) Loss: 0.0317 \n",
      "Epoch: [3][3800/4343] Elapsed 11m 20s (remain 1m 36s) Loss: 0.0320 \n",
      "Epoch: [3][3900/4343] Elapsed 11m 37s (remain 1m 19s) Loss: 0.0321 \n",
      "Epoch: [3][4000/4343] Elapsed 11m 55s (remain 1m 1s) Loss: 0.0320 \n",
      "Epoch: [3][4100/4343] Elapsed 12m 13s (remain 0m 43s) Loss: 0.0316 \n",
      "Epoch: [3][4200/4343] Elapsed 12m 31s (remain 0m 25s) Loss: 0.0316 \n",
      "Epoch: [3][4300/4343] Elapsed 12m 49s (remain 0m 7s) Loss: 0.0316 \n",
      "Epoch: [3][4342/4343] Elapsed 12m 56s (remain 0m 0s) Loss: 0.0316 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 2s) Loss: 0.0003 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0212 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0340 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.0293 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0384 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0388 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0473 \n",
      "EVAL: [700/1086] Elapsed 0m 36s (remain 0m 20s) Loss: 0.0477 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 14s) Loss: 0.0480 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0497 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0473 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8838579624748023, thresh=0.023282372444280715\n",
      "score2 = 0.8825367072789754,  thresh=0.2814475457077109\n",
      "Epoch 3 - avg_train_loss: 0.0316  avg_val_loss: 0.0481  time: 834s\n",
      "Epoch 3 - Score: 0.8825367072789754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0481 \n",
      "Epoch: [4][0/4343] Elapsed 0m 0s (remain 24m 43s) Loss: 0.0003 \n",
      "Epoch: [4][100/4343] Elapsed 0m 18s (remain 12m 49s) Loss: 0.0138 \n",
      "Epoch: [4][200/4343] Elapsed 0m 36s (remain 12m 34s) Loss: 0.0096 \n",
      "Epoch: [4][300/4343] Elapsed 0m 54s (remain 12m 14s) Loss: 0.0166 \n",
      "Epoch: [4][400/4343] Elapsed 1m 13s (remain 12m 0s) Loss: 0.0231 \n",
      "Epoch: [4][500/4343] Elapsed 1m 31s (remain 11m 41s) Loss: 0.0242 \n",
      "Epoch: [4][600/4343] Elapsed 1m 49s (remain 11m 21s) Loss: 0.0285 \n",
      "Epoch: [4][700/4343] Elapsed 2m 7s (remain 11m 3s) Loss: 0.0294 \n",
      "Epoch: [4][800/4343] Elapsed 2m 25s (remain 10m 44s) Loss: 0.0283 \n",
      "Epoch: [4][900/4343] Elapsed 2m 44s (remain 10m 27s) Loss: 0.0291 \n",
      "Epoch: [4][1000/4343] Elapsed 3m 2s (remain 10m 8s) Loss: 0.0283 \n",
      "Epoch: [4][1100/4343] Elapsed 3m 20s (remain 9m 49s) Loss: 0.0288 \n",
      "Epoch: [4][1200/4343] Elapsed 3m 38s (remain 9m 31s) Loss: 0.0289 \n",
      "Epoch: [4][1300/4343] Elapsed 3m 56s (remain 9m 13s) Loss: 0.0284 \n",
      "Epoch: [4][1400/4343] Elapsed 4m 15s (remain 8m 55s) Loss: 0.0280 \n",
      "Epoch: [4][1500/4343] Elapsed 4m 33s (remain 8m 37s) Loss: 0.0282 \n",
      "Epoch: [4][1600/4343] Elapsed 4m 52s (remain 8m 20s) Loss: 0.0281 \n",
      "Epoch: [4][1700/4343] Elapsed 5m 10s (remain 8m 2s) Loss: 0.0284 \n",
      "Epoch: [4][1800/4343] Elapsed 5m 29s (remain 7m 44s) Loss: 0.0286 \n",
      "Epoch: [4][1900/4343] Elapsed 5m 47s (remain 7m 26s) Loss: 0.0286 \n",
      "Epoch: [4][2000/4343] Elapsed 6m 5s (remain 7m 7s) Loss: 0.0286 \n",
      "Epoch: [4][2100/4343] Elapsed 6m 23s (remain 6m 49s) Loss: 0.0277 \n",
      "Epoch: [4][2200/4343] Elapsed 6m 41s (remain 6m 31s) Loss: 0.0273 \n",
      "Epoch: [4][2300/4343] Elapsed 7m 0s (remain 6m 12s) Loss: 0.0271 \n",
      "Epoch: [4][2400/4343] Elapsed 7m 18s (remain 5m 54s) Loss: 0.0275 \n",
      "Epoch: [4][2500/4343] Elapsed 7m 36s (remain 5m 36s) Loss: 0.0272 \n",
      "Epoch: [4][2600/4343] Elapsed 7m 55s (remain 5m 18s) Loss: 0.0284 \n",
      "Epoch: [4][2700/4343] Elapsed 8m 13s (remain 5m 0s) Loss: 0.0283 \n",
      "Epoch: [4][2800/4343] Elapsed 8m 32s (remain 4m 42s) Loss: 0.0286 \n",
      "Epoch: [4][2900/4343] Elapsed 8m 51s (remain 4m 23s) Loss: 0.0289 \n",
      "Epoch: [4][3000/4343] Elapsed 9m 9s (remain 4m 5s) Loss: 0.0291 \n",
      "Epoch: [4][3100/4343] Elapsed 9m 28s (remain 3m 47s) Loss: 0.0286 \n",
      "Epoch: [4][3200/4343] Elapsed 9m 46s (remain 3m 29s) Loss: 0.0293 \n",
      "Epoch: [4][3300/4343] Elapsed 10m 5s (remain 3m 11s) Loss: 0.0292 \n",
      "Epoch: [4][3400/4343] Elapsed 10m 23s (remain 2m 52s) Loss: 0.0298 \n",
      "Epoch: [4][3500/4343] Elapsed 10m 41s (remain 2m 34s) Loss: 0.0304 \n",
      "Epoch: [4][3600/4343] Elapsed 11m 0s (remain 2m 16s) Loss: 0.0307 \n",
      "Epoch: [4][3700/4343] Elapsed 11m 18s (remain 1m 57s) Loss: 0.0304 \n",
      "Epoch: [4][3800/4343] Elapsed 11m 36s (remain 1m 39s) Loss: 0.0301 \n",
      "Epoch: [4][3900/4343] Elapsed 11m 54s (remain 1m 20s) Loss: 0.0298 \n",
      "Epoch: [4][4000/4343] Elapsed 12m 13s (remain 1m 2s) Loss: 0.0295 \n",
      "Epoch: [4][4100/4343] Elapsed 12m 31s (remain 0m 44s) Loss: 0.0291 \n",
      "Epoch: [4][4200/4343] Elapsed 12m 49s (remain 0m 26s) Loss: 0.0289 \n",
      "Epoch: [4][4300/4343] Elapsed 13m 7s (remain 0m 7s) Loss: 0.0292 \n",
      "Epoch: [4][4342/4343] Elapsed 13m 15s (remain 0m 0s) Loss: 0.0291 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 10s) Loss: 0.0004 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 54s) Loss: 0.0216 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 48s) Loss: 0.0373 \n",
      "EVAL: [300/1086] Elapsed 0m 16s (remain 0m 42s) Loss: 0.0319 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0398 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 31s) Loss: 0.0368 \n",
      "EVAL: [600/1086] Elapsed 0m 32s (remain 0m 25s) Loss: 0.0444 \n",
      "EVAL: [700/1086] Elapsed 0m 37s (remain 0m 20s) Loss: 0.0488 \n",
      "EVAL: [800/1086] Elapsed 0m 43s (remain 0m 15s) Loss: 0.0492 \n",
      "EVAL: [900/1086] Elapsed 0m 48s (remain 0m 9s) Loss: 0.0491 \n",
      "EVAL: [1000/1086] Elapsed 0m 53s (remain 0m 4s) Loss: 0.0475 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8501013882389643, thresh=0.023282372444280715\n",
      "score2 = 0.8506321211175278,  thresh=0.03607754505588191\n",
      "Epoch 4 - avg_train_loss: 0.0291  avg_val_loss: 0.0477  time: 854s\n",
      "Epoch 4 - Score: 0.8506321211175278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 58s (remain 0m 0s) Loss: 0.0477 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 result ==========\n",
      "Score1: 0.82508\n",
      "best border: 0.00381\n",
      "Score2: 0.90334\n",
      "=*========= fold: 1 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/4343] Elapsed 0m 0s (remain 26m 44s) Loss: 0.6946 \n",
      "Epoch: [1][100/4343] Elapsed 0m 18s (remain 12m 48s) Loss: 0.1549 \n",
      "Epoch: [1][200/4343] Elapsed 0m 36s (remain 12m 28s) Loss: 0.1387 \n",
      "Epoch: [1][300/4343] Elapsed 0m 54s (remain 12m 14s) Loss: 0.1293 \n",
      "Epoch: [1][400/4343] Elapsed 1m 12s (remain 11m 57s) Loss: 0.1231 \n",
      "Epoch: [1][500/4343] Elapsed 1m 31s (remain 11m 39s) Loss: 0.1190 \n",
      "Epoch: [1][600/4343] Elapsed 1m 49s (remain 11m 22s) Loss: 0.1163 \n",
      "Epoch: [1][700/4343] Elapsed 2m 7s (remain 11m 4s) Loss: 0.1120 \n",
      "Epoch: [1][800/4343] Elapsed 2m 25s (remain 10m 44s) Loss: 0.1052 \n",
      "Epoch: [1][900/4343] Elapsed 2m 44s (remain 10m 27s) Loss: 0.1054 \n",
      "Epoch: [1][1000/4343] Elapsed 3m 2s (remain 10m 8s) Loss: 0.1005 \n",
      "Epoch: [1][1100/4343] Elapsed 3m 20s (remain 9m 50s) Loss: 0.0980 \n",
      "Epoch: [1][1200/4343] Elapsed 3m 38s (remain 9m 32s) Loss: 0.0960 \n",
      "Epoch: [1][1300/4343] Elapsed 3m 57s (remain 9m 14s) Loss: 0.0928 \n",
      "Epoch: [1][1400/4343] Elapsed 4m 15s (remain 8m 57s) Loss: 0.0913 \n",
      "Epoch: [1][1500/4343] Elapsed 4m 33s (remain 8m 38s) Loss: 0.0905 \n",
      "Epoch: [1][1600/4343] Elapsed 4m 52s (remain 8m 20s) Loss: 0.0889 \n",
      "Epoch: [1][1700/4343] Elapsed 5m 10s (remain 8m 1s) Loss: 0.0881 \n",
      "Epoch: [1][1800/4343] Elapsed 5m 28s (remain 7m 43s) Loss: 0.0865 \n",
      "Epoch: [1][1900/4343] Elapsed 5m 47s (remain 7m 26s) Loss: 0.0842 \n",
      "Epoch: [1][2000/4343] Elapsed 6m 6s (remain 7m 9s) Loss: 0.0822 \n",
      "Epoch: [1][2100/4343] Elapsed 6m 25s (remain 6m 50s) Loss: 0.0831 \n",
      "Epoch: [1][2200/4343] Elapsed 6m 43s (remain 6m 32s) Loss: 0.0816 \n",
      "Epoch: [1][2300/4343] Elapsed 7m 1s (remain 6m 14s) Loss: 0.0813 \n",
      "Epoch: [1][2400/4343] Elapsed 7m 19s (remain 5m 55s) Loss: 0.0807 \n",
      "Epoch: [1][2500/4343] Elapsed 7m 38s (remain 5m 37s) Loss: 0.0784 \n",
      "Epoch: [1][2600/4343] Elapsed 7m 55s (remain 5m 18s) Loss: 0.0778 \n",
      "Epoch: [1][2700/4343] Elapsed 8m 13s (remain 5m 0s) Loss: 0.0776 \n",
      "Epoch: [1][2800/4343] Elapsed 8m 32s (remain 4m 42s) Loss: 0.0761 \n",
      "Epoch: [1][2900/4343] Elapsed 8m 50s (remain 4m 23s) Loss: 0.0757 \n",
      "Epoch: [1][3000/4343] Elapsed 9m 9s (remain 4m 5s) Loss: 0.0756 \n",
      "Epoch: [1][3100/4343] Elapsed 9m 28s (remain 3m 47s) Loss: 0.0740 \n",
      "Epoch: [1][3200/4343] Elapsed 9m 47s (remain 3m 29s) Loss: 0.0735 \n",
      "Epoch: [1][3300/4343] Elapsed 10m 5s (remain 3m 11s) Loss: 0.0725 \n",
      "Epoch: [1][3400/4343] Elapsed 10m 24s (remain 2m 52s) Loss: 0.0726 \n",
      "Epoch: [1][3500/4343] Elapsed 10m 42s (remain 2m 34s) Loss: 0.0727 \n",
      "Epoch: [1][3600/4343] Elapsed 11m 0s (remain 2m 16s) Loss: 0.0717 \n",
      "Epoch: [1][3700/4343] Elapsed 11m 18s (remain 1m 57s) Loss: 0.0711 \n",
      "Epoch: [1][3800/4343] Elapsed 11m 36s (remain 1m 39s) Loss: 0.0701 \n",
      "Epoch: [1][3900/4343] Elapsed 11m 56s (remain 1m 21s) Loss: 0.0708 \n",
      "Epoch: [1][4000/4343] Elapsed 12m 14s (remain 1m 2s) Loss: 0.0706 \n",
      "Epoch: [1][4100/4343] Elapsed 12m 32s (remain 0m 44s) Loss: 0.0704 \n",
      "Epoch: [1][4200/4343] Elapsed 12m 50s (remain 0m 26s) Loss: 0.0701 \n",
      "Epoch: [1][4300/4343] Elapsed 13m 8s (remain 0m 7s) Loss: 0.0694 \n",
      "Epoch: [1][4342/4343] Elapsed 13m 15s (remain 0m 0s) Loss: 0.0689 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 12s) Loss: 0.0030 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0469 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0470 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.0509 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0522 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0474 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0436 \n",
      "EVAL: [700/1086] Elapsed 0m 37s (remain 0m 20s) Loss: 0.0414 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 15s) Loss: 0.0398 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0407 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0428 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.9143553794574825, thresh=0.023282372444280715\n",
      "score2 = 0.9271621421154131,  thresh=0.010693158304877623\n",
      "Epoch 1 - avg_train_loss: 0.0689  avg_val_loss: 0.0429  time: 853s\n",
      "Epoch 1 - Score: 0.9271621421154131\n",
      "Epoch 1 - Save Best Score: 0.9272 - Best Loss: 0.0429 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0429 \n",
      "Epoch: [2][0/4343] Elapsed 0m 0s (remain 26m 10s) Loss: 0.1540 \n",
      "Epoch: [2][100/4343] Elapsed 0m 18s (remain 12m 49s) Loss: 0.0553 \n",
      "Epoch: [2][200/4343] Elapsed 0m 37s (remain 12m 45s) Loss: 0.0455 \n",
      "Epoch: [2][300/4343] Elapsed 0m 55s (remain 12m 21s) Loss: 0.0416 \n",
      "Epoch: [2][400/4343] Elapsed 1m 13s (remain 12m 1s) Loss: 0.0416 \n",
      "Epoch: [2][500/4343] Elapsed 1m 31s (remain 11m 40s) Loss: 0.0436 \n",
      "Epoch: [2][600/4343] Elapsed 1m 49s (remain 11m 21s) Loss: 0.0414 \n",
      "Epoch: [2][700/4343] Elapsed 2m 7s (remain 11m 1s) Loss: 0.0400 \n",
      "Epoch: [2][800/4343] Elapsed 2m 25s (remain 10m 43s) Loss: 0.0419 \n",
      "Epoch: [2][900/4343] Elapsed 2m 43s (remain 10m 25s) Loss: 0.0445 \n",
      "Epoch: [2][1000/4343] Elapsed 3m 1s (remain 10m 7s) Loss: 0.0456 \n",
      "Epoch: [2][1100/4343] Elapsed 3m 20s (remain 9m 49s) Loss: 0.0470 \n",
      "Epoch: [2][1200/4343] Elapsed 3m 38s (remain 9m 30s) Loss: 0.0457 \n",
      "Epoch: [2][1300/4343] Elapsed 3m 56s (remain 9m 12s) Loss: 0.0492 \n",
      "Epoch: [2][1400/4343] Elapsed 4m 14s (remain 8m 54s) Loss: 0.0483 \n",
      "Epoch: [2][1500/4343] Elapsed 4m 33s (remain 8m 36s) Loss: 0.0467 \n",
      "Epoch: [2][1600/4343] Elapsed 4m 51s (remain 8m 18s) Loss: 0.0457 \n",
      "Epoch: [2][1700/4343] Elapsed 5m 9s (remain 8m 1s) Loss: 0.0464 \n",
      "Epoch: [2][1800/4343] Elapsed 5m 27s (remain 7m 42s) Loss: 0.0456 \n",
      "Epoch: [2][1900/4343] Elapsed 5m 45s (remain 7m 24s) Loss: 0.0443 \n",
      "Epoch: [2][2000/4343] Elapsed 6m 3s (remain 7m 5s) Loss: 0.0442 \n",
      "Epoch: [2][2100/4343] Elapsed 6m 21s (remain 6m 47s) Loss: 0.0438 \n",
      "Epoch: [2][2200/4343] Elapsed 6m 40s (remain 6m 29s) Loss: 0.0436 \n",
      "Epoch: [2][2300/4343] Elapsed 6m 59s (remain 6m 12s) Loss: 0.0431 \n",
      "Epoch: [2][2400/4343] Elapsed 7m 17s (remain 5m 53s) Loss: 0.0433 \n",
      "Epoch: [2][2500/4343] Elapsed 7m 35s (remain 5m 35s) Loss: 0.0428 \n",
      "Epoch: [2][2600/4343] Elapsed 7m 54s (remain 5m 17s) Loss: 0.0429 \n",
      "Epoch: [2][2700/4343] Elapsed 8m 12s (remain 4m 59s) Loss: 0.0429 \n",
      "Epoch: [2][2800/4343] Elapsed 8m 30s (remain 4m 41s) Loss: 0.0418 \n",
      "Epoch: [2][2900/4343] Elapsed 8m 48s (remain 4m 22s) Loss: 0.0413 \n",
      "Epoch: [2][3000/4343] Elapsed 9m 6s (remain 4m 4s) Loss: 0.0417 \n",
      "Epoch: [2][3100/4343] Elapsed 9m 24s (remain 3m 46s) Loss: 0.0412 \n",
      "Epoch: [2][3200/4343] Elapsed 9m 42s (remain 3m 27s) Loss: 0.0411 \n",
      "Epoch: [2][3300/4343] Elapsed 10m 0s (remain 3m 9s) Loss: 0.0410 \n",
      "Epoch: [2][3400/4343] Elapsed 10m 18s (remain 2m 51s) Loss: 0.0409 \n",
      "Epoch: [2][3500/4343] Elapsed 10m 36s (remain 2m 33s) Loss: 0.0406 \n",
      "Epoch: [2][3600/4343] Elapsed 10m 54s (remain 2m 14s) Loss: 0.0411 \n",
      "Epoch: [2][3700/4343] Elapsed 11m 13s (remain 1m 56s) Loss: 0.0409 \n",
      "Epoch: [2][3800/4343] Elapsed 11m 31s (remain 1m 38s) Loss: 0.0407 \n",
      "Epoch: [2][3900/4343] Elapsed 11m 50s (remain 1m 20s) Loss: 0.0403 \n",
      "Epoch: [2][4000/4343] Elapsed 12m 8s (remain 1m 2s) Loss: 0.0398 \n",
      "Epoch: [2][4100/4343] Elapsed 12m 26s (remain 0m 44s) Loss: 0.0404 \n",
      "Epoch: [2][4200/4343] Elapsed 12m 44s (remain 0m 25s) Loss: 0.0406 \n",
      "Epoch: [2][4300/4343] Elapsed 13m 2s (remain 0m 7s) Loss: 0.0405 \n",
      "Epoch: [2][4342/4343] Elapsed 13m 9s (remain 0m 0s) Loss: 0.0403 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 19s) Loss: 0.0004 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0311 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0354 \n",
      "EVAL: [300/1086] Elapsed 0m 16s (remain 0m 41s) Loss: 0.0405 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0503 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0448 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0419 \n",
      "EVAL: [700/1086] Elapsed 0m 37s (remain 0m 20s) Loss: 0.0398 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 15s) Loss: 0.0376 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0380 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0405 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.9044129112739747, thresh=0.023282372444280715\n",
      "score2 = 0.930769230769231,  thresh=0.002756214898647213\n",
      "Epoch 2 - avg_train_loss: 0.0403  avg_val_loss: 0.0391  time: 847s\n",
      "Epoch 2 - Score: 0.930769230769231\n",
      "Epoch 2 - Save Best Score: 0.9308 - Best Loss: 0.0391 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0391 \n",
      "Epoch: [3][0/4343] Elapsed 0m 0s (remain 27m 5s) Loss: 0.0055 \n",
      "Epoch: [3][100/4343] Elapsed 0m 18s (remain 12m 49s) Loss: 0.0295 \n",
      "Epoch: [3][200/4343] Elapsed 0m 36s (remain 12m 26s) Loss: 0.0264 \n",
      "Epoch: [3][300/4343] Elapsed 0m 54s (remain 12m 6s) Loss: 0.0229 \n",
      "Epoch: [3][400/4343] Elapsed 1m 11s (remain 11m 47s) Loss: 0.0210 \n",
      "Epoch: [3][500/4343] Elapsed 1m 29s (remain 11m 29s) Loss: 0.0235 \n",
      "Epoch: [3][600/4343] Elapsed 1m 47s (remain 11m 10s) Loss: 0.0254 \n",
      "Epoch: [3][700/4343] Elapsed 2m 5s (remain 10m 52s) Loss: 0.0294 \n",
      "Epoch: [3][800/4343] Elapsed 2m 23s (remain 10m 34s) Loss: 0.0271 \n",
      "Epoch: [3][900/4343] Elapsed 2m 41s (remain 10m 16s) Loss: 0.0271 \n",
      "Epoch: [3][1000/4343] Elapsed 2m 59s (remain 9m 59s) Loss: 0.0297 \n",
      "Epoch: [3][1100/4343] Elapsed 3m 17s (remain 9m 41s) Loss: 0.0296 \n",
      "Epoch: [3][1200/4343] Elapsed 3m 35s (remain 9m 23s) Loss: 0.0303 \n",
      "Epoch: [3][1300/4343] Elapsed 3m 53s (remain 9m 5s) Loss: 0.0304 \n",
      "Epoch: [3][1400/4343] Elapsed 4m 11s (remain 8m 47s) Loss: 0.0307 \n",
      "Epoch: [3][1500/4343] Elapsed 4m 28s (remain 8m 29s) Loss: 0.0311 \n",
      "Epoch: [3][1600/4343] Elapsed 4m 46s (remain 8m 11s) Loss: 0.0300 \n",
      "Epoch: [3][1700/4343] Elapsed 5m 4s (remain 7m 53s) Loss: 0.0313 \n",
      "Epoch: [3][1800/4343] Elapsed 5m 22s (remain 7m 35s) Loss: 0.0313 \n",
      "Epoch: [3][1900/4343] Elapsed 5m 40s (remain 7m 17s) Loss: 0.0304 \n",
      "Epoch: [3][2000/4343] Elapsed 5m 58s (remain 6m 59s) Loss: 0.0294 \n",
      "Epoch: [3][2100/4343] Elapsed 6m 16s (remain 6m 41s) Loss: 0.0300 \n",
      "Epoch: [3][2200/4343] Elapsed 6m 34s (remain 6m 23s) Loss: 0.0304 \n",
      "Epoch: [3][2300/4343] Elapsed 6m 51s (remain 6m 5s) Loss: 0.0302 \n",
      "Epoch: [3][2400/4343] Elapsed 7m 9s (remain 5m 47s) Loss: 0.0308 \n",
      "Epoch: [3][2500/4343] Elapsed 7m 27s (remain 5m 29s) Loss: 0.0306 \n",
      "Epoch: [3][2600/4343] Elapsed 7m 45s (remain 5m 11s) Loss: 0.0301 \n",
      "Epoch: [3][2700/4343] Elapsed 8m 3s (remain 4m 53s) Loss: 0.0298 \n",
      "Epoch: [3][2800/4343] Elapsed 8m 21s (remain 4m 36s) Loss: 0.0302 \n",
      "Epoch: [3][2900/4343] Elapsed 8m 39s (remain 4m 18s) Loss: 0.0298 \n",
      "Epoch: [3][3000/4343] Elapsed 8m 57s (remain 4m 0s) Loss: 0.0296 \n",
      "Epoch: [3][3100/4343] Elapsed 9m 15s (remain 3m 42s) Loss: 0.0294 \n",
      "Epoch: [3][3200/4343] Elapsed 9m 32s (remain 3m 24s) Loss: 0.0292 \n",
      "Epoch: [3][3300/4343] Elapsed 9m 50s (remain 3m 6s) Loss: 0.0292 \n",
      "Epoch: [3][3400/4343] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0289 \n",
      "Epoch: [3][3500/4343] Elapsed 10m 26s (remain 2m 30s) Loss: 0.0291 \n",
      "Epoch: [3][3600/4343] Elapsed 10m 44s (remain 2m 12s) Loss: 0.0285 \n",
      "Epoch: [3][3700/4343] Elapsed 11m 2s (remain 1m 54s) Loss: 0.0285 \n",
      "Epoch: [3][3800/4343] Elapsed 11m 20s (remain 1m 36s) Loss: 0.0285 \n",
      "Epoch: [3][3900/4343] Elapsed 11m 38s (remain 1m 19s) Loss: 0.0285 \n",
      "Epoch: [3][4000/4343] Elapsed 11m 55s (remain 1m 1s) Loss: 0.0283 \n",
      "Epoch: [3][4100/4343] Elapsed 12m 14s (remain 0m 43s) Loss: 0.0284 \n",
      "Epoch: [3][4200/4343] Elapsed 12m 31s (remain 0m 25s) Loss: 0.0280 \n",
      "Epoch: [3][4300/4343] Elapsed 12m 49s (remain 0m 7s) Loss: 0.0281 \n",
      "Epoch: [3][4342/4343] Elapsed 12m 57s (remain 0m 0s) Loss: 0.0281 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 19s) Loss: 0.0003 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0660 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0486 \n",
      "EVAL: [300/1086] Elapsed 0m 16s (remain 0m 41s) Loss: 0.0521 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0744 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0622 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0576 \n",
      "EVAL: [700/1086] Elapsed 0m 37s (remain 0m 20s) Loss: 0.0548 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 15s) Loss: 0.0544 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0550 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0600 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.7413249211356467, thresh=0.023282372444280715\n",
      "score2 = 0.7428481112691638,  thresh=0.041949503098213776\n",
      "Epoch 3 - avg_train_loss: 0.0281  avg_val_loss: 0.0599  time: 835s\n",
      "Epoch 3 - Score: 0.7428481112691638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0599 \n",
      "Epoch: [4][0/4343] Elapsed 0m 0s (remain 25m 54s) Loss: 0.0003 \n",
      "Epoch: [4][100/4343] Elapsed 0m 18s (remain 12m 54s) Loss: 0.0160 \n",
      "Epoch: [4][200/4343] Elapsed 0m 36s (remain 12m 29s) Loss: 0.0275 \n",
      "Epoch: [4][300/4343] Elapsed 0m 54s (remain 12m 9s) Loss: 0.0213 \n",
      "Epoch: [4][400/4343] Elapsed 1m 12s (remain 11m 54s) Loss: 0.0183 \n",
      "Epoch: [4][500/4343] Elapsed 1m 30s (remain 11m 35s) Loss: 0.0198 \n",
      "Epoch: [4][600/4343] Elapsed 1m 48s (remain 11m 18s) Loss: 0.0235 \n",
      "Epoch: [4][700/4343] Elapsed 2m 7s (remain 11m 0s) Loss: 0.0312 \n",
      "Epoch: [4][800/4343] Elapsed 2m 25s (remain 10m 43s) Loss: 0.0302 \n",
      "Epoch: [4][900/4343] Elapsed 2m 43s (remain 10m 25s) Loss: 0.0299 \n",
      "Epoch: [4][1000/4343] Elapsed 3m 2s (remain 10m 8s) Loss: 0.0298 \n",
      "Epoch: [4][1100/4343] Elapsed 3m 20s (remain 9m 51s) Loss: 0.0291 \n",
      "Epoch: [4][1200/4343] Elapsed 3m 39s (remain 9m 33s) Loss: 0.0281 \n",
      "Epoch: [4][1300/4343] Elapsed 3m 57s (remain 9m 14s) Loss: 0.0269 \n",
      "Epoch: [4][1400/4343] Elapsed 4m 15s (remain 8m 55s) Loss: 0.0264 \n",
      "Epoch: [4][1500/4343] Elapsed 4m 33s (remain 8m 37s) Loss: 0.0266 \n",
      "Epoch: [4][1600/4343] Elapsed 4m 50s (remain 8m 18s) Loss: 0.0259 \n",
      "Epoch: [4][1700/4343] Elapsed 5m 8s (remain 7m 59s) Loss: 0.0254 \n",
      "Epoch: [4][1800/4343] Elapsed 5m 26s (remain 7m 41s) Loss: 0.0261 \n",
      "Epoch: [4][1900/4343] Elapsed 5m 44s (remain 7m 22s) Loss: 0.0258 \n",
      "Epoch: [4][2000/4343] Elapsed 6m 2s (remain 7m 4s) Loss: 0.0256 \n",
      "Epoch: [4][2100/4343] Elapsed 6m 20s (remain 6m 45s) Loss: 0.0255 \n",
      "Epoch: [4][2200/4343] Elapsed 6m 38s (remain 6m 27s) Loss: 0.0249 \n",
      "Epoch: [4][2300/4343] Elapsed 6m 56s (remain 6m 9s) Loss: 0.0249 \n",
      "Epoch: [4][2400/4343] Elapsed 7m 14s (remain 5m 51s) Loss: 0.0254 \n",
      "Epoch: [4][2500/4343] Elapsed 7m 31s (remain 5m 32s) Loss: 0.0249 \n",
      "Epoch: [4][2600/4343] Elapsed 7m 49s (remain 5m 14s) Loss: 0.0248 \n",
      "Epoch: [4][2700/4343] Elapsed 8m 7s (remain 4m 56s) Loss: 0.0249 \n",
      "Epoch: [4][2800/4343] Elapsed 8m 25s (remain 4m 38s) Loss: 0.0243 \n",
      "Epoch: [4][2900/4343] Elapsed 8m 43s (remain 4m 20s) Loss: 0.0244 \n",
      "Epoch: [4][3000/4343] Elapsed 9m 1s (remain 4m 2s) Loss: 0.0258 \n",
      "Epoch: [4][3100/4343] Elapsed 9m 19s (remain 3m 44s) Loss: 0.0261 \n",
      "Epoch: [4][3200/4343] Elapsed 9m 37s (remain 3m 26s) Loss: 0.0257 \n",
      "Epoch: [4][3300/4343] Elapsed 9m 55s (remain 3m 8s) Loss: 0.0253 \n",
      "Epoch: [4][3400/4343] Elapsed 10m 14s (remain 2m 50s) Loss: 0.0254 \n",
      "Epoch: [4][3500/4343] Elapsed 10m 32s (remain 2m 32s) Loss: 0.0252 \n",
      "Epoch: [4][3600/4343] Elapsed 10m 51s (remain 2m 14s) Loss: 0.0251 \n",
      "Epoch: [4][3700/4343] Elapsed 11m 9s (remain 1m 56s) Loss: 0.0247 \n",
      "Epoch: [4][3800/4343] Elapsed 11m 27s (remain 1m 38s) Loss: 0.0249 \n",
      "Epoch: [4][3900/4343] Elapsed 11m 45s (remain 1m 19s) Loss: 0.0251 \n",
      "Epoch: [4][4000/4343] Elapsed 12m 3s (remain 1m 1s) Loss: 0.0246 \n",
      "Epoch: [4][4100/4343] Elapsed 12m 22s (remain 0m 43s) Loss: 0.0244 \n",
      "Epoch: [4][4200/4343] Elapsed 12m 40s (remain 0m 25s) Loss: 0.0244 \n",
      "Epoch: [4][4300/4343] Elapsed 12m 58s (remain 0m 7s) Loss: 0.0242 \n",
      "Epoch: [4][4342/4343] Elapsed 13m 6s (remain 0m 0s) Loss: 0.0245 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 17s) Loss: 0.0010 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0657 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0600 \n",
      "EVAL: [300/1086] Elapsed 0m 16s (remain 0m 42s) Loss: 0.0625 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0716 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 31s) Loss: 0.0638 \n",
      "EVAL: [600/1086] Elapsed 0m 32s (remain 0m 25s) Loss: 0.0583 \n",
      "EVAL: [700/1086] Elapsed 0m 37s (remain 0m 20s) Loss: 0.0560 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 15s) Loss: 0.0533 \n",
      "EVAL: [900/1086] Elapsed 0m 48s (remain 0m 9s) Loss: 0.0538 \n",
      "EVAL: [1000/1086] Elapsed 0m 53s (remain 0m 4s) Loss: 0.0552 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8386894497570151, thresh=0.023282372444280715\n",
      "score2 = 0.8322864321608041,  thresh=0.04310924913099665\n",
      "Epoch 4 - avg_train_loss: 0.0245  avg_val_loss: 0.0542  time: 845s\n",
      "Epoch 4 - Score: 0.8322864321608041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 58s (remain 0m 0s) Loss: 0.0542 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 result ==========\n",
      "Score1: 0.90441\n",
      "best border: 0.00276\n",
      "Score2: 0.93077\n",
      "=*========= fold: 2 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/4343] Elapsed 0m 0s (remain 28m 0s) Loss: 0.9393 \n",
      "Epoch: [1][100/4343] Elapsed 0m 18s (remain 12m 50s) Loss: 0.2086 \n",
      "Epoch: [1][200/4343] Elapsed 0m 36s (remain 12m 33s) Loss: 0.1675 \n",
      "Epoch: [1][300/4343] Elapsed 0m 54s (remain 12m 11s) Loss: 0.1346 \n",
      "Epoch: [1][400/4343] Elapsed 1m 12s (remain 11m 51s) Loss: 0.1171 \n",
      "Epoch: [1][500/4343] Elapsed 1m 30s (remain 11m 33s) Loss: 0.1091 \n",
      "Epoch: [1][600/4343] Elapsed 1m 48s (remain 11m 14s) Loss: 0.1014 \n",
      "Epoch: [1][700/4343] Elapsed 2m 6s (remain 10m 55s) Loss: 0.1008 \n",
      "Epoch: [1][800/4343] Elapsed 2m 24s (remain 10m 37s) Loss: 0.0971 \n",
      "Epoch: [1][900/4343] Elapsed 2m 41s (remain 10m 18s) Loss: 0.1038 \n",
      "Epoch: [1][1000/4343] Elapsed 2m 59s (remain 10m 0s) Loss: 0.1053 \n",
      "Epoch: [1][1100/4343] Elapsed 3m 17s (remain 9m 42s) Loss: 0.1066 \n",
      "Epoch: [1][1200/4343] Elapsed 3m 35s (remain 9m 24s) Loss: 0.1058 \n",
      "Epoch: [1][1300/4343] Elapsed 3m 53s (remain 9m 6s) Loss: 0.1035 \n",
      "Epoch: [1][1400/4343] Elapsed 4m 11s (remain 8m 47s) Loss: 0.1030 \n",
      "Epoch: [1][1500/4343] Elapsed 4m 29s (remain 8m 29s) Loss: 0.1013 \n",
      "Epoch: [1][1600/4343] Elapsed 4m 47s (remain 8m 11s) Loss: 0.0992 \n",
      "Epoch: [1][1700/4343] Elapsed 5m 5s (remain 7m 53s) Loss: 0.0976 \n",
      "Epoch: [1][1800/4343] Elapsed 5m 22s (remain 7m 35s) Loss: 0.0977 \n",
      "Epoch: [1][1900/4343] Elapsed 5m 40s (remain 7m 17s) Loss: 0.0972 \n",
      "Epoch: [1][2000/4343] Elapsed 5m 58s (remain 6m 59s) Loss: 0.0961 \n",
      "Epoch: [1][2100/4343] Elapsed 6m 16s (remain 6m 41s) Loss: 0.0950 \n",
      "Epoch: [1][2200/4343] Elapsed 6m 34s (remain 6m 23s) Loss: 0.0928 \n",
      "Epoch: [1][2300/4343] Elapsed 6m 52s (remain 6m 5s) Loss: 0.0929 \n",
      "Epoch: [1][2400/4343] Elapsed 7m 10s (remain 5m 47s) Loss: 0.0933 \n",
      "Epoch: [1][2500/4343] Elapsed 7m 28s (remain 5m 29s) Loss: 0.0917 \n",
      "Epoch: [1][2600/4343] Elapsed 7m 45s (remain 5m 12s) Loss: 0.0908 \n",
      "Epoch: [1][2700/4343] Elapsed 8m 3s (remain 4m 54s) Loss: 0.0896 \n",
      "Epoch: [1][2800/4343] Elapsed 8m 21s (remain 4m 36s) Loss: 0.0885 \n",
      "Epoch: [1][2900/4343] Elapsed 8m 39s (remain 4m 18s) Loss: 0.0880 \n",
      "Epoch: [1][3000/4343] Elapsed 8m 57s (remain 4m 0s) Loss: 0.0868 \n",
      "Epoch: [1][3100/4343] Elapsed 9m 15s (remain 3m 42s) Loss: 0.0866 \n",
      "Epoch: [1][3200/4343] Elapsed 9m 33s (remain 3m 24s) Loss: 0.0854 \n",
      "Epoch: [1][3300/4343] Elapsed 9m 51s (remain 3m 6s) Loss: 0.0854 \n",
      "Epoch: [1][3400/4343] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0844 \n",
      "Epoch: [1][3500/4343] Elapsed 10m 26s (remain 2m 30s) Loss: 0.0838 \n",
      "Epoch: [1][3600/4343] Elapsed 10m 44s (remain 2m 12s) Loss: 0.0830 \n",
      "Epoch: [1][3700/4343] Elapsed 11m 2s (remain 1m 54s) Loss: 0.0819 \n",
      "Epoch: [1][3800/4343] Elapsed 11m 20s (remain 1m 37s) Loss: 0.0818 \n",
      "Epoch: [1][3900/4343] Elapsed 11m 38s (remain 1m 19s) Loss: 0.0813 \n",
      "Epoch: [1][4000/4343] Elapsed 11m 56s (remain 1m 1s) Loss: 0.0798 \n",
      "Epoch: [1][4100/4343] Elapsed 12m 14s (remain 0m 43s) Loss: 0.0796 \n",
      "Epoch: [1][4200/4343] Elapsed 12m 31s (remain 0m 25s) Loss: 0.0790 \n",
      "Epoch: [1][4300/4343] Elapsed 12m 49s (remain 0m 7s) Loss: 0.0782 \n",
      "Epoch: [1][4342/4343] Elapsed 12m 57s (remain 0m 0s) Loss: 0.0780 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 18s) Loss: 0.0017 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0586 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0428 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.0374 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0425 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0443 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0431 \n",
      "EVAL: [700/1086] Elapsed 0m 36s (remain 0m 20s) Loss: 0.0474 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 14s) Loss: 0.0479 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0502 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8612734543217471, thresh=0.023282372444280715\n",
      "score2 = 0.8637976245565324,  thresh=0.02664849497821347\n",
      "Epoch 1 - avg_train_loss: 0.0780  avg_val_loss: 0.0498  time: 835s\n",
      "Epoch 1 - Score: 0.8637976245565324\n",
      "Epoch 1 - Save Best Score: 0.8638 - Best Loss: 0.0498 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0498 \n",
      "Epoch: [2][0/4343] Elapsed 0m 0s (remain 26m 16s) Loss: 0.0018 \n",
      "Epoch: [2][100/4343] Elapsed 0m 18s (remain 12m 46s) Loss: 0.0491 \n",
      "Epoch: [2][200/4343] Elapsed 0m 36s (remain 12m 24s) Loss: 0.0398 \n",
      "Epoch: [2][300/4343] Elapsed 0m 53s (remain 12m 5s) Loss: 0.0408 \n",
      "Epoch: [2][400/4343] Elapsed 1m 11s (remain 11m 46s) Loss: 0.0487 \n",
      "Epoch: [2][500/4343] Elapsed 1m 29s (remain 11m 28s) Loss: 0.0458 \n",
      "Epoch: [2][600/4343] Elapsed 1m 47s (remain 11m 10s) Loss: 0.0446 \n",
      "Epoch: [2][700/4343] Elapsed 2m 5s (remain 10m 52s) Loss: 0.0429 \n",
      "Epoch: [2][800/4343] Elapsed 2m 23s (remain 10m 34s) Loss: 0.0445 \n",
      "Epoch: [2][900/4343] Elapsed 2m 41s (remain 10m 16s) Loss: 0.0460 \n",
      "Epoch: [2][1000/4343] Elapsed 2m 59s (remain 9m 58s) Loss: 0.0452 \n",
      "Epoch: [2][1100/4343] Elapsed 3m 16s (remain 9m 40s) Loss: 0.0444 \n",
      "Epoch: [2][1200/4343] Elapsed 3m 34s (remain 9m 22s) Loss: 0.0444 \n",
      "Epoch: [2][1300/4343] Elapsed 3m 52s (remain 9m 4s) Loss: 0.0434 \n",
      "Epoch: [2][1400/4343] Elapsed 4m 10s (remain 8m 46s) Loss: 0.0419 \n",
      "Epoch: [2][1500/4343] Elapsed 4m 28s (remain 8m 28s) Loss: 0.0417 \n",
      "Epoch: [2][1600/4343] Elapsed 4m 46s (remain 8m 10s) Loss: 0.0425 \n",
      "Epoch: [2][1700/4343] Elapsed 5m 4s (remain 7m 52s) Loss: 0.0426 \n",
      "Epoch: [2][1800/4343] Elapsed 5m 22s (remain 7m 34s) Loss: 0.0430 \n",
      "Epoch: [2][1900/4343] Elapsed 5m 39s (remain 7m 16s) Loss: 0.0423 \n",
      "Epoch: [2][2000/4343] Elapsed 5m 57s (remain 6m 58s) Loss: 0.0433 \n",
      "Epoch: [2][2100/4343] Elapsed 6m 15s (remain 6m 40s) Loss: 0.0435 \n",
      "Epoch: [2][2200/4343] Elapsed 6m 33s (remain 6m 23s) Loss: 0.0433 \n",
      "Epoch: [2][2300/4343] Elapsed 6m 51s (remain 6m 5s) Loss: 0.0421 \n",
      "Epoch: [2][2400/4343] Elapsed 7m 9s (remain 5m 47s) Loss: 0.0426 \n",
      "Epoch: [2][2500/4343] Elapsed 7m 27s (remain 5m 29s) Loss: 0.0429 \n",
      "Epoch: [2][2600/4343] Elapsed 7m 45s (remain 5m 11s) Loss: 0.0427 \n",
      "Epoch: [2][2700/4343] Elapsed 8m 2s (remain 4m 53s) Loss: 0.0427 \n",
      "Epoch: [2][2800/4343] Elapsed 8m 20s (remain 4m 35s) Loss: 0.0426 \n",
      "Epoch: [2][2900/4343] Elapsed 8m 38s (remain 4m 17s) Loss: 0.0434 \n",
      "Epoch: [2][3000/4343] Elapsed 8m 56s (remain 3m 59s) Loss: 0.0441 \n",
      "Epoch: [2][3100/4343] Elapsed 9m 14s (remain 3m 42s) Loss: 0.0443 \n",
      "Epoch: [2][3200/4343] Elapsed 9m 32s (remain 3m 24s) Loss: 0.0439 \n",
      "Epoch: [2][3300/4343] Elapsed 9m 50s (remain 3m 6s) Loss: 0.0438 \n",
      "Epoch: [2][3400/4343] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0433 \n",
      "Epoch: [2][3500/4343] Elapsed 10m 25s (remain 2m 30s) Loss: 0.0437 \n",
      "Epoch: [2][3600/4343] Elapsed 10m 43s (remain 2m 12s) Loss: 0.0443 \n",
      "Epoch: [2][3700/4343] Elapsed 11m 1s (remain 1m 54s) Loss: 0.0440 \n",
      "Epoch: [2][3800/4343] Elapsed 11m 19s (remain 1m 36s) Loss: 0.0446 \n",
      "Epoch: [2][3900/4343] Elapsed 11m 37s (remain 1m 19s) Loss: 0.0448 \n",
      "Epoch: [2][4000/4343] Elapsed 11m 55s (remain 1m 1s) Loss: 0.0448 \n",
      "Epoch: [2][4100/4343] Elapsed 12m 13s (remain 0m 43s) Loss: 0.0447 \n",
      "Epoch: [2][4200/4343] Elapsed 12m 31s (remain 0m 25s) Loss: 0.0462 \n",
      "Epoch: [2][4300/4343] Elapsed 12m 48s (remain 0m 7s) Loss: 0.0466 \n",
      "Epoch: [2][4342/4343] Elapsed 12m 56s (remain 0m 0s) Loss: 0.0463 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 21s) Loss: 0.0039 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0506 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0428 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.0355 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0400 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0399 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0399 \n",
      "EVAL: [700/1086] Elapsed 0m 36s (remain 0m 20s) Loss: 0.0429 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 14s) Loss: 0.0444 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0458 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0475 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8575401730531521, thresh=0.023282372444280715\n",
      "score2 = 0.8596654275092938,  thresh=0.026422397587156962\n",
      "Epoch 2 - avg_train_loss: 0.0463  avg_val_loss: 0.0465  time: 834s\n",
      "Epoch 2 - Score: 0.8596654275092938\n",
      "Epoch 2 - Save Best Score: 0.8597 - Best Loss: 0.0465 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0465 \n",
      "Epoch: [3][0/4343] Elapsed 0m 0s (remain 28m 12s) Loss: 0.0016 \n",
      "Epoch: [3][100/4343] Elapsed 0m 18s (remain 12m 48s) Loss: 0.0244 \n",
      "Epoch: [3][200/4343] Elapsed 0m 36s (remain 12m 25s) Loss: 0.0283 \n",
      "Epoch: [3][300/4343] Elapsed 0m 54s (remain 12m 6s) Loss: 0.0311 \n",
      "Epoch: [3][400/4343] Elapsed 1m 11s (remain 11m 47s) Loss: 0.0289 \n",
      "Epoch: [3][500/4343] Elapsed 1m 29s (remain 11m 28s) Loss: 0.0305 \n",
      "Epoch: [3][600/4343] Elapsed 1m 47s (remain 11m 10s) Loss: 0.0340 \n",
      "Epoch: [3][700/4343] Elapsed 2m 5s (remain 10m 52s) Loss: 0.0314 \n",
      "Epoch: [3][800/4343] Elapsed 2m 23s (remain 10m 34s) Loss: 0.0327 \n",
      "Epoch: [3][900/4343] Elapsed 2m 41s (remain 10m 16s) Loss: 0.0333 \n",
      "Epoch: [3][1000/4343] Elapsed 2m 59s (remain 9m 58s) Loss: 0.0331 \n",
      "Epoch: [3][1100/4343] Elapsed 3m 17s (remain 9m 40s) Loss: 0.0357 \n",
      "Epoch: [3][1200/4343] Elapsed 3m 35s (remain 9m 22s) Loss: 0.0349 \n",
      "Epoch: [3][1300/4343] Elapsed 3m 52s (remain 9m 4s) Loss: 0.0343 \n",
      "Epoch: [3][1400/4343] Elapsed 4m 10s (remain 8m 46s) Loss: 0.0351 \n",
      "Epoch: [3][1500/4343] Elapsed 4m 28s (remain 8m 28s) Loss: 0.0349 \n",
      "Epoch: [3][1600/4343] Elapsed 4m 46s (remain 8m 10s) Loss: 0.0344 \n",
      "Epoch: [3][1700/4343] Elapsed 5m 4s (remain 7m 52s) Loss: 0.0336 \n",
      "Epoch: [3][1800/4343] Elapsed 5m 22s (remain 7m 34s) Loss: 0.0325 \n",
      "Epoch: [3][1900/4343] Elapsed 5m 40s (remain 7m 17s) Loss: 0.0323 \n",
      "Epoch: [3][2000/4343] Elapsed 5m 58s (remain 6m 59s) Loss: 0.0330 \n",
      "Epoch: [3][2100/4343] Elapsed 6m 16s (remain 6m 41s) Loss: 0.0328 \n",
      "Epoch: [3][2200/4343] Elapsed 6m 33s (remain 6m 23s) Loss: 0.0331 \n",
      "Epoch: [3][2300/4343] Elapsed 6m 51s (remain 6m 5s) Loss: 0.0336 \n",
      "Epoch: [3][2400/4343] Elapsed 7m 9s (remain 5m 47s) Loss: 0.0333 \n",
      "Epoch: [3][2500/4343] Elapsed 7m 27s (remain 5m 29s) Loss: 0.0336 \n",
      "Epoch: [3][2600/4343] Elapsed 7m 45s (remain 5m 11s) Loss: 0.0336 \n",
      "Epoch: [3][2700/4343] Elapsed 8m 3s (remain 4m 53s) Loss: 0.0330 \n",
      "Epoch: [3][2800/4343] Elapsed 8m 21s (remain 4m 35s) Loss: 0.0332 \n",
      "Epoch: [3][2900/4343] Elapsed 8m 39s (remain 4m 18s) Loss: 0.0327 \n",
      "Epoch: [3][3000/4343] Elapsed 8m 56s (remain 4m 0s) Loss: 0.0320 \n",
      "Epoch: [3][3100/4343] Elapsed 9m 14s (remain 3m 42s) Loss: 0.0320 \n",
      "Epoch: [3][3200/4343] Elapsed 9m 32s (remain 3m 24s) Loss: 0.0325 \n",
      "Epoch: [3][3300/4343] Elapsed 9m 50s (remain 3m 6s) Loss: 0.0326 \n",
      "Epoch: [3][3400/4343] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0325 \n",
      "Epoch: [3][3500/4343] Elapsed 10m 26s (remain 2m 30s) Loss: 0.0323 \n",
      "Epoch: [3][3600/4343] Elapsed 10m 44s (remain 2m 12s) Loss: 0.0316 \n",
      "Epoch: [3][3700/4343] Elapsed 11m 2s (remain 1m 54s) Loss: 0.0317 \n",
      "Epoch: [3][3800/4343] Elapsed 11m 20s (remain 1m 36s) Loss: 0.0313 \n",
      "Epoch: [3][3900/4343] Elapsed 11m 37s (remain 1m 19s) Loss: 0.0317 \n",
      "Epoch: [3][4000/4343] Elapsed 11m 55s (remain 1m 1s) Loss: 0.0314 \n",
      "Epoch: [3][4100/4343] Elapsed 12m 13s (remain 0m 43s) Loss: 0.0321 \n",
      "Epoch: [3][4200/4343] Elapsed 12m 31s (remain 0m 25s) Loss: 0.0315 \n",
      "Epoch: [3][4300/4343] Elapsed 12m 49s (remain 0m 7s) Loss: 0.0312 \n",
      "Epoch: [3][4342/4343] Elapsed 12m 57s (remain 0m 0s) Loss: 0.0312 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 22s) Loss: 0.0004 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0583 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0425 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.0357 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0378 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0373 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0376 \n",
      "EVAL: [700/1086] Elapsed 0m 36s (remain 0m 20s) Loss: 0.0400 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 14s) Loss: 0.0409 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0428 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0427 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.9036144578313254, thresh=0.023282372444280715\n",
      "score2 = 0.8997828110456096,  thresh=0.07662752594038297\n",
      "Epoch 3 - avg_train_loss: 0.0312  avg_val_loss: 0.0417  time: 834s\n",
      "Epoch 3 - Score: 0.8997828110456096\n",
      "Epoch 3 - Save Best Score: 0.8998 - Best Loss: 0.0417 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0417 \n",
      "Epoch: [4][0/4343] Elapsed 0m 0s (remain 27m 38s) Loss: 0.0641 \n",
      "Epoch: [4][100/4343] Elapsed 0m 18s (remain 12m 48s) Loss: 0.0127 \n",
      "Epoch: [4][200/4343] Elapsed 0m 36s (remain 12m 25s) Loss: 0.0124 \n",
      "Epoch: [4][300/4343] Elapsed 0m 54s (remain 12m 5s) Loss: 0.0117 \n",
      "Epoch: [4][400/4343] Elapsed 1m 11s (remain 11m 47s) Loss: 0.0137 \n",
      "Epoch: [4][500/4343] Elapsed 1m 29s (remain 11m 28s) Loss: 0.0144 \n",
      "Epoch: [4][600/4343] Elapsed 1m 47s (remain 11m 10s) Loss: 0.0195 \n",
      "Epoch: [4][700/4343] Elapsed 2m 5s (remain 10m 52s) Loss: 0.0211 \n",
      "Epoch: [4][800/4343] Elapsed 2m 23s (remain 10m 34s) Loss: 0.0198 \n",
      "Epoch: [4][900/4343] Elapsed 2m 41s (remain 10m 16s) Loss: 0.0192 \n",
      "Epoch: [4][1000/4343] Elapsed 2m 59s (remain 9m 58s) Loss: 0.0192 \n",
      "Epoch: [4][1100/4343] Elapsed 3m 17s (remain 9m 40s) Loss: 0.0202 \n",
      "Epoch: [4][1200/4343] Elapsed 3m 34s (remain 9m 22s) Loss: 0.0201 \n",
      "Epoch: [4][1300/4343] Elapsed 3m 52s (remain 9m 4s) Loss: 0.0214 \n",
      "Epoch: [4][1400/4343] Elapsed 4m 10s (remain 8m 46s) Loss: 0.0211 \n",
      "Epoch: [4][1500/4343] Elapsed 4m 28s (remain 8m 28s) Loss: 0.0221 \n",
      "Epoch: [4][1600/4343] Elapsed 4m 46s (remain 8m 10s) Loss: 0.0240 \n",
      "Epoch: [4][1700/4343] Elapsed 5m 4s (remain 7m 52s) Loss: 0.0255 \n",
      "Epoch: [4][1800/4343] Elapsed 5m 22s (remain 7m 34s) Loss: 0.0253 \n",
      "Epoch: [4][1900/4343] Elapsed 5m 40s (remain 7m 17s) Loss: 0.0247 \n",
      "Epoch: [4][2000/4343] Elapsed 5m 58s (remain 6m 59s) Loss: 0.0254 \n",
      "Epoch: [4][2100/4343] Elapsed 6m 15s (remain 6m 41s) Loss: 0.0253 \n",
      "Epoch: [4][2200/4343] Elapsed 6m 33s (remain 6m 23s) Loss: 0.0258 \n",
      "Epoch: [4][2300/4343] Elapsed 6m 51s (remain 6m 5s) Loss: 0.0253 \n",
      "Epoch: [4][2400/4343] Elapsed 7m 9s (remain 5m 47s) Loss: 0.0248 \n",
      "Epoch: [4][2500/4343] Elapsed 7m 27s (remain 5m 29s) Loss: 0.0250 \n",
      "Epoch: [4][2600/4343] Elapsed 7m 45s (remain 5m 11s) Loss: 0.0253 \n",
      "Epoch: [4][2700/4343] Elapsed 8m 3s (remain 4m 53s) Loss: 0.0252 \n",
      "Epoch: [4][2800/4343] Elapsed 8m 21s (remain 4m 35s) Loss: 0.0260 \n",
      "Epoch: [4][2900/4343] Elapsed 8m 39s (remain 4m 18s) Loss: 0.0264 \n",
      "Epoch: [4][3000/4343] Elapsed 8m 56s (remain 4m 0s) Loss: 0.0289 \n",
      "Epoch: [4][3100/4343] Elapsed 9m 14s (remain 3m 42s) Loss: 0.0315 \n",
      "Epoch: [4][3200/4343] Elapsed 9m 32s (remain 3m 24s) Loss: 0.0332 \n",
      "Epoch: [4][3300/4343] Elapsed 9m 50s (remain 3m 6s) Loss: 0.0365 \n",
      "Epoch: [4][3400/4343] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0384 \n",
      "Epoch: [4][3500/4343] Elapsed 10m 26s (remain 2m 30s) Loss: 0.0409 \n",
      "Epoch: [4][3600/4343] Elapsed 10m 44s (remain 2m 12s) Loss: 0.0407 \n",
      "Epoch: [4][3700/4343] Elapsed 11m 2s (remain 1m 54s) Loss: 0.0407 \n",
      "Epoch: [4][3800/4343] Elapsed 11m 20s (remain 1m 36s) Loss: 0.0402 \n",
      "Epoch: [4][3900/4343] Elapsed 11m 37s (remain 1m 19s) Loss: 0.0406 \n",
      "Epoch: [4][4000/4343] Elapsed 11m 55s (remain 1m 1s) Loss: 0.0417 \n",
      "Epoch: [4][4100/4343] Elapsed 12m 13s (remain 0m 43s) Loss: 0.0417 \n",
      "Epoch: [4][4200/4343] Elapsed 12m 31s (remain 0m 25s) Loss: 0.0417 \n",
      "Epoch: [4][4300/4343] Elapsed 12m 49s (remain 0m 7s) Loss: 0.0426 \n",
      "Epoch: [4][4342/4343] Elapsed 12m 56s (remain 0m 0s) Loss: 0.0427 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 22s) Loss: 0.0049 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0605 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0486 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.0436 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0459 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0441 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0449 \n",
      "EVAL: [700/1086] Elapsed 0m 36s (remain 0m 20s) Loss: 0.0488 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 14s) Loss: 0.0505 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0512 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8325552443199503, thresh=0.023282372444280715\n",
      "score2 = 0.8635312259059368,  thresh=0.007945609010158974\n",
      "Epoch 4 - avg_train_loss: 0.0427  avg_val_loss: 0.0489  time: 834s\n",
      "Epoch 4 - Score: 0.8635312259059368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0489 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 result ==========\n",
      "Score1: 0.90361\n",
      "best border: 0.07663\n",
      "Score2: 0.89978\n",
      "=*========= fold: 3 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/4343] Elapsed 0m 0s (remain 26m 46s) Loss: 0.8073 \n",
      "Epoch: [1][100/4343] Elapsed 0m 18s (remain 12m 42s) Loss: 0.1365 \n",
      "Epoch: [1][200/4343] Elapsed 0m 35s (remain 12m 21s) Loss: 0.1243 \n",
      "Epoch: [1][300/4343] Elapsed 0m 53s (remain 12m 2s) Loss: 0.1347 \n",
      "Epoch: [1][400/4343] Elapsed 1m 11s (remain 11m 44s) Loss: 0.1213 \n",
      "Epoch: [1][500/4343] Elapsed 1m 29s (remain 11m 27s) Loss: 0.1168 \n",
      "Epoch: [1][600/4343] Elapsed 1m 47s (remain 11m 9s) Loss: 0.1108 \n",
      "Epoch: [1][700/4343] Elapsed 2m 5s (remain 10m 51s) Loss: 0.1124 \n",
      "Epoch: [1][800/4343] Elapsed 2m 23s (remain 10m 33s) Loss: 0.1061 \n",
      "Epoch: [1][900/4343] Elapsed 2m 41s (remain 10m 15s) Loss: 0.1017 \n",
      "Epoch: [1][1000/4343] Elapsed 2m 59s (remain 9m 57s) Loss: 0.0983 \n",
      "Epoch: [1][1100/4343] Elapsed 3m 16s (remain 9m 39s) Loss: 0.0948 \n",
      "Epoch: [1][1200/4343] Elapsed 3m 34s (remain 9m 21s) Loss: 0.0911 \n",
      "Epoch: [1][1300/4343] Elapsed 3m 52s (remain 9m 4s) Loss: 0.0895 \n",
      "Epoch: [1][1400/4343] Elapsed 4m 10s (remain 8m 46s) Loss: 0.0875 \n",
      "Epoch: [1][1500/4343] Elapsed 4m 28s (remain 8m 28s) Loss: 0.0868 \n",
      "Epoch: [1][1600/4343] Elapsed 4m 46s (remain 8m 10s) Loss: 0.0869 \n",
      "Epoch: [1][1700/4343] Elapsed 5m 4s (remain 7m 52s) Loss: 0.0867 \n",
      "Epoch: [1][1800/4343] Elapsed 5m 22s (remain 7m 34s) Loss: 0.0852 \n",
      "Epoch: [1][1900/4343] Elapsed 5m 40s (remain 7m 16s) Loss: 0.0836 \n",
      "Epoch: [1][2000/4343] Elapsed 5m 57s (remain 6m 58s) Loss: 0.0814 \n",
      "Epoch: [1][2100/4343] Elapsed 6m 15s (remain 6m 41s) Loss: 0.0798 \n",
      "Epoch: [1][2200/4343] Elapsed 6m 33s (remain 6m 23s) Loss: 0.0790 \n",
      "Epoch: [1][2300/4343] Elapsed 6m 51s (remain 6m 5s) Loss: 0.0765 \n",
      "Epoch: [1][2400/4343] Elapsed 7m 9s (remain 5m 47s) Loss: 0.0748 \n",
      "Epoch: [1][2500/4343] Elapsed 7m 27s (remain 5m 29s) Loss: 0.0739 \n",
      "Epoch: [1][2600/4343] Elapsed 7m 45s (remain 5m 11s) Loss: 0.0732 \n",
      "Epoch: [1][2700/4343] Elapsed 8m 3s (remain 4m 53s) Loss: 0.0723 \n",
      "Epoch: [1][2800/4343] Elapsed 8m 21s (remain 4m 35s) Loss: 0.0713 \n",
      "Epoch: [1][2900/4343] Elapsed 8m 38s (remain 4m 17s) Loss: 0.0701 \n",
      "Epoch: [1][3000/4343] Elapsed 8m 56s (remain 4m 0s) Loss: 0.0692 \n",
      "Epoch: [1][3100/4343] Elapsed 9m 14s (remain 3m 42s) Loss: 0.0691 \n",
      "Epoch: [1][3200/4343] Elapsed 9m 32s (remain 3m 24s) Loss: 0.0680 \n",
      "Epoch: [1][3300/4343] Elapsed 9m 50s (remain 3m 6s) Loss: 0.0677 \n",
      "Epoch: [1][3400/4343] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0673 \n",
      "Epoch: [1][3500/4343] Elapsed 10m 26s (remain 2m 30s) Loss: 0.0666 \n",
      "Epoch: [1][3600/4343] Elapsed 10m 44s (remain 2m 12s) Loss: 0.0656 \n",
      "Epoch: [1][3700/4343] Elapsed 11m 1s (remain 1m 54s) Loss: 0.0650 \n",
      "Epoch: [1][3800/4343] Elapsed 11m 19s (remain 1m 36s) Loss: 0.0643 \n",
      "Epoch: [1][3900/4343] Elapsed 11m 37s (remain 1m 19s) Loss: 0.0640 \n",
      "Epoch: [1][4000/4343] Elapsed 11m 55s (remain 1m 1s) Loss: 0.0643 \n",
      "Epoch: [1][4100/4343] Elapsed 12m 13s (remain 0m 43s) Loss: 0.0643 \n",
      "Epoch: [1][4200/4343] Elapsed 12m 31s (remain 0m 25s) Loss: 0.0635 \n",
      "Epoch: [1][4300/4343] Elapsed 12m 49s (remain 0m 7s) Loss: 0.0633 \n",
      "Epoch: [1][4342/4343] Elapsed 12m 56s (remain 0m 0s) Loss: 0.0631 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 17s) Loss: 0.0007 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0729 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0687 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.0537 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0532 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0505 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0459 \n",
      "EVAL: [700/1086] Elapsed 0m 36s (remain 0m 20s) Loss: 0.0494 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 14s) Loss: 0.0463 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0481 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0494 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8025929927457942, thresh=0.023282372444280715\n",
      "score2 = 0.8701573849878934,  thresh=0.007476132720844057\n",
      "Epoch 1 - avg_train_loss: 0.0631  avg_val_loss: 0.0487  time: 834s\n",
      "Epoch 1 - Score: 0.8701573849878934\n",
      "Epoch 1 - Save Best Score: 0.8702 - Best Loss: 0.0487 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0487 \n",
      "Epoch: [2][0/4343] Elapsed 0m 0s (remain 25m 58s) Loss: 0.0008 \n",
      "Epoch: [2][100/4343] Elapsed 0m 18s (remain 12m 46s) Loss: 0.0437 \n",
      "Epoch: [2][200/4343] Elapsed 0m 36s (remain 12m 24s) Loss: 0.0367 \n",
      "Epoch: [2][300/4343] Elapsed 0m 53s (remain 12m 4s) Loss: 0.0360 \n",
      "Epoch: [2][400/4343] Elapsed 1m 11s (remain 11m 46s) Loss: 0.0342 \n",
      "Epoch: [2][500/4343] Elapsed 1m 29s (remain 11m 28s) Loss: 0.0288 \n",
      "Epoch: [2][600/4343] Elapsed 1m 47s (remain 11m 9s) Loss: 0.0303 \n",
      "Epoch: [2][700/4343] Elapsed 2m 5s (remain 10m 52s) Loss: 0.0342 \n",
      "Epoch: [2][800/4343] Elapsed 2m 23s (remain 10m 33s) Loss: 0.0377 \n",
      "Epoch: [2][900/4343] Elapsed 2m 41s (remain 10m 15s) Loss: 0.0376 \n",
      "Epoch: [2][1000/4343] Elapsed 2m 59s (remain 9m 57s) Loss: 0.0403 \n",
      "Epoch: [2][1100/4343] Elapsed 3m 16s (remain 9m 40s) Loss: 0.0395 \n",
      "Epoch: [2][1200/4343] Elapsed 3m 34s (remain 9m 22s) Loss: 0.0402 \n",
      "Epoch: [2][1300/4343] Elapsed 3m 52s (remain 9m 4s) Loss: 0.0398 \n",
      "Epoch: [2][1400/4343] Elapsed 4m 10s (remain 8m 46s) Loss: 0.0394 \n",
      "Epoch: [2][1500/4343] Elapsed 4m 28s (remain 8m 28s) Loss: 0.0395 \n",
      "Epoch: [2][1600/4343] Elapsed 4m 46s (remain 8m 10s) Loss: 0.0399 \n",
      "Epoch: [2][1700/4343] Elapsed 5m 4s (remain 7m 52s) Loss: 0.0401 \n",
      "Epoch: [2][1800/4343] Elapsed 5m 22s (remain 7m 34s) Loss: 0.0398 \n",
      "Epoch: [2][1900/4343] Elapsed 5m 39s (remain 7m 16s) Loss: 0.0391 \n",
      "Epoch: [2][2000/4343] Elapsed 5m 57s (remain 6m 58s) Loss: 0.0396 \n",
      "Epoch: [2][2100/4343] Elapsed 6m 15s (remain 6m 40s) Loss: 0.0398 \n",
      "Epoch: [2][2200/4343] Elapsed 6m 33s (remain 6m 23s) Loss: 0.0383 \n",
      "Epoch: [2][2300/4343] Elapsed 6m 51s (remain 6m 5s) Loss: 0.0397 \n",
      "Epoch: [2][2400/4343] Elapsed 7m 9s (remain 5m 47s) Loss: 0.0396 \n",
      "Epoch: [2][2500/4343] Elapsed 7m 27s (remain 5m 29s) Loss: 0.0399 \n",
      "Epoch: [2][2600/4343] Elapsed 7m 45s (remain 5m 11s) Loss: 0.0404 \n",
      "Epoch: [2][2700/4343] Elapsed 8m 3s (remain 4m 53s) Loss: 0.0397 \n",
      "Epoch: [2][2800/4343] Elapsed 8m 20s (remain 4m 35s) Loss: 0.0394 \n",
      "Epoch: [2][2900/4343] Elapsed 8m 38s (remain 4m 17s) Loss: 0.0390 \n",
      "Epoch: [2][3000/4343] Elapsed 8m 56s (remain 3m 59s) Loss: 0.0381 \n",
      "Epoch: [2][3100/4343] Elapsed 9m 14s (remain 3m 42s) Loss: 0.0381 \n",
      "Epoch: [2][3200/4343] Elapsed 9m 32s (remain 3m 24s) Loss: 0.0379 \n",
      "Epoch: [2][3300/4343] Elapsed 9m 50s (remain 3m 6s) Loss: 0.0380 \n",
      "Epoch: [2][3400/4343] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0377 \n",
      "Epoch: [2][3500/4343] Elapsed 10m 26s (remain 2m 30s) Loss: 0.0371 \n",
      "Epoch: [2][3600/4343] Elapsed 10m 43s (remain 2m 12s) Loss: 0.0369 \n",
      "Epoch: [2][3700/4343] Elapsed 11m 1s (remain 1m 54s) Loss: 0.0365 \n",
      "Epoch: [2][3800/4343] Elapsed 11m 19s (remain 1m 36s) Loss: 0.0376 \n",
      "Epoch: [2][3900/4343] Elapsed 11m 37s (remain 1m 19s) Loss: 0.0375 \n",
      "Epoch: [2][4000/4343] Elapsed 11m 55s (remain 1m 1s) Loss: 0.0384 \n",
      "Epoch: [2][4100/4343] Elapsed 12m 13s (remain 0m 43s) Loss: 0.0382 \n",
      "Epoch: [2][4200/4343] Elapsed 12m 31s (remain 0m 25s) Loss: 0.0387 \n",
      "Epoch: [2][4300/4343] Elapsed 12m 49s (remain 0m 7s) Loss: 0.0384 \n",
      "Epoch: [2][4342/4343] Elapsed 12m 56s (remain 0m 0s) Loss: 0.0383 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 16s) Loss: 0.0006 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0653 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0620 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.0533 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0541 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0510 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0456 \n",
      "EVAL: [700/1086] Elapsed 0m 36s (remain 0m 20s) Loss: 0.0476 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 14s) Loss: 0.0464 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0470 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0463 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8464236693609881, thresh=0.023282372444280715\n",
      "score2 = 0.8191653786707883,  thresh=0.09170872020267483\n",
      "Epoch 2 - avg_train_loss: 0.0383  avg_val_loss: 0.0456  time: 834s\n",
      "Epoch 2 - Score: 0.8191653786707883\n",
      "Epoch 2 - Save Best Score: 0.8192 - Best Loss: 0.0456 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0456 \n",
      "Epoch: [3][0/4343] Elapsed 0m 0s (remain 28m 5s) Loss: 0.0007 \n",
      "Epoch: [3][100/4343] Elapsed 0m 18s (remain 12m 48s) Loss: 0.0142 \n",
      "Epoch: [3][200/4343] Elapsed 0m 36s (remain 12m 25s) Loss: 0.0233 \n",
      "Epoch: [3][300/4343] Elapsed 0m 54s (remain 12m 5s) Loss: 0.0209 \n",
      "Epoch: [3][400/4343] Elapsed 1m 11s (remain 11m 46s) Loss: 0.0188 \n",
      "Epoch: [3][500/4343] Elapsed 1m 29s (remain 11m 28s) Loss: 0.0224 \n",
      "Epoch: [3][600/4343] Elapsed 1m 47s (remain 11m 10s) Loss: 0.0205 \n",
      "Epoch: [3][700/4343] Elapsed 2m 5s (remain 10m 52s) Loss: 0.0248 \n",
      "Epoch: [3][800/4343] Elapsed 2m 23s (remain 10m 34s) Loss: 0.0263 \n",
      "Epoch: [3][900/4343] Elapsed 2m 41s (remain 10m 16s) Loss: 0.0263 \n",
      "Epoch: [3][1000/4343] Elapsed 2m 59s (remain 9m 58s) Loss: 0.0271 \n",
      "Epoch: [3][1100/4343] Elapsed 3m 17s (remain 9m 40s) Loss: 0.0273 \n",
      "Epoch: [3][1200/4343] Elapsed 3m 34s (remain 9m 22s) Loss: 0.0274 \n",
      "Epoch: [3][1300/4343] Elapsed 3m 52s (remain 9m 4s) Loss: 0.0270 \n",
      "Epoch: [3][1400/4343] Elapsed 4m 10s (remain 8m 46s) Loss: 0.0267 \n",
      "Epoch: [3][1500/4343] Elapsed 4m 28s (remain 8m 28s) Loss: 0.0267 \n",
      "Epoch: [3][1600/4343] Elapsed 4m 46s (remain 8m 10s) Loss: 0.0267 \n",
      "Epoch: [3][1700/4343] Elapsed 5m 4s (remain 7m 52s) Loss: 0.0271 \n",
      "Epoch: [3][1800/4343] Elapsed 5m 22s (remain 7m 34s) Loss: 0.0271 \n",
      "Epoch: [3][1900/4343] Elapsed 5m 40s (remain 7m 16s) Loss: 0.0266 \n",
      "Epoch: [3][2000/4343] Elapsed 5m 57s (remain 6m 58s) Loss: 0.0270 \n",
      "Epoch: [3][2100/4343] Elapsed 6m 15s (remain 6m 40s) Loss: 0.0263 \n",
      "Epoch: [3][2200/4343] Elapsed 6m 33s (remain 6m 23s) Loss: 0.0270 \n",
      "Epoch: [3][2300/4343] Elapsed 6m 51s (remain 6m 5s) Loss: 0.0277 \n",
      "Epoch: [3][2400/4343] Elapsed 7m 9s (remain 5m 47s) Loss: 0.0272 \n",
      "Epoch: [3][2500/4343] Elapsed 7m 27s (remain 5m 29s) Loss: 0.0274 \n",
      "Epoch: [3][2600/4343] Elapsed 7m 45s (remain 5m 11s) Loss: 0.0275 \n",
      "Epoch: [3][2700/4343] Elapsed 8m 3s (remain 4m 53s) Loss: 0.0279 \n",
      "Epoch: [3][2800/4343] Elapsed 8m 20s (remain 4m 35s) Loss: 0.0280 \n",
      "Epoch: [3][2900/4343] Elapsed 8m 38s (remain 4m 17s) Loss: 0.0278 \n",
      "Epoch: [3][3000/4343] Elapsed 8m 56s (remain 3m 59s) Loss: 0.0275 \n",
      "Epoch: [3][3100/4343] Elapsed 9m 14s (remain 3m 42s) Loss: 0.0278 \n",
      "Epoch: [3][3200/4343] Elapsed 9m 32s (remain 3m 24s) Loss: 0.0274 \n",
      "Epoch: [3][3300/4343] Elapsed 9m 50s (remain 3m 6s) Loss: 0.0272 \n",
      "Epoch: [3][3400/4343] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0277 \n",
      "Epoch: [3][3500/4343] Elapsed 10m 26s (remain 2m 30s) Loss: 0.0275 \n",
      "Epoch: [3][3600/4343] Elapsed 10m 43s (remain 2m 12s) Loss: 0.0277 \n",
      "Epoch: [3][3700/4343] Elapsed 11m 1s (remain 1m 54s) Loss: 0.0280 \n",
      "Epoch: [3][3800/4343] Elapsed 11m 19s (remain 1m 36s) Loss: 0.0278 \n",
      "Epoch: [3][3900/4343] Elapsed 11m 37s (remain 1m 19s) Loss: 0.0279 \n",
      "Epoch: [3][4000/4343] Elapsed 11m 55s (remain 1m 1s) Loss: 0.0277 \n",
      "Epoch: [3][4100/4343] Elapsed 12m 13s (remain 0m 43s) Loss: 0.0276 \n",
      "Epoch: [3][4200/4343] Elapsed 12m 31s (remain 0m 25s) Loss: 0.0274 \n",
      "Epoch: [3][4300/4343] Elapsed 12m 49s (remain 0m 7s) Loss: 0.0271 \n",
      "Epoch: [3][4342/4343] Elapsed 12m 56s (remain 0m 0s) Loss: 0.0273 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 19s) Loss: 0.0024 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0746 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0727 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.0703 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0722 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0664 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0632 \n",
      "EVAL: [700/1086] Elapsed 0m 36s (remain 0m 20s) Loss: 0.0643 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 14s) Loss: 0.0653 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0649 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0650 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8910312591294186, thresh=0.023282372444280715\n",
      "score2 = 0.849128127369219,  thresh=0.13297091770270206\n",
      "Epoch 3 - avg_train_loss: 0.0273  avg_val_loss: 0.0635  time: 834s\n",
      "Epoch 3 - Score: 0.849128127369219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0635 \n",
      "Epoch: [4][0/4343] Elapsed 0m 0s (remain 26m 4s) Loss: 0.0049 \n",
      "Epoch: [4][100/4343] Elapsed 0m 18s (remain 12m 47s) Loss: 0.0219 \n",
      "Epoch: [4][200/4343] Elapsed 0m 36s (remain 12m 24s) Loss: 0.0226 \n",
      "Epoch: [4][300/4343] Elapsed 0m 54s (remain 12m 5s) Loss: 0.0211 \n",
      "Epoch: [4][400/4343] Elapsed 1m 11s (remain 11m 46s) Loss: 0.0199 \n",
      "Epoch: [4][500/4343] Elapsed 1m 29s (remain 11m 28s) Loss: 0.0181 \n",
      "Epoch: [4][600/4343] Elapsed 1m 47s (remain 11m 10s) Loss: 0.0185 \n",
      "Epoch: [4][700/4343] Elapsed 2m 5s (remain 10m 52s) Loss: 0.0185 \n",
      "Epoch: [4][800/4343] Elapsed 2m 23s (remain 10m 34s) Loss: 0.0204 \n",
      "Epoch: [4][900/4343] Elapsed 2m 41s (remain 10m 16s) Loss: 0.0263 \n",
      "Epoch: [4][1000/4343] Elapsed 2m 59s (remain 9m 58s) Loss: 0.0311 \n",
      "Epoch: [4][1100/4343] Elapsed 3m 17s (remain 9m 40s) Loss: 0.0371 \n",
      "Epoch: [4][1200/4343] Elapsed 3m 34s (remain 9m 22s) Loss: 0.0432 \n",
      "Epoch: [4][1300/4343] Elapsed 3m 52s (remain 9m 4s) Loss: 0.0500 \n",
      "Epoch: [4][1400/4343] Elapsed 4m 10s (remain 8m 46s) Loss: 0.0557 \n",
      "Epoch: [4][1500/4343] Elapsed 4m 28s (remain 8m 28s) Loss: 0.0584 \n",
      "Epoch: [4][1600/4343] Elapsed 4m 46s (remain 8m 10s) Loss: 0.0623 \n",
      "Epoch: [4][1700/4343] Elapsed 5m 4s (remain 7m 52s) Loss: 0.0645 \n",
      "Epoch: [4][1800/4343] Elapsed 5m 22s (remain 7m 34s) Loss: 0.0654 \n",
      "Epoch: [4][1900/4343] Elapsed 5m 40s (remain 7m 16s) Loss: 0.0656 \n",
      "Epoch: [4][2000/4343] Elapsed 5m 57s (remain 6m 58s) Loss: 0.0680 \n",
      "Epoch: [4][2100/4343] Elapsed 6m 15s (remain 6m 41s) Loss: 0.0675 \n",
      "Epoch: [4][2200/4343] Elapsed 6m 33s (remain 6m 23s) Loss: 0.0704 \n",
      "Epoch: [4][2300/4343] Elapsed 6m 51s (remain 6m 5s) Loss: 0.0708 \n",
      "Epoch: [4][2400/4343] Elapsed 7m 9s (remain 5m 47s) Loss: 0.0715 \n",
      "Epoch: [4][2500/4343] Elapsed 7m 27s (remain 5m 29s) Loss: 0.0729 \n",
      "Epoch: [4][2600/4343] Elapsed 7m 45s (remain 5m 11s) Loss: 0.0746 \n",
      "Epoch: [4][2700/4343] Elapsed 8m 3s (remain 4m 53s) Loss: 0.0771 \n",
      "Epoch: [4][2800/4343] Elapsed 8m 20s (remain 4m 35s) Loss: 0.0771 \n",
      "Epoch: [4][2900/4343] Elapsed 8m 38s (remain 4m 17s) Loss: 0.0782 \n",
      "Epoch: [4][3000/4343] Elapsed 8m 56s (remain 3m 59s) Loss: 0.0774 \n",
      "Epoch: [4][3100/4343] Elapsed 9m 14s (remain 3m 42s) Loss: 0.0784 \n",
      "Epoch: [4][3200/4343] Elapsed 9m 32s (remain 3m 24s) Loss: 0.0786 \n",
      "Epoch: [4][3300/4343] Elapsed 9m 50s (remain 3m 6s) Loss: 0.0786 \n",
      "Epoch: [4][3400/4343] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0792 \n",
      "Epoch: [4][3500/4343] Elapsed 10m 26s (remain 2m 30s) Loss: 0.0782 \n",
      "Epoch: [4][3600/4343] Elapsed 10m 43s (remain 2m 12s) Loss: 0.0779 \n",
      "Epoch: [4][3700/4343] Elapsed 11m 1s (remain 1m 54s) Loss: 0.0781 \n",
      "Epoch: [4][3800/4343] Elapsed 11m 19s (remain 1m 36s) Loss: 0.0779 \n",
      "Epoch: [4][3900/4343] Elapsed 11m 37s (remain 1m 19s) Loss: 0.0774 \n",
      "Epoch: [4][4000/4343] Elapsed 11m 55s (remain 1m 1s) Loss: 0.0768 \n",
      "Epoch: [4][4100/4343] Elapsed 12m 13s (remain 0m 43s) Loss: 0.0758 \n",
      "Epoch: [4][4200/4343] Elapsed 12m 31s (remain 0m 25s) Loss: 0.0753 \n",
      "Epoch: [4][4300/4343] Elapsed 12m 49s (remain 0m 7s) Loss: 0.0760 \n",
      "Epoch: [4][4342/4343] Elapsed 12m 56s (remain 0m 0s) Loss: 0.0768 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 19s) Loss: 0.0213 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.1200 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.1247 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.1133 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.1114 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.1118 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.1032 \n",
      "EVAL: [700/1086] Elapsed 0m 36s (remain 0m 20s) Loss: 0.1035 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 14s) Loss: 0.1000 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.1032 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.1065 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.024092515258592993, thresh=0.023282372444280715\n",
      "score2 = 0.024092515258592993,  thresh=0.49999553788108364\n",
      "Epoch 4 - avg_train_loss: 0.0768  avg_val_loss: 0.1091  time: 834s\n",
      "Epoch 4 - Score: 0.024092515258592993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.1091 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 result ==========\n",
      "Score1: 0.84642\n",
      "best border: 0.09171\n",
      "Score2: 0.81917\n",
      "=*========= fold: 4 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/4343] Elapsed 0m 0s (remain 26m 48s) Loss: 0.6928 \n",
      "Epoch: [1][100/4343] Elapsed 0m 18s (remain 12m 42s) Loss: 0.1118 \n",
      "Epoch: [1][200/4343] Elapsed 0m 35s (remain 12m 21s) Loss: 0.1286 \n",
      "Epoch: [1][300/4343] Elapsed 0m 53s (remain 12m 2s) Loss: 0.1325 \n",
      "Epoch: [1][400/4343] Elapsed 1m 11s (remain 11m 44s) Loss: 0.1264 \n",
      "Epoch: [1][500/4343] Elapsed 1m 29s (remain 11m 26s) Loss: 0.1237 \n",
      "Epoch: [1][600/4343] Elapsed 1m 47s (remain 11m 8s) Loss: 0.1214 \n",
      "Epoch: [1][700/4343] Elapsed 2m 5s (remain 10m 51s) Loss: 0.1118 \n",
      "Epoch: [1][800/4343] Elapsed 2m 23s (remain 10m 33s) Loss: 0.1074 \n",
      "Epoch: [1][900/4343] Elapsed 2m 41s (remain 10m 15s) Loss: 0.1049 \n",
      "Epoch: [1][1000/4343] Elapsed 2m 58s (remain 9m 57s) Loss: 0.1041 \n",
      "Epoch: [1][1100/4343] Elapsed 3m 16s (remain 9m 39s) Loss: 0.1035 \n",
      "Epoch: [1][1200/4343] Elapsed 3m 34s (remain 9m 21s) Loss: 0.0982 \n",
      "Epoch: [1][1300/4343] Elapsed 3m 52s (remain 9m 3s) Loss: 0.0964 \n",
      "Epoch: [1][1400/4343] Elapsed 4m 10s (remain 8m 45s) Loss: 0.0950 \n",
      "Epoch: [1][1500/4343] Elapsed 4m 28s (remain 8m 28s) Loss: 0.0928 \n",
      "Epoch: [1][1600/4343] Elapsed 4m 46s (remain 8m 10s) Loss: 0.0901 \n",
      "Epoch: [1][1700/4343] Elapsed 5m 4s (remain 7m 52s) Loss: 0.0878 \n",
      "Epoch: [1][1800/4343] Elapsed 5m 21s (remain 7m 34s) Loss: 0.0864 \n",
      "Epoch: [1][1900/4343] Elapsed 5m 39s (remain 7m 16s) Loss: 0.0843 \n",
      "Epoch: [1][2000/4343] Elapsed 5m 57s (remain 6m 58s) Loss: 0.0828 \n",
      "Epoch: [1][2100/4343] Elapsed 6m 15s (remain 6m 40s) Loss: 0.0817 \n",
      "Epoch: [1][2200/4343] Elapsed 6m 33s (remain 6m 22s) Loss: 0.0805 \n",
      "Epoch: [1][2300/4343] Elapsed 6m 51s (remain 6m 5s) Loss: 0.0786 \n",
      "Epoch: [1][2400/4343] Elapsed 7m 9s (remain 5m 47s) Loss: 0.0788 \n",
      "Epoch: [1][2500/4343] Elapsed 7m 27s (remain 5m 29s) Loss: 0.0774 \n",
      "Epoch: [1][2600/4343] Elapsed 7m 44s (remain 5m 11s) Loss: 0.0754 \n",
      "Epoch: [1][2700/4343] Elapsed 8m 2s (remain 4m 53s) Loss: 0.0754 \n",
      "Epoch: [1][2800/4343] Elapsed 8m 20s (remain 4m 35s) Loss: 0.0749 \n",
      "Epoch: [1][2900/4343] Elapsed 8m 38s (remain 4m 17s) Loss: 0.0741 \n",
      "Epoch: [1][3000/4343] Elapsed 8m 56s (remain 3m 59s) Loss: 0.0734 \n",
      "Epoch: [1][3100/4343] Elapsed 9m 14s (remain 3m 42s) Loss: 0.0730 \n",
      "Epoch: [1][3200/4343] Elapsed 9m 32s (remain 3m 24s) Loss: 0.0719 \n",
      "Epoch: [1][3300/4343] Elapsed 9m 50s (remain 3m 6s) Loss: 0.0715 \n",
      "Epoch: [1][3400/4343] Elapsed 10m 7s (remain 2m 48s) Loss: 0.0707 \n",
      "Epoch: [1][3500/4343] Elapsed 10m 25s (remain 2m 30s) Loss: 0.0698 \n",
      "Epoch: [1][3600/4343] Elapsed 10m 43s (remain 2m 12s) Loss: 0.0698 \n",
      "Epoch: [1][3700/4343] Elapsed 11m 1s (remain 1m 54s) Loss: 0.0696 \n",
      "Epoch: [1][3800/4343] Elapsed 11m 19s (remain 1m 36s) Loss: 0.0689 \n",
      "Epoch: [1][3900/4343] Elapsed 11m 37s (remain 1m 19s) Loss: 0.0690 \n",
      "Epoch: [1][4000/4343] Elapsed 11m 55s (remain 1m 1s) Loss: 0.0677 \n",
      "Epoch: [1][4100/4343] Elapsed 12m 13s (remain 0m 43s) Loss: 0.0670 \n",
      "Epoch: [1][4200/4343] Elapsed 12m 30s (remain 0m 25s) Loss: 0.0669 \n",
      "Epoch: [1][4300/4343] Elapsed 12m 48s (remain 0m 7s) Loss: 0.0666 \n",
      "Epoch: [1][4342/4343] Elapsed 12m 56s (remain 0m 0s) Loss: 0.0665 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 16s) Loss: 0.0008 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0352 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0456 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.0402 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0418 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0394 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0393 \n",
      "EVAL: [700/1086] Elapsed 0m 36s (remain 0m 20s) Loss: 0.0383 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 14s) Loss: 0.0395 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0420 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0451 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8983844179374906, thresh=0.023282372444280715\n",
      "score2 = 0.9034783918084626,  thresh=0.02052113652790305\n",
      "Epoch 1 - avg_train_loss: 0.0665  avg_val_loss: 0.0451  time: 834s\n",
      "Epoch 1 - Score: 0.9034783918084626\n",
      "Epoch 1 - Save Best Score: 0.9035 - Best Loss: 0.0451 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0451 \n",
      "Epoch: [2][0/4343] Elapsed 0m 0s (remain 26m 10s) Loss: 0.0060 \n",
      "Epoch: [2][100/4343] Elapsed 0m 18s (remain 12m 46s) Loss: 0.0261 \n",
      "Epoch: [2][200/4343] Elapsed 0m 36s (remain 12m 24s) Loss: 0.0384 \n",
      "Epoch: [2][300/4343] Elapsed 0m 53s (remain 12m 5s) Loss: 0.0401 \n",
      "Epoch: [2][400/4343] Elapsed 1m 11s (remain 11m 46s) Loss: 0.0413 \n",
      "Epoch: [2][500/4343] Elapsed 1m 29s (remain 11m 28s) Loss: 0.0400 \n",
      "Epoch: [2][600/4343] Elapsed 1m 47s (remain 11m 10s) Loss: 0.0397 \n",
      "Epoch: [2][700/4343] Elapsed 2m 5s (remain 10m 52s) Loss: 0.0393 \n",
      "Epoch: [2][800/4343] Elapsed 2m 23s (remain 10m 34s) Loss: 0.0371 \n",
      "Epoch: [2][900/4343] Elapsed 2m 41s (remain 10m 16s) Loss: 0.0398 \n",
      "Epoch: [2][1000/4343] Elapsed 2m 59s (remain 9m 58s) Loss: 0.0387 \n",
      "Epoch: [2][1100/4343] Elapsed 3m 16s (remain 9m 40s) Loss: 0.0373 \n",
      "Epoch: [2][1200/4343] Elapsed 3m 34s (remain 9m 22s) Loss: 0.0372 \n",
      "Epoch: [2][1300/4343] Elapsed 3m 52s (remain 9m 4s) Loss: 0.0369 \n",
      "Epoch: [2][1400/4343] Elapsed 4m 10s (remain 8m 46s) Loss: 0.0382 \n",
      "Epoch: [2][1500/4343] Elapsed 4m 28s (remain 8m 28s) Loss: 0.0375 \n",
      "Epoch: [2][1600/4343] Elapsed 4m 46s (remain 8m 10s) Loss: 0.0376 \n",
      "Epoch: [2][1700/4343] Elapsed 5m 4s (remain 7m 52s) Loss: 0.0383 \n",
      "Epoch: [2][1800/4343] Elapsed 5m 22s (remain 7m 34s) Loss: 0.0394 \n",
      "Epoch: [2][1900/4343] Elapsed 5m 39s (remain 7m 16s) Loss: 0.0389 \n",
      "Epoch: [2][2000/4343] Elapsed 5m 57s (remain 6m 58s) Loss: 0.0388 \n",
      "Epoch: [2][2100/4343] Elapsed 6m 15s (remain 6m 40s) Loss: 0.0385 \n",
      "Epoch: [2][2200/4343] Elapsed 6m 33s (remain 6m 23s) Loss: 0.0387 \n",
      "Epoch: [2][2300/4343] Elapsed 6m 51s (remain 6m 5s) Loss: 0.0391 \n",
      "Epoch: [2][2400/4343] Elapsed 7m 9s (remain 5m 47s) Loss: 0.0392 \n",
      "Epoch: [2][2500/4343] Elapsed 7m 27s (remain 5m 29s) Loss: 0.0387 \n",
      "Epoch: [2][2600/4343] Elapsed 7m 45s (remain 5m 11s) Loss: 0.0386 \n",
      "Epoch: [2][2700/4343] Elapsed 8m 2s (remain 4m 53s) Loss: 0.0379 \n",
      "Epoch: [2][2800/4343] Elapsed 8m 20s (remain 4m 35s) Loss: 0.0377 \n",
      "Epoch: [2][2900/4343] Elapsed 8m 38s (remain 4m 17s) Loss: 0.0372 \n",
      "Epoch: [2][3000/4343] Elapsed 8m 56s (remain 3m 59s) Loss: 0.0371 \n",
      "Epoch: [2][3100/4343] Elapsed 9m 14s (remain 3m 42s) Loss: 0.0367 \n",
      "Epoch: [2][3200/4343] Elapsed 9m 32s (remain 3m 24s) Loss: 0.0370 \n",
      "Epoch: [2][3300/4343] Elapsed 9m 50s (remain 3m 6s) Loss: 0.0373 \n",
      "Epoch: [2][3400/4343] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0369 \n",
      "Epoch: [2][3500/4343] Elapsed 10m 26s (remain 2m 30s) Loss: 0.0369 \n",
      "Epoch: [2][3600/4343] Elapsed 10m 43s (remain 2m 12s) Loss: 0.0369 \n",
      "Epoch: [2][3700/4343] Elapsed 11m 1s (remain 1m 54s) Loss: 0.0372 \n",
      "Epoch: [2][3800/4343] Elapsed 11m 19s (remain 1m 36s) Loss: 0.0367 \n",
      "Epoch: [2][3900/4343] Elapsed 11m 37s (remain 1m 19s) Loss: 0.0371 \n",
      "Epoch: [2][4000/4343] Elapsed 11m 55s (remain 1m 1s) Loss: 0.0369 \n",
      "Epoch: [2][4100/4343] Elapsed 12m 13s (remain 0m 43s) Loss: 0.0365 \n",
      "Epoch: [2][4200/4343] Elapsed 12m 31s (remain 0m 25s) Loss: 0.0363 \n",
      "Epoch: [2][4300/4343] Elapsed 12m 49s (remain 0m 7s) Loss: 0.0363 \n",
      "Epoch: [2][4342/4343] Elapsed 12m 56s (remain 0m 0s) Loss: 0.0362 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 23s) Loss: 0.0026 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0316 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0408 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.0397 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0400 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0363 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0367 \n",
      "EVAL: [700/1086] Elapsed 0m 36s (remain 0m 20s) Loss: 0.0356 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 14s) Loss: 0.0356 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0364 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0389 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8510024520409636, thresh=0.023282372444280715\n",
      "score2 = 0.8784991066110781,  thresh=0.054656810160536126\n",
      "Epoch 2 - avg_train_loss: 0.0362  avg_val_loss: 0.0398  time: 834s\n",
      "Epoch 2 - Score: 0.8784991066110781\n",
      "Epoch 2 - Save Best Score: 0.8785 - Best Loss: 0.0398 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0398 \n",
      "Epoch: [3][0/4343] Elapsed 0m 0s (remain 28m 40s) Loss: 0.0174 \n",
      "Epoch: [3][100/4343] Elapsed 0m 18s (remain 12m 48s) Loss: 0.0151 \n",
      "Epoch: [3][200/4343] Elapsed 0m 36s (remain 12m 25s) Loss: 0.0125 \n",
      "Epoch: [3][300/4343] Elapsed 0m 54s (remain 12m 5s) Loss: 0.0161 \n",
      "Epoch: [3][400/4343] Elapsed 1m 11s (remain 11m 46s) Loss: 0.0179 \n",
      "Epoch: [3][500/4343] Elapsed 1m 29s (remain 11m 28s) Loss: 0.0240 \n",
      "Epoch: [3][600/4343] Elapsed 1m 47s (remain 11m 10s) Loss: 0.0259 \n",
      "Epoch: [3][700/4343] Elapsed 2m 5s (remain 10m 52s) Loss: 0.0237 \n",
      "Epoch: [3][800/4343] Elapsed 2m 23s (remain 10m 34s) Loss: 0.0246 \n",
      "Epoch: [3][900/4343] Elapsed 2m 41s (remain 10m 16s) Loss: 0.0252 \n",
      "Epoch: [3][1000/4343] Elapsed 2m 59s (remain 9m 58s) Loss: 0.0257 \n",
      "Epoch: [3][1100/4343] Elapsed 3m 17s (remain 9m 40s) Loss: 0.0276 \n",
      "Epoch: [3][1200/4343] Elapsed 3m 34s (remain 9m 22s) Loss: 0.0259 \n",
      "Epoch: [3][1300/4343] Elapsed 3m 52s (remain 9m 4s) Loss: 0.0263 \n",
      "Epoch: [3][1400/4343] Elapsed 4m 10s (remain 8m 46s) Loss: 0.0257 \n",
      "Epoch: [3][1500/4343] Elapsed 4m 28s (remain 8m 28s) Loss: 0.0248 \n",
      "Epoch: [3][1600/4343] Elapsed 4m 46s (remain 8m 10s) Loss: 0.0242 \n",
      "Epoch: [3][1700/4343] Elapsed 5m 4s (remain 7m 52s) Loss: 0.0237 \n",
      "Epoch: [3][1800/4343] Elapsed 5m 22s (remain 7m 34s) Loss: 0.0263 \n",
      "Epoch: [3][1900/4343] Elapsed 5m 39s (remain 7m 16s) Loss: 0.0267 \n",
      "Epoch: [3][2000/4343] Elapsed 5m 57s (remain 6m 58s) Loss: 0.0258 \n",
      "Epoch: [3][2100/4343] Elapsed 6m 15s (remain 6m 40s) Loss: 0.0261 \n",
      "Epoch: [3][2200/4343] Elapsed 6m 33s (remain 6m 23s) Loss: 0.0260 \n",
      "Epoch: [3][2300/4343] Elapsed 6m 51s (remain 6m 5s) Loss: 0.0262 \n",
      "Epoch: [3][2400/4343] Elapsed 7m 9s (remain 5m 47s) Loss: 0.0257 \n",
      "Epoch: [3][2500/4343] Elapsed 7m 27s (remain 5m 29s) Loss: 0.0263 \n",
      "Epoch: [3][2600/4343] Elapsed 7m 45s (remain 5m 11s) Loss: 0.0263 \n",
      "Epoch: [3][2700/4343] Elapsed 8m 2s (remain 4m 53s) Loss: 0.0268 \n",
      "Epoch: [3][2800/4343] Elapsed 8m 20s (remain 4m 35s) Loss: 0.0273 \n",
      "Epoch: [3][2900/4343] Elapsed 8m 38s (remain 4m 17s) Loss: 0.0272 \n",
      "Epoch: [3][3000/4343] Elapsed 8m 56s (remain 3m 59s) Loss: 0.0277 \n",
      "Epoch: [3][3100/4343] Elapsed 9m 14s (remain 3m 42s) Loss: 0.0283 \n",
      "Epoch: [3][3200/4343] Elapsed 9m 32s (remain 3m 24s) Loss: 0.0289 \n",
      "Epoch: [3][3300/4343] Elapsed 9m 50s (remain 3m 6s) Loss: 0.0288 \n",
      "Epoch: [3][3400/4343] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0295 \n",
      "Epoch: [3][3500/4343] Elapsed 10m 25s (remain 2m 30s) Loss: 0.0293 \n",
      "Epoch: [3][3600/4343] Elapsed 10m 43s (remain 2m 12s) Loss: 0.0288 \n",
      "Epoch: [3][3700/4343] Elapsed 11m 1s (remain 1m 54s) Loss: 0.0289 \n",
      "Epoch: [3][3800/4343] Elapsed 11m 19s (remain 1m 36s) Loss: 0.0290 \n",
      "Epoch: [3][3900/4343] Elapsed 11m 37s (remain 1m 19s) Loss: 0.0296 \n",
      "Epoch: [3][4000/4343] Elapsed 11m 55s (remain 1m 1s) Loss: 0.0301 \n",
      "Epoch: [3][4100/4343] Elapsed 12m 13s (remain 0m 43s) Loss: 0.0300 \n",
      "Epoch: [3][4200/4343] Elapsed 12m 31s (remain 0m 25s) Loss: 0.0298 \n",
      "Epoch: [3][4300/4343] Elapsed 12m 48s (remain 0m 7s) Loss: 0.0305 \n",
      "Epoch: [3][4342/4343] Elapsed 12m 56s (remain 0m 0s) Loss: 0.0306 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 18s) Loss: 0.0009 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0405 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.0482 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.0469 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.0533 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.0496 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.0515 \n",
      "EVAL: [700/1086] Elapsed 0m 36s (remain 0m 20s) Loss: 0.0509 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 14s) Loss: 0.0517 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.0527 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.0558 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.8348240906380441, thresh=0.023282372444280715\n",
      "score2 = 0.8159564823209429,  thresh=0.10400718253966176\n",
      "Epoch 3 - avg_train_loss: 0.0306  avg_val_loss: 0.0551  time: 834s\n",
      "Epoch 3 - Score: 0.8159564823209429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 56s (remain 0m 0s) Loss: 0.0551 \n",
      "Epoch: [4][0/4343] Elapsed 0m 0s (remain 25m 52s) Loss: 0.0012 \n",
      "Epoch: [4][100/4343] Elapsed 0m 18s (remain 12m 47s) Loss: 0.0232 \n",
      "Epoch: [4][200/4343] Elapsed 0m 36s (remain 12m 24s) Loss: 0.0141 \n",
      "Epoch: [4][300/4343] Elapsed 0m 54s (remain 12m 5s) Loss: 0.0188 \n",
      "Epoch: [4][400/4343] Elapsed 1m 11s (remain 11m 46s) Loss: 0.0155 \n",
      "Epoch: [4][500/4343] Elapsed 1m 29s (remain 11m 28s) Loss: 0.0200 \n",
      "Epoch: [4][600/4343] Elapsed 1m 47s (remain 11m 10s) Loss: 0.0234 \n",
      "Epoch: [4][700/4343] Elapsed 2m 5s (remain 10m 52s) Loss: 0.0215 \n",
      "Epoch: [4][800/4343] Elapsed 2m 23s (remain 10m 34s) Loss: 0.0218 \n",
      "Epoch: [4][900/4343] Elapsed 2m 41s (remain 10m 16s) Loss: 0.0214 \n",
      "Epoch: [4][1000/4343] Elapsed 2m 59s (remain 9m 58s) Loss: 0.0222 \n",
      "Epoch: [4][1100/4343] Elapsed 3m 16s (remain 9m 40s) Loss: 0.0212 \n",
      "Epoch: [4][1200/4343] Elapsed 3m 34s (remain 9m 22s) Loss: 0.0217 \n",
      "Epoch: [4][1300/4343] Elapsed 3m 52s (remain 9m 4s) Loss: 0.0216 \n",
      "Epoch: [4][1400/4343] Elapsed 4m 10s (remain 8m 46s) Loss: 0.0207 \n",
      "Epoch: [4][1500/4343] Elapsed 4m 28s (remain 8m 28s) Loss: 0.0228 \n",
      "Epoch: [4][1600/4343] Elapsed 4m 46s (remain 8m 10s) Loss: 0.0223 \n",
      "Epoch: [4][1700/4343] Elapsed 5m 4s (remain 7m 52s) Loss: 0.0220 \n",
      "Epoch: [4][1800/4343] Elapsed 5m 22s (remain 7m 34s) Loss: 0.0239 \n",
      "Epoch: [4][1900/4343] Elapsed 5m 39s (remain 7m 16s) Loss: 0.0248 \n",
      "Epoch: [4][2000/4343] Elapsed 5m 57s (remain 6m 58s) Loss: 0.0269 \n",
      "Epoch: [4][2100/4343] Elapsed 6m 15s (remain 6m 40s) Loss: 0.0275 \n",
      "Epoch: [4][2200/4343] Elapsed 6m 33s (remain 6m 23s) Loss: 0.0289 \n",
      "Epoch: [4][2300/4343] Elapsed 6m 51s (remain 6m 5s) Loss: 0.0300 \n",
      "Epoch: [4][2400/4343] Elapsed 7m 9s (remain 5m 47s) Loss: 0.0299 \n",
      "Epoch: [4][2500/4343] Elapsed 7m 27s (remain 5m 29s) Loss: 0.0308 \n",
      "Epoch: [4][2600/4343] Elapsed 7m 45s (remain 5m 11s) Loss: 0.0307 \n",
      "Epoch: [4][2700/4343] Elapsed 8m 2s (remain 4m 53s) Loss: 0.0304 \n",
      "Epoch: [4][2800/4343] Elapsed 8m 20s (remain 4m 35s) Loss: 0.0317 \n",
      "Epoch: [4][2900/4343] Elapsed 8m 38s (remain 4m 17s) Loss: 0.0316 \n",
      "Epoch: [4][3000/4343] Elapsed 8m 56s (remain 3m 59s) Loss: 0.0316 \n",
      "Epoch: [4][3100/4343] Elapsed 9m 14s (remain 3m 42s) Loss: 0.0327 \n",
      "Epoch: [4][3200/4343] Elapsed 9m 32s (remain 3m 24s) Loss: 0.0337 \n",
      "Epoch: [4][3300/4343] Elapsed 9m 50s (remain 3m 6s) Loss: 0.0337 \n",
      "Epoch: [4][3400/4343] Elapsed 10m 8s (remain 2m 48s) Loss: 0.0345 \n",
      "Epoch: [4][3500/4343] Elapsed 10m 26s (remain 2m 30s) Loss: 0.0352 \n",
      "Epoch: [4][3600/4343] Elapsed 10m 43s (remain 2m 12s) Loss: 0.0348 \n",
      "Epoch: [4][3700/4343] Elapsed 11m 1s (remain 1m 54s) Loss: 0.0348 \n",
      "Epoch: [4][3800/4343] Elapsed 11m 19s (remain 1m 36s) Loss: 0.0347 \n",
      "Epoch: [4][3900/4343] Elapsed 11m 37s (remain 1m 19s) Loss: 0.0354 \n",
      "Epoch: [4][4000/4343] Elapsed 11m 55s (remain 1m 1s) Loss: 0.0357 \n",
      "Epoch: [4][4100/4343] Elapsed 12m 13s (remain 0m 43s) Loss: 0.0363 \n",
      "Epoch: [4][4200/4343] Elapsed 12m 31s (remain 0m 25s) Loss: 0.0362 \n",
      "Epoch: [4][4300/4343] Elapsed 12m 48s (remain 0m 7s) Loss: 0.0376 \n",
      "Epoch: [4][4342/4343] Elapsed 12m 56s (remain 0m 0s) Loss: 0.0384 \n",
      "EVAL: [0/1086] Elapsed 0m 0s (remain 4m 19s) Loss: 0.0370 \n",
      "EVAL: [100/1086] Elapsed 0m 5s (remain 0m 53s) Loss: 0.0953 \n",
      "EVAL: [200/1086] Elapsed 0m 10s (remain 0m 47s) Loss: 0.1087 \n",
      "EVAL: [300/1086] Elapsed 0m 15s (remain 0m 41s) Loss: 0.1045 \n",
      "EVAL: [400/1086] Elapsed 0m 21s (remain 0m 36s) Loss: 0.1089 \n",
      "EVAL: [500/1086] Elapsed 0m 26s (remain 0m 30s) Loss: 0.1089 \n",
      "EVAL: [600/1086] Elapsed 0m 31s (remain 0m 25s) Loss: 0.1046 \n",
      "EVAL: [700/1086] Elapsed 0m 36s (remain 0m 20s) Loss: 0.1024 \n",
      "EVAL: [800/1086] Elapsed 0m 42s (remain 0m 14s) Loss: 0.1041 \n",
      "EVAL: [900/1086] Elapsed 0m 47s (remain 0m 9s) Loss: 0.1083 \n",
      "EVAL: [1000/1086] Elapsed 0m 52s (remain 0m 4s) Loss: 0.1123 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score1 = 0.5454389280192407, thresh=0.023282372444280715\n",
      "score2 = 0.0,  thresh=0.49999553788108364\n",
      "Epoch 4 - avg_train_loss: 0.0384  avg_val_loss: 0.1136  time: 834s\n",
      "Epoch 4 - Score: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [1085/1086] Elapsed 0m 56s (remain 0m 0s) Loss: 0.1136 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 4 result ==========\n",
      "Score1: 0.85100\n",
      "best border: 0.05466\n",
      "Score2: 0.87850\n",
      "========== CV ==========\n",
      "Score1: 0.86591\n",
      "best border: 0.00706\n",
      "Score2: 0.89477\n"
     ]
    }
   ],
   "source": [
    "##main\n",
    "\n",
    "oof_df = pd.DataFrame()\n",
    "for fold in range(CFG.n_splits): \n",
    "    _oof_df = train_loop(train, fold)\n",
    "    oof_df = pd.concat([oof_df, _oof_df])\n",
    "    LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "    get_result(_oof_df)\n",
    "        \n",
    "# CV result\n",
    "LOGGER.info(f\"========== CV ==========\")\n",
    "score, best_border = get_result(oof_df)\n",
    "    \n",
    "# Save OOF result\n",
    "oof_df.to_csv(OUTPUT_DIR + f\"oof_df_{CFG.version}_{score:<.5f}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df60abde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.023282372444280715, 0.0070590010853141885)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "border, best_border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db831d56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: bert-base-uncased fold: 0 inference ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9865252943784af195b043a2adaa11bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: bert-base-uncased fold: 1 inference ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51520df2e6164d00af18f3048654dc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: bert-base-uncased fold: 2 inference ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42715f63e2e24f0bbb838d2bde1ecf3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: bert-base-uncased fold: 3 inference ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fd540ba2d9402f8afe2ba6f338ddbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: bert-base-uncased fold: 4 inference ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cf3b30783a4d3aab6efbc47885dc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inference\n",
    "predictions = inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37ba564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ensemble\n",
    "test[\"judgement\"] = predictions\n",
    "test.to_csv(OUTPUT_DIR + f\"predictions_{CFG.version}.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae54f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#border2 = 0.0070590010853141885\n",
    "# submission\n",
    "predictions2 = np.where(predictions < border, 0, 1)\n",
    "sub[\"judgement\"] = predictions2\n",
    "sub.to_csv(OUTPUT_DIR + f\"submission_{CFG.version}-2.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e924f46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
